[
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "torch.nn.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "rearrange",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "einsum",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "rearrange",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "rearrange",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "einsum",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "rearrange",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "SimpleRMSNorm",
        "importPath": "zeta.nn.modules.simple_rmsnorm",
        "description": "zeta.nn.modules.simple_rmsnorm",
        "isExtraImport": true,
        "detail": "zeta.nn.modules.simple_rmsnorm",
        "documentation": {}
    },
    {
        "label": "SimpleRMSNorm",
        "importPath": "zeta.nn.modules.simple_rmsnorm",
        "description": "zeta.nn.modules.simple_rmsnorm",
        "isExtraImport": true,
        "detail": "zeta.nn.modules.simple_rmsnorm",
        "documentation": {}
    },
    {
        "label": "SimpleRMSNorm",
        "importPath": "zeta.nn.modules.simple_rmsnorm",
        "description": "zeta.nn.modules.simple_rmsnorm",
        "isExtraImport": true,
        "detail": "zeta.nn.modules.simple_rmsnorm",
        "documentation": {}
    },
    {
        "label": "SimpleRMSNorm",
        "importPath": "zeta.nn.modules.simple_rmsnorm",
        "description": "zeta.nn.modules.simple_rmsnorm",
        "isExtraImport": true,
        "detail": "zeta.nn.modules.simple_rmsnorm",
        "documentation": {}
    },
    {
        "label": "SimpleRMSNorm",
        "importPath": "zeta.nn.modules.simple_rmsnorm",
        "description": "zeta.nn.modules.simple_rmsnorm",
        "isExtraImport": true,
        "detail": "zeta.nn.modules.simple_rmsnorm",
        "documentation": {}
    },
    {
        "label": "SimpleRMSNorm",
        "importPath": "zeta.nn.modules.simple_rmsnorm",
        "description": "zeta.nn.modules.simple_rmsnorm",
        "isExtraImport": true,
        "detail": "zeta.nn.modules.simple_rmsnorm",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "BitLinear",
        "importPath": "bitnet.bitlinear",
        "description": "bitnet.bitlinear",
        "isExtraImport": true,
        "detail": "bitnet.bitlinear",
        "documentation": {}
    },
    {
        "label": "BitLinear",
        "importPath": "bitnet.bitlinear",
        "description": "bitnet.bitlinear",
        "isExtraImport": true,
        "detail": "bitnet.bitlinear",
        "documentation": {}
    },
    {
        "label": "BitLinear",
        "importPath": "bitnet.bitlinear",
        "description": "bitnet.bitlinear",
        "isExtraImport": true,
        "detail": "bitnet.bitlinear",
        "documentation": {}
    },
    {
        "label": "BitLinear",
        "importPath": "bitnet.bitlinear",
        "description": "bitnet.bitlinear",
        "isExtraImport": true,
        "detail": "bitnet.bitlinear",
        "documentation": {}
    },
    {
        "label": "BitLinear",
        "importPath": "bitnet.bitlinear",
        "description": "bitnet.bitlinear",
        "isExtraImport": true,
        "detail": "bitnet.bitlinear",
        "documentation": {}
    },
    {
        "label": "BitLinear",
        "importPath": "bitnet.bitlinear",
        "description": "bitnet.bitlinear",
        "isExtraImport": true,
        "detail": "bitnet.bitlinear",
        "documentation": {}
    },
    {
        "label": "BitLinear",
        "importPath": "bitnet.bitlinear",
        "description": "bitnet.bitlinear",
        "isExtraImport": true,
        "detail": "bitnet.bitlinear",
        "documentation": {}
    },
    {
        "label": "absmax_quantize",
        "importPath": "bitnet.bitlinear",
        "description": "bitnet.bitlinear",
        "isExtraImport": true,
        "detail": "bitnet.bitlinear",
        "documentation": {}
    },
    {
        "label": "BitLinear",
        "importPath": "bitnet.bitlinear",
        "description": "bitnet.bitlinear",
        "isExtraImport": true,
        "detail": "bitnet.bitlinear",
        "documentation": {}
    },
    {
        "label": "BitLinear",
        "importPath": "bitnet.bitlinear",
        "description": "bitnet.bitlinear",
        "isExtraImport": true,
        "detail": "bitnet.bitlinear",
        "documentation": {}
    },
    {
        "label": "BitLinear",
        "importPath": "bitnet.bitlinear",
        "description": "bitnet.bitlinear",
        "isExtraImport": true,
        "detail": "bitnet.bitlinear",
        "documentation": {}
    },
    {
        "label": "BitLinear",
        "importPath": "bitnet.bitlinear",
        "description": "bitnet.bitlinear",
        "isExtraImport": true,
        "detail": "bitnet.bitlinear",
        "documentation": {}
    },
    {
        "label": "BitLinear",
        "importPath": "bitnet.bitlinear",
        "description": "bitnet.bitlinear",
        "isExtraImport": true,
        "detail": "bitnet.bitlinear",
        "documentation": {}
    },
    {
        "label": "BitLinear",
        "importPath": "bitnet.bitlinear",
        "description": "bitnet.bitlinear",
        "isExtraImport": true,
        "detail": "bitnet.bitlinear",
        "documentation": {}
    },
    {
        "label": "BitLinear",
        "importPath": "bitnet.bitlinear",
        "description": "bitnet.bitlinear",
        "isExtraImport": true,
        "detail": "bitnet.bitlinear",
        "documentation": {}
    },
    {
        "label": "BitLinear",
        "importPath": "bitnet.bitlinear",
        "description": "bitnet.bitlinear",
        "isExtraImport": true,
        "detail": "bitnet.bitlinear",
        "documentation": {}
    },
    {
        "label": "BitLinear",
        "importPath": "bitnet.bitlinear",
        "description": "bitnet.bitlinear",
        "isExtraImport": true,
        "detail": "bitnet.bitlinear",
        "documentation": {}
    },
    {
        "label": "absmax_quantize",
        "importPath": "bitnet.bitlinear",
        "description": "bitnet.bitlinear",
        "isExtraImport": true,
        "detail": "bitnet.bitlinear",
        "documentation": {}
    },
    {
        "label": "BitLinear",
        "importPath": "bitnet.bitlinear",
        "description": "bitnet.bitlinear",
        "isExtraImport": true,
        "detail": "bitnet.bitlinear",
        "documentation": {}
    },
    {
        "label": "BitLinear",
        "importPath": "bitnet.bitlinear",
        "description": "bitnet.bitlinear",
        "isExtraImport": true,
        "detail": "bitnet.bitlinear",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "fairscale.nn.model_parallel.initialize",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fairscale.nn.model_parallel.initialize",
        "description": "fairscale.nn.model_parallel.initialize",
        "detail": "fairscale.nn.model_parallel.initialize",
        "documentation": {}
    },
    {
        "label": "ColumnParallelLinear",
        "importPath": "fairscale.nn.model_parallel.layers",
        "description": "fairscale.nn.model_parallel.layers",
        "isExtraImport": true,
        "detail": "fairscale.nn.model_parallel.layers",
        "documentation": {}
    },
    {
        "label": "ParallelEmbedding",
        "importPath": "fairscale.nn.model_parallel.layers",
        "description": "fairscale.nn.model_parallel.layers",
        "isExtraImport": true,
        "detail": "fairscale.nn.model_parallel.layers",
        "documentation": {}
    },
    {
        "label": "RowParallelLinear",
        "importPath": "fairscale.nn.model_parallel.layers",
        "description": "fairscale.nn.model_parallel.layers",
        "isExtraImport": true,
        "detail": "fairscale.nn.model_parallel.layers",
        "documentation": {}
    },
    {
        "label": "ColumnParallelLinear",
        "importPath": "fairscale.nn.model_parallel.layers",
        "description": "fairscale.nn.model_parallel.layers",
        "isExtraImport": true,
        "detail": "fairscale.nn.model_parallel.layers",
        "documentation": {}
    },
    {
        "label": "ParallelEmbedding",
        "importPath": "fairscale.nn.model_parallel.layers",
        "description": "fairscale.nn.model_parallel.layers",
        "isExtraImport": true,
        "detail": "fairscale.nn.model_parallel.layers",
        "documentation": {}
    },
    {
        "label": "RowParallelLinear",
        "importPath": "fairscale.nn.model_parallel.layers",
        "description": "fairscale.nn.model_parallel.layers",
        "isExtraImport": true,
        "detail": "fairscale.nn.model_parallel.layers",
        "documentation": {}
    },
    {
        "label": "OutputHead",
        "importPath": "zeta.nn",
        "description": "zeta.nn",
        "isExtraImport": true,
        "detail": "zeta.nn",
        "documentation": {}
    },
    {
        "label": "OutputHead",
        "importPath": "zeta.nn",
        "description": "zeta.nn",
        "isExtraImport": true,
        "detail": "zeta.nn",
        "documentation": {}
    },
    {
        "label": "BitFeedForward",
        "importPath": "bitnet.bit_ffn",
        "description": "bitnet.bit_ffn",
        "isExtraImport": true,
        "detail": "bitnet.bit_ffn",
        "documentation": {}
    },
    {
        "label": "BitFeedForward",
        "importPath": "bitnet.bit_ffn",
        "description": "bitnet.bit_ffn",
        "isExtraImport": true,
        "detail": "bitnet.bit_ffn",
        "documentation": {}
    },
    {
        "label": "BitFeedForward",
        "importPath": "bitnet.bit_ffn",
        "description": "bitnet.bit_ffn",
        "isExtraImport": true,
        "detail": "bitnet.bit_ffn",
        "documentation": {}
    },
    {
        "label": "BitFeedForward",
        "importPath": "bitnet.bit_ffn",
        "description": "bitnet.bit_ffn",
        "isExtraImport": true,
        "detail": "bitnet.bit_ffn",
        "documentation": {}
    },
    {
        "label": "BitMGQA",
        "importPath": "bitnet.bit_attention",
        "description": "bitnet.bit_attention",
        "isExtraImport": true,
        "detail": "bitnet.bit_attention",
        "documentation": {}
    },
    {
        "label": "BitMGQA",
        "importPath": "bitnet.bit_attention",
        "description": "bitnet.bit_attention",
        "isExtraImport": true,
        "detail": "bitnet.bit_attention",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "AutoregressiveWrapper",
        "importPath": "bitnet.at",
        "description": "bitnet.at",
        "isExtraImport": true,
        "detail": "bitnet.at",
        "documentation": {}
    },
    {
        "label": "AutoregressiveWrapper",
        "importPath": "bitnet.at",
        "description": "bitnet.at",
        "isExtraImport": true,
        "detail": "bitnet.at",
        "documentation": {}
    },
    {
        "label": "AutoregressiveWrapper",
        "importPath": "bitnet.at",
        "description": "bitnet.at",
        "isExtraImport": true,
        "detail": "bitnet.at",
        "documentation": {}
    },
    {
        "label": "AutoregressiveWrapper",
        "importPath": "bitnet.at",
        "description": "bitnet.at",
        "isExtraImport": true,
        "detail": "bitnet.at",
        "documentation": {}
    },
    {
        "label": "BitNetTransformer",
        "importPath": "bitnet.bit_transformer",
        "description": "bitnet.bit_transformer",
        "isExtraImport": true,
        "detail": "bitnet.bit_transformer",
        "documentation": {}
    },
    {
        "label": "BitNetTransformer",
        "importPath": "bitnet.bit_transformer",
        "description": "bitnet.bit_transformer",
        "isExtraImport": true,
        "detail": "bitnet.bit_transformer",
        "documentation": {}
    },
    {
        "label": "ParallelTransformerBlock",
        "importPath": "bitnet.bit_transformer",
        "description": "bitnet.bit_transformer",
        "isExtraImport": true,
        "detail": "bitnet.bit_transformer",
        "documentation": {}
    },
    {
        "label": "Transformer",
        "importPath": "bitnet.bit_transformer",
        "description": "bitnet.bit_transformer",
        "isExtraImport": true,
        "detail": "bitnet.bit_transformer",
        "documentation": {}
    },
    {
        "label": "BitFeedForward",
        "importPath": "bitnet.bit_transformer",
        "description": "bitnet.bit_transformer",
        "isExtraImport": true,
        "detail": "bitnet.bit_transformer",
        "documentation": {}
    },
    {
        "label": "BitNetTransformer",
        "importPath": "bitnet.bit_transformer",
        "description": "bitnet.bit_transformer",
        "isExtraImport": true,
        "detail": "bitnet.bit_transformer",
        "documentation": {}
    },
    {
        "label": "MultiheadAttention",
        "importPath": "bitnet.bit_transformer",
        "description": "bitnet.bit_transformer",
        "isExtraImport": true,
        "detail": "bitnet.bit_transformer",
        "documentation": {}
    },
    {
        "label": "BitNetTransformer",
        "importPath": "bitnet.bit_transformer",
        "description": "bitnet.bit_transformer",
        "isExtraImport": true,
        "detail": "bitnet.bit_transformer",
        "documentation": {}
    },
    {
        "label": "BitNetTransformer",
        "importPath": "bitnet.bit_transformer",
        "description": "bitnet.bit_transformer",
        "isExtraImport": true,
        "detail": "bitnet.bit_transformer",
        "documentation": {}
    },
    {
        "label": "ParallelTransformerBlock",
        "importPath": "bitnet.bit_transformer",
        "description": "bitnet.bit_transformer",
        "isExtraImport": true,
        "detail": "bitnet.bit_transformer",
        "documentation": {}
    },
    {
        "label": "Transformer",
        "importPath": "bitnet.bit_transformer",
        "description": "bitnet.bit_transformer",
        "isExtraImport": true,
        "detail": "bitnet.bit_transformer",
        "documentation": {}
    },
    {
        "label": "BitFeedForward",
        "importPath": "bitnet.bit_transformer",
        "description": "bitnet.bit_transformer",
        "isExtraImport": true,
        "detail": "bitnet.bit_transformer",
        "documentation": {}
    },
    {
        "label": "BitNetTransformer",
        "importPath": "bitnet.bit_transformer",
        "description": "bitnet.bit_transformer",
        "isExtraImport": true,
        "detail": "bitnet.bit_transformer",
        "documentation": {}
    },
    {
        "label": "MultiheadAttention",
        "importPath": "bitnet.bit_transformer",
        "description": "bitnet.bit_transformer",
        "isExtraImport": true,
        "detail": "bitnet.bit_transformer",
        "documentation": {}
    },
    {
        "label": "pytest",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pytest",
        "description": "pytest",
        "detail": "pytest",
        "documentation": {}
    },
    {
        "label": "torch.nn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn",
        "description": "torch.nn",
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "BitMGQA",
        "importPath": "bitnet",
        "description": "bitnet",
        "isExtraImport": true,
        "detail": "bitnet",
        "documentation": {}
    },
    {
        "label": "BitFeedForward",
        "importPath": "bitnet",
        "description": "bitnet",
        "isExtraImport": true,
        "detail": "bitnet",
        "documentation": {}
    },
    {
        "label": "BitLinearNew",
        "importPath": "bitnet",
        "description": "bitnet",
        "isExtraImport": true,
        "detail": "bitnet",
        "documentation": {}
    },
    {
        "label": "BitMamba",
        "importPath": "bitnet",
        "description": "bitnet",
        "isExtraImport": true,
        "detail": "bitnet",
        "documentation": {}
    },
    {
        "label": "BitLinear",
        "importPath": "bitnet",
        "description": "bitnet",
        "isExtraImport": true,
        "detail": "bitnet",
        "documentation": {}
    },
    {
        "label": "replace_linears_in_hf",
        "importPath": "bitnet",
        "description": "bitnet",
        "isExtraImport": true,
        "detail": "bitnet",
        "documentation": {}
    },
    {
        "label": "BitNetTransformer",
        "importPath": "bitnet",
        "description": "bitnet",
        "isExtraImport": true,
        "detail": "bitnet",
        "documentation": {}
    },
    {
        "label": "BitNetTransformer",
        "importPath": "bitnet",
        "description": "bitnet",
        "isExtraImport": true,
        "detail": "bitnet",
        "documentation": {}
    },
    {
        "label": "BitMGQA",
        "importPath": "bitnet",
        "description": "bitnet",
        "isExtraImport": true,
        "detail": "bitnet",
        "documentation": {}
    },
    {
        "label": "BitFeedForward",
        "importPath": "bitnet",
        "description": "bitnet",
        "isExtraImport": true,
        "detail": "bitnet",
        "documentation": {}
    },
    {
        "label": "BitLinearNew",
        "importPath": "bitnet",
        "description": "bitnet",
        "isExtraImport": true,
        "detail": "bitnet",
        "documentation": {}
    },
    {
        "label": "BitMamba",
        "importPath": "bitnet",
        "description": "bitnet",
        "isExtraImport": true,
        "detail": "bitnet",
        "documentation": {}
    },
    {
        "label": "BitLinear",
        "importPath": "bitnet",
        "description": "bitnet",
        "isExtraImport": true,
        "detail": "bitnet",
        "documentation": {}
    },
    {
        "label": "replace_linears_in_hf",
        "importPath": "bitnet",
        "description": "bitnet",
        "isExtraImport": true,
        "detail": "bitnet",
        "documentation": {}
    },
    {
        "label": "BitNetTransformer",
        "importPath": "bitnet",
        "description": "bitnet",
        "isExtraImport": true,
        "detail": "bitnet",
        "documentation": {}
    },
    {
        "label": "BitNetTransformer",
        "importPath": "bitnet",
        "description": "bitnet",
        "isExtraImport": true,
        "detail": "bitnet",
        "documentation": {}
    },
    {
        "label": "BitMoE",
        "importPath": "bitnet.bit_moe",
        "description": "bitnet.bit_moe",
        "isExtraImport": true,
        "detail": "bitnet.bit_moe",
        "documentation": {}
    },
    {
        "label": "BitMoE",
        "importPath": "bitnet.bit_moe",
        "description": "bitnet.bit_moe",
        "isExtraImport": true,
        "detail": "bitnet.bit_moe",
        "documentation": {}
    },
    {
        "label": "AutoModelForSequenceClassification",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoModelForSequenceClassification",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "AutoTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "gemm_lowbit",
        "importPath": "gemm_lowbit_ext",
        "description": "gemm_lowbit_ext",
        "isExtraImport": true,
        "detail": "gemm_lowbit_ext",
        "documentation": {}
    },
    {
        "label": "gemm_lowbit",
        "importPath": "gemm_lowbit_ext",
        "description": "gemm_lowbit_ext",
        "isExtraImport": true,
        "detail": "gemm_lowbit_ext",
        "documentation": {}
    },
    {
        "label": "ast",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "ast",
        "description": "ast",
        "detail": "ast",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "platform",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "platform",
        "description": "platform",
        "detail": "platform",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "shutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "shutil",
        "description": "shutil",
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "subprocess",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "subprocess",
        "description": "subprocess",
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "urllib.error",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "urllib.error",
        "description": "urllib.error",
        "detail": "urllib.error",
        "documentation": {}
    },
    {
        "label": "urllib.request",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "urllib.request",
        "description": "urllib.request",
        "detail": "urllib.request",
        "documentation": {}
    },
    {
        "label": "warnings",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "warnings",
        "description": "warnings",
        "detail": "warnings",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Version",
        "importPath": "packaging.version",
        "description": "packaging.version",
        "isExtraImport": true,
        "detail": "packaging.version",
        "documentation": {}
    },
    {
        "label": "parse",
        "importPath": "packaging.version",
        "description": "packaging.version",
        "isExtraImport": true,
        "detail": "packaging.version",
        "documentation": {}
    },
    {
        "label": "Version",
        "importPath": "packaging.version",
        "description": "packaging.version",
        "isExtraImport": true,
        "detail": "packaging.version",
        "documentation": {}
    },
    {
        "label": "parse",
        "importPath": "packaging.version",
        "description": "packaging.version",
        "isExtraImport": true,
        "detail": "packaging.version",
        "documentation": {}
    },
    {
        "label": "find_packages",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "find_packages",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "CUDA_HOME",
        "importPath": "torch.utils.cpp_extension",
        "description": "torch.utils.cpp_extension",
        "isExtraImport": true,
        "detail": "torch.utils.cpp_extension",
        "documentation": {}
    },
    {
        "label": "BuildExtension",
        "importPath": "torch.utils.cpp_extension",
        "description": "torch.utils.cpp_extension",
        "isExtraImport": true,
        "detail": "torch.utils.cpp_extension",
        "documentation": {}
    },
    {
        "label": "CUDAExtension",
        "importPath": "torch.utils.cpp_extension",
        "description": "torch.utils.cpp_extension",
        "isExtraImport": true,
        "detail": "torch.utils.cpp_extension",
        "documentation": {}
    },
    {
        "label": "CUDA_HOME",
        "importPath": "torch.utils.cpp_extension",
        "description": "torch.utils.cpp_extension",
        "isExtraImport": true,
        "detail": "torch.utils.cpp_extension",
        "documentation": {}
    },
    {
        "label": "BuildExtension",
        "importPath": "torch.utils.cpp_extension",
        "description": "torch.utils.cpp_extension",
        "isExtraImport": true,
        "detail": "torch.utils.cpp_extension",
        "documentation": {}
    },
    {
        "label": "CUDAExtension",
        "importPath": "torch.utils.cpp_extension",
        "description": "torch.utils.cpp_extension",
        "isExtraImport": true,
        "detail": "torch.utils.cpp_extension",
        "documentation": {}
    },
    {
        "label": "bdist_wheel",
        "importPath": "wheel.bdist_wheel",
        "description": "wheel.bdist_wheel",
        "isExtraImport": true,
        "detail": "wheel.bdist_wheel",
        "documentation": {}
    },
    {
        "label": "bdist_wheel",
        "importPath": "wheel.bdist_wheel",
        "description": "wheel.bdist_wheel",
        "isExtraImport": true,
        "detail": "wheel.bdist_wheel",
        "documentation": {}
    },
    {
        "label": "gzip",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gzip",
        "description": "gzip",
        "detail": "gzip",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tqdm",
        "description": "tqdm",
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "StableAdamWUnfused",
        "importPath": "zeta.optim",
        "description": "zeta.optim",
        "isExtraImport": true,
        "detail": "zeta.optim",
        "documentation": {}
    },
    {
        "label": "StableAdamWUnfused",
        "importPath": "zeta.optim",
        "description": "zeta.optim",
        "isExtraImport": true,
        "detail": "zeta.optim",
        "documentation": {}
    },
    {
        "label": "KAN",
        "importPath": "efficient_kan",
        "description": "efficient_kan",
        "isExtraImport": true,
        "detail": "efficient_kan",
        "documentation": {}
    },
    {
        "label": "KAN",
        "importPath": "efficient_kan",
        "description": "efficient_kan",
        "isExtraImport": true,
        "detail": "efficient_kan",
        "documentation": {}
    },
    {
        "label": "torch.optim",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.optim",
        "description": "torch.optim",
        "detail": "torch.optim",
        "documentation": {}
    },
    {
        "label": "torchvision",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchvision",
        "description": "torchvision",
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "torchvision.transforms",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "MambaConfig",
        "importPath": "mambabyte",
        "description": "mambabyte",
        "isExtraImport": true,
        "detail": "mambabyte",
        "documentation": {}
    },
    {
        "label": "Mamba",
        "importPath": "mambabyte",
        "description": "mambabyte",
        "isExtraImport": true,
        "detail": "mambabyte",
        "documentation": {}
    },
    {
        "label": "AutoregressiveWrapper",
        "kind": 6,
        "importPath": "BitNet-main.BitNet-main.bitnet.at",
        "description": "BitNet-main.BitNet-main.bitnet.at",
        "peekOfCode": "class AutoregressiveWrapper(nn.Module):\n    \"\"\"\n    AutoregressiveWrapper is a wrapper class that adds autoregressive generation functionality to a given neural network.\n    Args:\n        net (nn.Module): The neural network model.\n        max_seq_len (int): The maximum sequence length for generation. Defaults to 2048.\n        pad_value (int): The padding value for generated sequences. Defaults to 0.\n    \"\"\"\n    def __init__(self, net, max_seq_len=2048, pad_value=0):\n        super().__init__()",
        "detail": "BitNet-main.BitNet-main.bitnet.at",
        "documentation": {}
    },
    {
        "label": "exists",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.bitnet.at",
        "description": "BitNet-main.BitNet-main.bitnet.at",
        "peekOfCode": "def exists(val):\n    return val is not None\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner",
        "detail": "BitNet-main.BitNet-main.bitnet.at",
        "documentation": {}
    },
    {
        "label": "eval_decorator",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.bitnet.at",
        "description": "BitNet-main.BitNet-main.bitnet.at",
        "peekOfCode": "def eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n# top k filtering\ndef top_k(logits, thres=0.9):",
        "detail": "BitNet-main.BitNet-main.bitnet.at",
        "documentation": {}
    },
    {
        "label": "top_k",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.bitnet.at",
        "description": "BitNet-main.BitNet-main.bitnet.at",
        "peekOfCode": "def top_k(logits, thres=0.9):\n    k = int((1 - thres) * logits.shape[-1])\n    val, ind = torch.topk(logits, k)\n    probs = torch.full_like(logits, float(\"-inf\"))\n    probs.scatter_(1, ind, val)\n    return probs\nclass AutoregressiveWrapper(nn.Module):\n    \"\"\"\n    AutoregressiveWrapper is a wrapper class that adds autoregressive generation functionality to a given neural network.\n    Args:",
        "detail": "BitNet-main.BitNet-main.bitnet.at",
        "documentation": {}
    },
    {
        "label": "BitLinear",
        "kind": 6,
        "importPath": "BitNet-main.BitNet-main.bitnet.bitlinear",
        "description": "BitNet-main.BitNet-main.bitnet.bitlinear",
        "peekOfCode": "class BitLinear(nn.Linear):\n    \"\"\"\n    Custom linear layer with bit quantization.\n    Args:\n        dim (int): The input dimension of the layer.\n        training (bool, optional): Whether the layer is in training mode or not. Defaults to False.\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n    Attributes:\n        dim (int): The input dimension of the layer.",
        "detail": "BitNet-main.BitNet-main.bitnet.bitlinear",
        "documentation": {}
    },
    {
        "label": "activation_quant",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.bitnet.bitlinear",
        "description": "BitNet-main.BitNet-main.bitnet.bitlinear",
        "peekOfCode": "def activation_quant(x: Tensor):\n    \"\"\"Per token quantization to 8bits. No grouping is needed for quantization\n    Args:\n        x (Tensor): _description_\n    Returns:\n        _type_: _description_\n    \"\"\"\n    scale = 127.0 / x.abs().max(dim=-1, keepdim=True).values.clamp_(min=1e-5)\n    y = (x * scale).round().clamp_(-128, 127) / scale\n    return y",
        "detail": "BitNet-main.BitNet-main.bitnet.bitlinear",
        "documentation": {}
    },
    {
        "label": "weight_quant",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.bitnet.bitlinear",
        "description": "BitNet-main.BitNet-main.bitnet.bitlinear",
        "peekOfCode": "def weight_quant(w: Tensor):\n    scale = w.abs().mean()\n    e = w.mean()\n    u = (w - e).sign() * scale\n    return u\nclass BitLinear(nn.Linear):\n    \"\"\"\n    Custom linear layer with bit quantization.\n    Args:\n        dim (int): The input dimension of the layer.",
        "detail": "BitNet-main.BitNet-main.bitnet.bitlinear",
        "documentation": {}
    },
    {
        "label": "BitMGQA",
        "kind": 6,
        "importPath": "BitNet-main.BitNet-main.bitnet.bit_attention",
        "description": "BitNet-main.BitNet-main.bitnet.bit_attention",
        "peekOfCode": "class BitMGQA(nn.Module):\n    \"\"\"Multi-head grouped query attention (GQA) layer.\n    Reference:\n        \"GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints\"\n        https://arxiv.org/pdf/2305.13245v1.pdf\n    GQA is a variant of multihead attention (MHA) that uses fewer write heads\n    (key / value) than query heads.  GQA can be viewed as a generalization of\n    multi-query attention (MQA), which uses a single write head. GQA and MQA give\n    significant speedups over standard MHA in decoder layers, with minimal loss in\n    accuracy. In the paper, GQA is shown to be more accurate than MQA, while still",
        "detail": "BitNet-main.BitNet-main.bitnet.bit_attention",
        "documentation": {}
    },
    {
        "label": "scaled_dot_product_gqa",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.bitnet.bit_attention",
        "description": "BitNet-main.BitNet-main.bitnet.bit_attention",
        "peekOfCode": "def scaled_dot_product_gqa(\n    query: Tensor,\n    key: Tensor,\n    value: Tensor,\n    dropout: float = 0.0,\n    scale: Optional[float] = None,\n    mask: Optional[Tensor] = None,\n    is_causal: Optional[bool] = None,\n    need_weights: bool = False,\n    average_attn_weights: bool = False,",
        "detail": "BitNet-main.BitNet-main.bitnet.bit_attention",
        "documentation": {}
    },
    {
        "label": "GLU",
        "kind": 6,
        "importPath": "BitNet-main.BitNet-main.bitnet.bit_ffn",
        "description": "BitNet-main.BitNet-main.bitnet.bit_ffn",
        "peekOfCode": "class GLU(nn.Module):\n    \"\"\"\n    Gated Linear Unit (GLU) module.\n    Args:\n        dim_in (int): Input dimension.\n        dim_out (int): Output dimension.\n        activation (Callable): Activation function to be applied to the gate.\n        mult_bias (bool, optional): Whether to multiply the bias term. Defaults to False.\n        linear (Callable, optional): Linear function to be used for projection. Defaults to False.\n    \"\"\"",
        "detail": "BitNet-main.BitNet-main.bitnet.bit_ffn",
        "documentation": {}
    },
    {
        "label": "BitFeedForward",
        "kind": 6,
        "importPath": "BitNet-main.BitNet-main.bitnet.bit_ffn",
        "description": "BitNet-main.BitNet-main.bitnet.bit_ffn",
        "peekOfCode": "class BitFeedForward(nn.Module):\n    \"\"\"\n    BitFeedForward module performs feed-forward operations on the input tensor.\n    Args:\n        dim (int): The input dimension.\n        dim_out (int, optional): The output dimension. If not provided, it is set to the input dimension.\n        mult (int, optional): The multiplier for the inner dimension. Default is 4.\n        glu (bool, optional): Whether to use Gated Linear Unit (GLU) activation. Default is False.\n        glu_mult_bias (bool, optional): Whether to apply bias to the GLU activation. Default is False.\n        swish (bool, optional): Whether to use Swish activation. Default is False.",
        "detail": "BitNet-main.BitNet-main.bitnet.bit_ffn",
        "documentation": {}
    },
    {
        "label": "default",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.bitnet.bit_ffn",
        "description": "BitNet-main.BitNet-main.bitnet.bit_ffn",
        "peekOfCode": "def default(val, d):\n    return val if val is not None else d\ndef init_zero_(tensor):\n    nn.init.constant_(tensor, 0.0)\n# [GLU]\nclass GLU(nn.Module):\n    \"\"\"\n    Gated Linear Unit (GLU) module.\n    Args:\n        dim_in (int): Input dimension.",
        "detail": "BitNet-main.BitNet-main.bitnet.bit_ffn",
        "documentation": {}
    },
    {
        "label": "init_zero_",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.bitnet.bit_ffn",
        "description": "BitNet-main.BitNet-main.bitnet.bit_ffn",
        "peekOfCode": "def init_zero_(tensor):\n    nn.init.constant_(tensor, 0.0)\n# [GLU]\nclass GLU(nn.Module):\n    \"\"\"\n    Gated Linear Unit (GLU) module.\n    Args:\n        dim_in (int): Input dimension.\n        dim_out (int): Output dimension.\n        activation (Callable): Activation function to be applied to the gate.",
        "detail": "BitNet-main.BitNet-main.bitnet.bit_ffn",
        "documentation": {}
    },
    {
        "label": "BitLinearNew",
        "kind": 6,
        "importPath": "BitNet-main.BitNet-main.bitnet.bit_linear_new",
        "description": "BitNet-main.BitNet-main.bitnet.bit_linear_new",
        "peekOfCode": "class BitLinearNew(nn.Linear):\n    \"\"\"\n    Custom linear layer with bit quantization.\n    Args:\n        dim (int): The input dimension of the layer.\n        training (bool, optional): Whether the layer is in training mode or not. Defaults to False.\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n    Attributes:\n        dim (int): The input dimension of the layer.",
        "detail": "BitNet-main.BitNet-main.bitnet.bit_linear_new",
        "documentation": {}
    },
    {
        "label": "activation_quant",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.bitnet.bit_linear_new",
        "description": "BitNet-main.BitNet-main.bitnet.bit_linear_new",
        "peekOfCode": "def activation_quant(x: Tensor):\n    \"\"\"Per token quantization to 8bits. No grouping is needed for quantization\n    Args:\n        x (Tensor): _description_\n    Returns:\n        _type_: _description_\n    \"\"\"\n    scale = 127.0 / x.abs().max(dim=-1, keepdim=True).values.clamp_(min=1e-5)\n    y = (x * scale).round().clamp_(-128, 127) / scale\n    return y",
        "detail": "BitNet-main.BitNet-main.bitnet.bit_linear_new",
        "documentation": {}
    },
    {
        "label": "weight_quant",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.bitnet.bit_linear_new",
        "description": "BitNet-main.BitNet-main.bitnet.bit_linear_new",
        "peekOfCode": "def weight_quant(w: Tensor):\n    scale = w.abs().mean()\n    e = w.mean()\n    u = (w - e).sign() * scale\n    return u\nclass BitLinearNew(nn.Linear):\n    \"\"\"\n    Custom linear layer with bit quantization.\n    Args:\n        dim (int): The input dimension of the layer.",
        "detail": "BitNet-main.BitNet-main.bitnet.bit_linear_new",
        "documentation": {}
    },
    {
        "label": "ModelArgs",
        "kind": 6,
        "importPath": "BitNet-main.BitNet-main.bitnet.bit_llama",
        "description": "BitNet-main.BitNet-main.bitnet.bit_llama",
        "peekOfCode": "class ModelArgs:\n    dim: int = 4096\n    n_layers: int = 32\n    n_heads: int = 32\n    n_kv_heads: Optional[int] = None\n    vocab_size: int = -1  # defined later by tokenizer\n    multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2\n    ffn_dim_multiplier: Optional[float] = None\n    norm_eps: float = 1e-5\n    max_batch_size: int = 32",
        "detail": "BitNet-main.BitNet-main.bitnet.bit_llama",
        "documentation": {}
    },
    {
        "label": "RMSNorm",
        "kind": 6,
        "importPath": "BitNet-main.BitNet-main.bitnet.bit_llama",
        "description": "BitNet-main.BitNet-main.bitnet.bit_llama",
        "peekOfCode": "class RMSNorm(torch.nn.Module):\n    def __init__(self, dim: int, eps: float = 1e-6):\n        \"\"\"\n        Initialize the RMSNorm normalization layer.\n        Args:\n            dim (int): The dimension of the input tensor.\n            eps (float, optional): A small value added to the denominator for numerical stability. Default is 1e-6.\n        Attributes:\n            eps (float): A small value added to the denominator for numerical stability.\n            weight (nn.Parameter): Learnable scaling parameter.",
        "detail": "BitNet-main.BitNet-main.bitnet.bit_llama",
        "documentation": {}
    },
    {
        "label": "Attention",
        "kind": 6,
        "importPath": "BitNet-main.BitNet-main.bitnet.bit_llama",
        "description": "BitNet-main.BitNet-main.bitnet.bit_llama",
        "peekOfCode": "class Attention(nn.Module):\n    \"\"\"Multi-head attention module.\"\"\"\n    def __init__(self, args: ModelArgs):\n        \"\"\"\n        Initialize the Attention module.\n        Args:\n            args (ModelArgs): Model configuration parameters.\n        Attributes:\n            n_kv_heads (int): Number of key and value heads.\n            n_local_heads (int): Number of local query heads.",
        "detail": "BitNet-main.BitNet-main.bitnet.bit_llama",
        "documentation": {}
    },
    {
        "label": "FeedForward",
        "kind": 6,
        "importPath": "BitNet-main.BitNet-main.bitnet.bit_llama",
        "description": "BitNet-main.BitNet-main.bitnet.bit_llama",
        "peekOfCode": "class FeedForward(nn.Module):\n    def __init__(\n        self,\n        dim: int,\n        hidden_dim: int,\n        multiple_of: int,\n        ffn_dim_multiplier: Optional[float],\n    ):\n        \"\"\"\n        Initialize the FeedForward module.",
        "detail": "BitNet-main.BitNet-main.bitnet.bit_llama",
        "documentation": {}
    },
    {
        "label": "TransformerBlock",
        "kind": 6,
        "importPath": "BitNet-main.BitNet-main.bitnet.bit_llama",
        "description": "BitNet-main.BitNet-main.bitnet.bit_llama",
        "peekOfCode": "class TransformerBlock(nn.Module):\n    def __init__(self, layer_id: int, args: ModelArgs):\n        \"\"\"\n        Initialize a TransformerBlock.\n        Args:\n            layer_id (int): Identifier for the layer.\n            args (ModelArgs): Model configuration parameters.\n        Attributes:\n            n_heads (int): Number of attention heads.\n            dim (int): Dimension size of the model.",
        "detail": "BitNet-main.BitNet-main.bitnet.bit_llama",
        "documentation": {}
    },
    {
        "label": "Transformer",
        "kind": 6,
        "importPath": "BitNet-main.BitNet-main.bitnet.bit_llama",
        "description": "BitNet-main.BitNet-main.bitnet.bit_llama",
        "peekOfCode": "class Transformer(nn.Module):\n    def __init__(self, params: ModelArgs):\n        \"\"\"\n        Initialize a Transformer model.\n        Args:\n            params (ModelArgs): Model configuration parameters.\n        Attributes:\n            params (ModelArgs): Model configuration parameters.\n            vocab_size (int): Vocabulary size.\n            n_layers (int): Number of layers in the model.",
        "detail": "BitNet-main.BitNet-main.bitnet.bit_llama",
        "documentation": {}
    },
    {
        "label": "precompute_freqs_cis",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.bitnet.bit_llama",
        "description": "BitNet-main.BitNet-main.bitnet.bit_llama",
        "peekOfCode": "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n    \"\"\"\n    Precompute the frequency tensor for complex exponentials (cis) with given dimensions.\n    This function calculates a frequency tensor with complex exponentials using the given dimension 'dim'\n    and the end index 'end'. The 'theta' parameter scales the frequencies.\n    The returned tensor contains complex values in complex64 data type.\n    Args:\n        dim (int): Dimension of the frequency tensor.\n        end (int): End index for precomputing frequencies.\n        theta (float, optional): Scaling factor for frequency computation. Defaults to 10000.0.",
        "detail": "BitNet-main.BitNet-main.bitnet.bit_llama",
        "documentation": {}
    },
    {
        "label": "reshape_for_broadcast",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.bitnet.bit_llama",
        "description": "BitNet-main.BitNet-main.bitnet.bit_llama",
        "peekOfCode": "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n    \"\"\"\n    Reshape frequency tensor for broadcasting it with another tensor.\n    This function reshapes the frequency tensor to have the same shape as the target tensor 'x'\n    for the purpose of broadcasting the frequency tensor during element-wise operations.\n    Args:\n        freqs_cis (torch.Tensor): Frequency tensor to be reshaped.\n        x (torch.Tensor): Target tensor for broadcasting compatibility.\n    Returns:\n        torch.Tensor: Reshaped frequency tensor.",
        "detail": "BitNet-main.BitNet-main.bitnet.bit_llama",
        "documentation": {}
    },
    {
        "label": "apply_rotary_emb",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.bitnet.bit_llama",
        "description": "BitNet-main.BitNet-main.bitnet.bit_llama",
        "peekOfCode": "def apply_rotary_emb(\n    xq: torch.Tensor,\n    xk: torch.Tensor,\n    freqs_cis: torch.Tensor,\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Apply rotary embeddings to input tensors using the given frequency tensor.\n    This function applies rotary embeddings to the given query 'xq' and key 'xk' tensors using the provided\n    frequency tensor 'freqs_cis'. The input tensors are reshaped as complex numbers, and the frequency tensor\n    is reshaped for broadcasting compatibility. The resulting tensors contain rotary embeddings and are",
        "detail": "BitNet-main.BitNet-main.bitnet.bit_llama",
        "documentation": {}
    },
    {
        "label": "repeat_kv",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.bitnet.bit_llama",
        "description": "BitNet-main.BitNet-main.bitnet.bit_llama",
        "peekOfCode": "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n    \"\"\"torch.repeat_interleave(x, dim=2, repeats=n_rep)\"\"\"\n    bs, slen, n_kv_heads, head_dim = x.shape\n    if n_rep == 1:\n        return x\n    return (\n        x[:, :, :, None, :]\n        .expand(bs, slen, n_kv_heads, n_rep, head_dim)\n        .reshape(bs, slen, n_kv_heads * n_rep, head_dim)\n    )",
        "detail": "BitNet-main.BitNet-main.bitnet.bit_llama",
        "documentation": {}
    },
    {
        "label": "BitLora",
        "kind": 6,
        "importPath": "BitNet-main.BitNet-main.bitnet.bit_lora",
        "description": "BitNet-main.BitNet-main.bitnet.bit_lora",
        "peekOfCode": "class BitLora(BitLinear):\n    \"\"\"\n    BitLora class represents a custom linear layer with LoRa (Low Rank) regularization.\n    Args:\n        rank (int): The rank of the LoRa regularization. Default is 4.\n        lora_alpha (int): The scaling factor for LoRa regularization. Default is 1.\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n    Attributes:\n        rank (int): The rank of the LoRa regularization.",
        "detail": "BitNet-main.BitNet-main.bitnet.bit_lora",
        "documentation": {}
    },
    {
        "label": "weight_quant",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.bitnet.bit_lora",
        "description": "BitNet-main.BitNet-main.bitnet.bit_lora",
        "peekOfCode": "def weight_quant(w: Tensor):\n    scale = w.abs().mean()\n    e = w.mean()\n    u = (w - e).sign() * scale\n    return u\ndef activation_quant(x: Tensor):\n    \"\"\"Per token quantization to 8bits. No grouping is needed for quantization\n    Args:\n        x (Tensor): _description_\n    Returns:",
        "detail": "BitNet-main.BitNet-main.bitnet.bit_lora",
        "documentation": {}
    },
    {
        "label": "activation_quant",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.bitnet.bit_lora",
        "description": "BitNet-main.BitNet-main.bitnet.bit_lora",
        "peekOfCode": "def activation_quant(x: Tensor):\n    \"\"\"Per token quantization to 8bits. No grouping is needed for quantization\n    Args:\n        x (Tensor): _description_\n    Returns:\n        _type_: _description_\n    \"\"\"\n    scale = 127.0 / x.abs().max(dim=-1, keepdim=True).values.clamp_(min=1e-5)\n    y = (x * scale).round().clamp_(-128, 127) / scale\n    return y",
        "detail": "BitNet-main.BitNet-main.bitnet.bit_lora",
        "documentation": {}
    },
    {
        "label": "RMSNorm",
        "kind": 6,
        "importPath": "BitNet-main.BitNet-main.bitnet.bit_mamba",
        "description": "BitNet-main.BitNet-main.bitnet.bit_mamba",
        "peekOfCode": "class RMSNorm(nn.Module):\n    def __init__(self, dim: int, eps: float = 1e-5):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(dim))\n    def forward(self, x):\n        output = (\n            x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight\n        )\n        return output",
        "detail": "BitNet-main.BitNet-main.bitnet.bit_mamba",
        "documentation": {}
    },
    {
        "label": "PScan",
        "kind": 6,
        "importPath": "BitNet-main.BitNet-main.bitnet.bit_mamba",
        "description": "BitNet-main.BitNet-main.bitnet.bit_mamba",
        "peekOfCode": "class PScan(torch.autograd.Function):\n    @staticmethod\n    def pscan(A, X):\n        # A : (B, D, L, N)\n        # X : (B, D, L, N)\n        # modifies X in place by doing a parallel scan.\n        # more formally, X will be populated by these values :\n        # H[t] = A[t] * H[t-1] + X[t] with H[0] = 0\n        # which are computed in parallel (2*log2(T) sequential steps (ideally), instead of T sequential steps)\n        B, D, L, _ = A.size()",
        "detail": "BitNet-main.BitNet-main.bitnet.bit_mamba",
        "documentation": {}
    },
    {
        "label": "MambaConfig",
        "kind": 6,
        "importPath": "BitNet-main.BitNet-main.bitnet.bit_mamba",
        "description": "BitNet-main.BitNet-main.bitnet.bit_mamba",
        "peekOfCode": "class MambaConfig:\n    dim: int  # D\n    depth: int\n    dt_rank: Union[int, str] = \"auto\"\n    d_state: int = 16  # N in paper/comments\n    expand_factor: int = 2  # E in paper/comments\n    d_conv: int = 4\n    dt_min: float = 0.001\n    dt_max: float = 0.1\n    dt_init: str = \"random\"  # \"random\" or \"constant\"",
        "detail": "BitNet-main.BitNet-main.bitnet.bit_mamba",
        "documentation": {}
    },
    {
        "label": "ResidualBlock",
        "kind": 6,
        "importPath": "BitNet-main.BitNet-main.bitnet.bit_mamba",
        "description": "BitNet-main.BitNet-main.bitnet.bit_mamba",
        "peekOfCode": "class ResidualBlock(nn.Module):\n    def __init__(self, config: MambaConfig):\n        super().__init__()\n        self.mixer = MambaBlock(config)\n        self.norm = RMSNorm(config.dim)\n    def forward(self, x):\n        # x : (B, L, D)\n        # output : (B, L, D)\n        output = self.mixer(self.norm(x)) + x\n        return output",
        "detail": "BitNet-main.BitNet-main.bitnet.bit_mamba",
        "documentation": {}
    },
    {
        "label": "MambaBlock",
        "kind": 6,
        "importPath": "BitNet-main.BitNet-main.bitnet.bit_mamba",
        "description": "BitNet-main.BitNet-main.bitnet.bit_mamba",
        "peekOfCode": "class MambaBlock(nn.Module):\n    def __init__(self, config: MambaConfig):\n        super().__init__()\n        self.config = config\n        # projects block input from D to 2*ED (two branches)\n        self.in_proj = BitLinear(config.dim, 2 * config.d_inner, bias=config.bias)\n        self.conv1d = nn.Conv1d(\n            in_channels=config.d_inner,\n            out_channels=config.d_inner,\n            kernel_size=config.d_conv,",
        "detail": "BitNet-main.BitNet-main.bitnet.bit_mamba",
        "documentation": {}
    },
    {
        "label": "Mamba",
        "kind": 6,
        "importPath": "BitNet-main.BitNet-main.bitnet.bit_mamba",
        "description": "BitNet-main.BitNet-main.bitnet.bit_mamba",
        "peekOfCode": "class Mamba(nn.Module):\n    def __init__(\n        self,\n        num_tokens: int,\n        sequence_length: int,\n        config: MambaConfig,\n        return_embeddings: bool = True,\n        return_tokens: bool = True,\n    ):\n        super().__init__()",
        "detail": "BitNet-main.BitNet-main.bitnet.bit_mamba",
        "documentation": {}
    },
    {
        "label": "BitMamba",
        "kind": 6,
        "importPath": "BitNet-main.BitNet-main.bitnet.bit_mamba",
        "description": "BitNet-main.BitNet-main.bitnet.bit_mamba",
        "peekOfCode": "class BitMamba(nn.Module):\n    \"\"\"\n    BitMamba module for performing computations using the BitNet architecture.\n    Args:\n        dim (int): The input dimension (D).\n        depth (int): The depth of the BitNet architecture.\n        dt_rank (Union[int, str], optional): The rank of the time step tensor. Defaults to \"auto\".\n        d_state (int, optional): The dimension of the state tensor (N in paper/comments). Defaults to 16.\n        expand_factor (int, optional): The expansion factor for the inner dimension (E in paper/comments). Defaults to 2.\n        d_conv (int, optional): The dimension of the convolutional filters. Defaults to 4.",
        "detail": "BitNet-main.BitNet-main.bitnet.bit_mamba",
        "documentation": {}
    },
    {
        "label": "pscan",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.bitnet.bit_mamba",
        "description": "BitNet-main.BitNet-main.bitnet.bit_mamba",
        "peekOfCode": "pscan = PScan.apply\n@dataclass\nclass MambaConfig:\n    dim: int  # D\n    depth: int\n    dt_rank: Union[int, str] = \"auto\"\n    d_state: int = 16  # N in paper/comments\n    expand_factor: int = 2  # E in paper/comments\n    d_conv: int = 4\n    dt_min: float = 0.001",
        "detail": "BitNet-main.BitNet-main.bitnet.bit_mamba",
        "documentation": {}
    },
    {
        "label": "Expert",
        "kind": 6,
        "importPath": "BitNet-main.BitNet-main.bitnet.bit_moe",
        "description": "BitNet-main.BitNet-main.bitnet.bit_moe",
        "peekOfCode": "class Expert(nn.Module):\n    \"\"\"An MLP is a simple linear layer followed by a non-linearity i.e. each Expert\n    Args:\n        dim (int): The input dimension of the linear layer.\n        dropout (float, optional): The dropout probability. Defaults to 0.1.\n    Attributes:\n        net (nn.Sequential): The sequential network consisting of linear layers, ReLU activation, and dropout.\n    \"\"\"\n    def __init__(self, dim: int, dropout: int = 0.1):\n        super().__init__()",
        "detail": "BitNet-main.BitNet-main.bitnet.bit_moe",
        "documentation": {}
    },
    {
        "label": "NoisyTopkRouter",
        "kind": 6,
        "importPath": "BitNet-main.BitNet-main.bitnet.bit_moe",
        "description": "BitNet-main.BitNet-main.bitnet.bit_moe",
        "peekOfCode": "class NoisyTopkRouter(nn.Module):\n    \"\"\"\n    A class representing a Noisy Top-k Router module.\n    This module takes the output tensor from a multihead self attention block and performs routing\n    by selecting the top-k experts based on the logits. It adds scaled unit Gaussian noise to the logits\n    and applies softmax to obtain the final router output.\n    Args:\n        dim (int): The input dimension of the tensor.\n        num_experts (int): The number of experts.\n        top_k (int): The number of experts to select.",
        "detail": "BitNet-main.BitNet-main.bitnet.bit_moe",
        "documentation": {}
    },
    {
        "label": "BitMoE",
        "kind": 6,
        "importPath": "BitNet-main.BitNet-main.bitnet.bit_moe",
        "description": "BitNet-main.BitNet-main.bitnet.bit_moe",
        "peekOfCode": "class BitMoE(nn.Module):\n    \"\"\"\n    BitMoE (Bitwise Mixture of Experts) module.\n    Args:\n        dim (int): The input dimension.\n        num_experts (int): The number of experts in the mixture.\n        top_k (int, optional): The number of experts to select for each input. Defaults to 2.\n    \"\"\"\n    def __init__(self, dim: int, num_experts: int, top_k: int = 2):\n        super(BitMoE, self).__init__()",
        "detail": "BitNet-main.BitNet-main.bitnet.bit_moe",
        "documentation": {}
    },
    {
        "label": "#",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.bitnet.bit_moe",
        "description": "BitNet-main.BitNet-main.bitnet.bit_moe",
        "peekOfCode": "# = Linear projection + non linear activation functions like [RELU, GELU, etc] + Dropout[optional] + Normalization[optional, LayerNorm]\n# Expert module\nclass Expert(nn.Module):\n    \"\"\"An MLP is a simple linear layer followed by a non-linearity i.e. each Expert\n    Args:\n        dim (int): The input dimension of the linear layer.\n        dropout (float, optional): The dropout probability. Defaults to 0.1.\n    Attributes:\n        net (nn.Sequential): The sequential network consisting of linear layers, ReLU activation, and dropout.\n    \"\"\"",
        "detail": "BitNet-main.BitNet-main.bitnet.bit_moe",
        "documentation": {}
    },
    {
        "label": "RMSNorm",
        "kind": 6,
        "importPath": "BitNet-main.BitNet-main.bitnet.bit_transformer",
        "description": "BitNet-main.BitNet-main.bitnet.bit_transformer",
        "peekOfCode": "class RMSNorm(nn.Module):\n    \"\"\"\n    Root Mean Square Normalization (RMSNorm) module.\n    Args:\n        dim (int): The input dimension.\n        affine (bool, optional): If True, apply an affine transformation to the normalized output.\n            Default is True.\n    Attributes:\n        scale (float): The scaling factor for the normalized output.\n        gamma (torch.Tensor or float): The learnable parameter for the affine transformation.",
        "detail": "BitNet-main.BitNet-main.bitnet.bit_transformer",
        "documentation": {}
    },
    {
        "label": "Transformer",
        "kind": 6,
        "importPath": "BitNet-main.BitNet-main.bitnet.bit_transformer",
        "description": "BitNet-main.BitNet-main.bitnet.bit_transformer",
        "peekOfCode": "class Transformer(nn.Module):\n    \"\"\"\n    Transformer module that applies multi-head attention and feed-forward layers.\n    Args:\n        dim (int): The dimension of the input and output tensors.\n        heads (int): The number of attention heads.\n        depth (int): The number of transformer layers.\n        ff_mult (int, optional): The multiplier for the hidden dimension in the feed-forward layers.\n            Defaults to 2.\n        *args: Variable length argument list.",
        "detail": "BitNet-main.BitNet-main.bitnet.bit_transformer",
        "documentation": {}
    },
    {
        "label": "BitNetTransformer",
        "kind": 6,
        "importPath": "BitNet-main.BitNet-main.bitnet.bit_transformer",
        "description": "BitNet-main.BitNet-main.bitnet.bit_transformer",
        "peekOfCode": "class BitNetTransformer(nn.Module):\n    \"\"\"\n    BitNetTransformer is a transformer-based model for BitNet.\n    Args:\n        dim (int): The dimension of the token embeddings.\n        depth (int): The number of transformer layers.\n        num_tokens (int): The number of tokens in the vocabulary.\n        heads (int, optional): The number of attention heads in the transformer. Defaults to 8.\n        ff_mult (int, optional): The multiplier for the feed-forward layer dimension. Defaults to 4.\n    Examples:",
        "detail": "BitNet-main.BitNet-main.bitnet.bit_transformer",
        "documentation": {}
    },
    {
        "label": "l2norm",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.bitnet.bit_transformer",
        "description": "BitNet-main.BitNet-main.bitnet.bit_transformer",
        "peekOfCode": "def l2norm(t, dim=-1):\n    return F.normalize(t, dim=dim)\nclass RMSNorm(nn.Module):\n    \"\"\"\n    Root Mean Square Normalization (RMSNorm) module.\n    Args:\n        dim (int): The input dimension.\n        affine (bool, optional): If True, apply an affine transformation to the normalized output.\n            Default is True.\n    Attributes:",
        "detail": "BitNet-main.BitNet-main.bitnet.bit_transformer",
        "documentation": {}
    },
    {
        "label": "BitNetInference",
        "kind": 6,
        "importPath": "BitNet-main.BitNet-main.bitnet.inference",
        "description": "BitNet-main.BitNet-main.bitnet.inference",
        "peekOfCode": "class BitNetInference:\n    \"\"\"\n    A class used to perform inference with the BitNetTransformer model.\n    ...\n    Attributes\n    ----------\n    model : torch.nn.Module\n        an instance of the BitNetTransformer model\n    device : str\n        the device to run the model on ('cpu' or 'cuda')",
        "detail": "BitNet-main.BitNet-main.bitnet.inference",
        "documentation": {}
    },
    {
        "label": "replace_linears_in_hf",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.bitnet.replace_hf",
        "description": "BitNet-main.BitNet-main.bitnet.replace_hf",
        "peekOfCode": "def replace_linears_in_hf(\n    model,\n):\n    \"\"\"\n    Replaces all instances of nn.Linear in the given model with BitLinear15b.\n    Args:\n        model (nn.Module): The model to modify.\n    Returns:\n        None\n    \"\"\"",
        "detail": "BitNet-main.BitNet-main.bitnet.replace_hf",
        "documentation": {}
    },
    {
        "label": "replace_linears_in_pytorch_model",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.bitnet.replace_hf",
        "description": "BitNet-main.BitNet-main.bitnet.replace_hf",
        "peekOfCode": "def replace_linears_in_pytorch_model(\n    model,\n):\n    \"\"\"\n    Replaces all instances of nn.Linear in the given model with BitLinear15b.\n    Args:\n        model (nn.Module): The model to modify.\n    Returns:\n        None\n    \"\"\"",
        "detail": "BitNet-main.BitNet-main.bitnet.replace_hf",
        "documentation": {}
    },
    {
        "label": "test_absmax_quantize",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.tests.tests",
        "description": "BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_absmax_quantize():\n    tensor = torch.tensor([1.5, -2.0, 3.0, -4.0])\n    quant, dequant = absmax_quantize(tensor)\n    assert quant.dtype == torch.int8\n    assert torch.allclose(dequant, tensor, atol=1e-2)\ndef test_bitlinear_initialization():\n    layer = BitLinear(10, 20)\n    assert layer.in_features == 10\n    assert layer.out_features == 20\n    assert layer.weight.shape == (20, 10)",
        "detail": "BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_bitlinear_initialization",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.tests.tests",
        "description": "BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_bitlinear_initialization():\n    layer = BitLinear(10, 20)\n    assert layer.in_features == 10\n    assert layer.out_features == 20\n    assert layer.weight.shape == (20, 10)\ndef test_bitlinear_forward():\n    layer = BitLinear(10, 20)\n    input_tensor = torch.randn(5, 10)\n    output = layer(input_tensor)\n    assert output.shape == (5, 20)",
        "detail": "BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_bitlinear_forward",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.tests.tests",
        "description": "BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_bitlinear_forward():\n    layer = BitLinear(10, 20)\n    input_tensor = torch.randn(5, 10)\n    output = layer(input_tensor)\n    assert output.shape == (5, 20)\n# Fixtures:\n@pytest.fixture\ndef random_tensor():\n    return torch.randn(5, 10)\n# Parameterized Testing:",
        "detail": "BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "random_tensor",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.tests.tests",
        "description": "BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def random_tensor():\n    return torch.randn(5, 10)\n# Parameterized Testing:\n@pytest.mark.parametrize(\"bits\", [4, 8, 12, 16])\ndef test_absmax_quantize_bits(random_tensor, bits):\n    quant, dequant = absmax_quantize(random_tensor, bits=bits)\n    assert quant.dtype == torch.int8\n    assert torch.allclose(dequant, random_tensor, atol=1e-2)\n# More Tests for BitLinear:\n@pytest.mark.parametrize(",
        "detail": "BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_absmax_quantize_bits",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.tests.tests",
        "description": "BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_absmax_quantize_bits(random_tensor, bits):\n    quant, dequant = absmax_quantize(random_tensor, bits=bits)\n    assert quant.dtype == torch.int8\n    assert torch.allclose(dequant, random_tensor, atol=1e-2)\n# More Tests for BitLinear:\n@pytest.mark.parametrize(\n    \"in_features,out_features\", [(10, 20), (20, 40), (5, 10), (15, 10)]\n)\ndef test_bitlinear_shapes(in_features, out_features):\n    layer = BitLinear(in_features, out_features)",
        "detail": "BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_bitlinear_shapes",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.tests.tests",
        "description": "BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_bitlinear_shapes(in_features, out_features):\n    layer = BitLinear(in_features, out_features)\n    assert layer.weight.shape == (out_features, in_features)\n@pytest.mark.parametrize(\"groups\", [1, 2, 5])\ndef test_bitlinear_groups(groups):\n    layer = BitLinear(10, 20, groups=groups)\n    assert layer.groups == groups\ndef test_bitlinear_reset_parameters():\n    layer = BitLinear(10, 20)\n    original_weights = layer.weight.clone()",
        "detail": "BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_bitlinear_groups",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.tests.tests",
        "description": "BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_bitlinear_groups(groups):\n    layer = BitLinear(10, 20, groups=groups)\n    assert layer.groups == groups\ndef test_bitlinear_reset_parameters():\n    layer = BitLinear(10, 20)\n    original_weights = layer.weight.clone()\n    layer.reset_parameters()\n    assert not torch.equal(original_weights, layer.weight)\n@pytest.mark.parametrize(\"groups\", [1, 2, 5])\ndef test_bitlinear_forward_with_groups(random_tensor, groups):",
        "detail": "BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_bitlinear_reset_parameters",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.tests.tests",
        "description": "BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_bitlinear_reset_parameters():\n    layer = BitLinear(10, 20)\n    original_weights = layer.weight.clone()\n    layer.reset_parameters()\n    assert not torch.equal(original_weights, layer.weight)\n@pytest.mark.parametrize(\"groups\", [1, 2, 5])\ndef test_bitlinear_forward_with_groups(random_tensor, groups):\n    layer = BitLinear(10, 20, groups=groups)\n    output = layer(random_tensor)\n    assert output.shape == (5, 20)",
        "detail": "BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_bitlinear_forward_with_groups",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.tests.tests",
        "description": "BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_bitlinear_forward_with_groups(random_tensor, groups):\n    layer = BitLinear(10, 20, groups=groups)\n    output = layer(random_tensor)\n    assert output.shape == (5, 20)\ndef test_bitlinear_zero_input():\n    layer = BitLinear(10, 20)\n    input_tensor = torch.zeros(5, 10)\n    output = layer(input_tensor)\n    assert torch.allclose(output, torch.zeros(5, 20), atol=1e-2)\ndef test_bitlinear_weight_sign():",
        "detail": "BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_bitlinear_zero_input",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.tests.tests",
        "description": "BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_bitlinear_zero_input():\n    layer = BitLinear(10, 20)\n    input_tensor = torch.zeros(5, 10)\n    output = layer(input_tensor)\n    assert torch.allclose(output, torch.zeros(5, 20), atol=1e-2)\ndef test_bitlinear_weight_sign():\n    layer = BitLinear(10, 20)\n    input_tensor = torch.randn(5, 10)\n    output_before = layer(input_tensor)\n    layer.weight.data = torch.abs(layer.weight.data)",
        "detail": "BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_bitlinear_weight_sign",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.tests.tests",
        "description": "BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_bitlinear_weight_sign():\n    layer = BitLinear(10, 20)\n    input_tensor = torch.randn(5, 10)\n    output_before = layer(input_tensor)\n    layer.weight.data = torch.abs(layer.weight.data)\n    output_after = layer(input_tensor)\n    assert not torch.allclose(output_before, output_after)\n@pytest.mark.parametrize(\"groups\", [1, 2, 5])\ndef test_bitlinear_weight_group_normalization(groups):\n    layer = BitLinear(10, 20, groups=groups)",
        "detail": "BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_bitlinear_weight_group_normalization",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.tests.tests",
        "description": "BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_bitlinear_weight_group_normalization(groups):\n    layer = BitLinear(10, 20, groups=groups)\n    weight = layer.weight.view(groups, -1)\n    mean = weight.mean(dim=1, keepdim=True)\n    assert torch.allclose(mean, torch.zeros_like(mean), atol=1e-2)\ndef test_bitlinear_weight_group_scaling():\n    layer = BitLinear(10, 20, groups=5)\n    weight = layer.weight.view(layer.groups, -1)\n    beta = torch.abs(weight).sum(dim=1, keepdim=True) / (\n        weight.shape[0] * weight.shape[1]",
        "detail": "BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_bitlinear_weight_group_scaling",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.tests.tests",
        "description": "BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_bitlinear_weight_group_scaling():\n    layer = BitLinear(10, 20, groups=5)\n    weight = layer.weight.view(layer.groups, -1)\n    beta = torch.abs(weight).sum(dim=1, keepdim=True) / (\n        weight.shape[0] * weight.shape[1]\n    )\n    scaled_weight = weight * beta\n    assert torch.allclose(scaled_weight, layer.weight.view(20, 10))\ndef test_bitlinear_input_quantization(random_tensor):\n    layer = BitLinear(10, 20)",
        "detail": "BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_bitlinear_input_quantization",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.tests.tests",
        "description": "BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_bitlinear_input_quantization(random_tensor):\n    layer = BitLinear(10, 20)\n    quant_input, _ = absmax_quantize(random_tensor)\n    output = layer(quant_input.float())\n    assert output.shape == (5, 20)\n# ... Continue adding more tests ...\n# - Test the forward pass with extreme input values.\n# - Test with different types of input tensors (e.g., int8, float16).\n# - Test the forward pass with batch sizes other than 5.\n# - Verify that using different initializations produces different results.",
        "detail": "BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "random_tensor",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.tests.tests",
        "description": "BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def random_tensor():\n    \"\"\"A fixture to generate a random tensor\"\"\"\n    return torch.randn(32, 512)\n@pytest.fixture\ndef bitnet_model():\n    \"\"\"A fixture to create an instance of BitNetTransformer model\"\"\"\n    return BitNetTransformer(\n        num_tokens=20000,\n        dim=512,\n        depth=6,",
        "detail": "BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "bitnet_model",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.tests.tests",
        "description": "BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def bitnet_model():\n    \"\"\"A fixture to create an instance of BitNetTransformer model\"\"\"\n    return BitNetTransformer(\n        num_tokens=20000,\n        dim=512,\n        depth=6,\n        dim_head=64,\n        heads=8,\n        ff_mult=4,\n    )",
        "detail": "BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_parallel_transformer_block",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.tests.tests",
        "description": "BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_parallel_transformer_block(dim, dim_head, heads, ff_mult, random_tensor):\n    block = ParallelTransformerBlock(dim, dim_head, heads, ff_mult)\n    output = block(random_tensor)\n    assert output.shape == random_tensor.shape\n@pytest.mark.parametrize(\n    \"dim, depth, heads, dim_head, ff_mult\",\n    [\n        (512, 6, 8, 64, 4),\n        (256, 3, 4, 32, 2),\n        (128, 2, 2, 16, 1),",
        "detail": "BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_transformer",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.tests.tests",
        "description": "BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_transformer(dim, depth, heads, dim_head, ff_mult, random_tensor):\n    transformer = Transformer(dim, depth, heads, dim_head, ff_mult)\n    output = transformer(random_tensor)\n    assert output.shape == random_tensor.shape\ndef test_bitnet_transformer_forward(bitnet_model):\n    tokens = torch.randint(0, 20000, (1, 512))\n    logits = bitnet_model(tokens)\n    assert logits.shape == (1, 20000)\ndef test_parallel_transformer_block_masking(random_tensor):\n    block = ParallelTransformerBlock(512, 64, 8, 4)",
        "detail": "BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_bitnet_transformer_forward",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.tests.tests",
        "description": "BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_bitnet_transformer_forward(bitnet_model):\n    tokens = torch.randint(0, 20000, (1, 512))\n    logits = bitnet_model(tokens)\n    assert logits.shape == (1, 20000)\ndef test_parallel_transformer_block_masking(random_tensor):\n    block = ParallelTransformerBlock(512, 64, 8, 4)\n    mask1 = block.get_mask(100, random_tensor.device)\n    mask2 = block.get_mask(200, random_tensor.device)\n    assert mask1.shape == (100, 100)\n    assert mask2.shape == (200, 200)",
        "detail": "BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_parallel_transformer_block_masking",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.tests.tests",
        "description": "BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_parallel_transformer_block_masking(random_tensor):\n    block = ParallelTransformerBlock(512, 64, 8, 4)\n    mask1 = block.get_mask(100, random_tensor.device)\n    mask2 = block.get_mask(200, random_tensor.device)\n    assert mask1.shape == (100, 100)\n    assert mask2.shape == (200, 200)\ndef test_bitnet_transformer_embed(bitnet_model):\n    tokens = torch.randint(0, 20000, (1, 512))\n    embedded = bitnet_model.emb(tokens)\n    assert embedded.shape == (1, 512, 512)",
        "detail": "BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_bitnet_transformer_embed",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.tests.tests",
        "description": "BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_bitnet_transformer_embed(bitnet_model):\n    tokens = torch.randint(0, 20000, (1, 512))\n    embedded = bitnet_model.emb(tokens)\n    assert embedded.shape == (1, 512, 512)\n@pytest.mark.parametrize(\n    \"dim, dim_head, heads, ff_mult\",\n    [\n        (512, 64, 8, 4),\n        (256, 32, 4, 2),\n        (128, 16, 2, 1),",
        "detail": "BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_parallel_transformer_block_raises_for_incorrect_input",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.tests.tests",
        "description": "BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_parallel_transformer_block_raises_for_incorrect_input(\n    dim, dim_head, heads, ff_mult\n):\n    block = ParallelTransformerBlock(dim, dim_head, heads, ff_mult)\n    with pytest.raises(Exception):\n        block(torch.randn(32, 100))\n@pytest.mark.parametrize(\n    \"batch_size, seq_len\",\n    [\n        (1, 512),",
        "detail": "BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_bitnet_transformer_for_various_input_shapes",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.tests.tests",
        "description": "BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_bitnet_transformer_for_various_input_shapes(bitnet_model, batch_size, seq_len):\n    tokens = torch.randint(0, 20000, (batch_size, seq_len))\n    logits = bitnet_model(tokens)\n    assert logits.shape == (batch_size, 20000)\ndef test_rotary_embedding(bitnet_model, random_tensor):\n    block = ParallelTransformerBlock(512, 64, 8, 4)\n    rotary_emb1 = block.get_rotary_embedding(100, random_tensor.device)\n    rotary_emb2 = block.get_rotary_embedding(200, random_tensor.device)\n    assert rotary_emb1.shape == (100, 64)\n    assert rotary_emb2.shape == (200, 64)",
        "detail": "BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_rotary_embedding",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.tests.tests",
        "description": "BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_rotary_embedding(bitnet_model, random_tensor):\n    block = ParallelTransformerBlock(512, 64, 8, 4)\n    rotary_emb1 = block.get_rotary_embedding(100, random_tensor.device)\n    rotary_emb2 = block.get_rotary_embedding(200, random_tensor.device)\n    assert rotary_emb1.shape == (100, 64)\n    assert rotary_emb2.shape == (200, 64)\n@pytest.mark.parametrize(\"mask_value\", [100, 200, 300])\ndef test_mask_persistency(random_tensor, mask_value):\n    block = ParallelTransformerBlock(512, 64, 8, 4)\n    block.get_mask(mask_value, random_tensor.device)",
        "detail": "BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_mask_persistency",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.tests.tests",
        "description": "BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_mask_persistency(random_tensor, mask_value):\n    block = ParallelTransformerBlock(512, 64, 8, 4)\n    block.get_mask(mask_value, random_tensor.device)\n    assert block.mask.shape[0] == mask_value\n@pytest.mark.parametrize(\n    \"input_value, expected_output_shape\",\n    [\n        (torch.randint(0, 20000, (1, 512)), (1, 20000)),\n        (torch.randint(0, 20000, (32, 256)), (32, 20000)),\n    ],",
        "detail": "BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_bitnet_transformer_output_shapes",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.tests.tests",
        "description": "BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_bitnet_transformer_output_shapes(\n    bitnet_model, input_value, expected_output_shape\n):\n    logits = bitnet_model(input_value)\n    assert logits.shape == expected_output_shape\ndef test_exceptions_on_wrong_dtype():\n    block = ParallelTransformerBlock(512, 64, 8, 4)\n    with pytest.raises(Exception):\n        block(torch.randn(32, 512).int())\ndef test_bitnet_transformer_logit_values(bitnet_model):",
        "detail": "BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_exceptions_on_wrong_dtype",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.tests.tests",
        "description": "BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_exceptions_on_wrong_dtype():\n    block = ParallelTransformerBlock(512, 64, 8, 4)\n    with pytest.raises(Exception):\n        block(torch.randn(32, 512).int())\ndef test_bitnet_transformer_logit_values(bitnet_model):\n    tokens = torch.randint(0, 20000, (1, 512))\n    logits = bitnet_model(tokens)\n    probs = F.softmax(logits, dim=-1)\n    assert torch.allclose(probs.sum(dim=-1), torch.tensor(1.0))\n# Mocking and Monkeypatching",
        "detail": "BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_bitnet_transformer_logit_values",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.tests.tests",
        "description": "BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_bitnet_transformer_logit_values(bitnet_model):\n    tokens = torch.randint(0, 20000, (1, 512))\n    logits = bitnet_model(tokens)\n    probs = F.softmax(logits, dim=-1)\n    assert torch.allclose(probs.sum(dim=-1), torch.tensor(1.0))\n# Mocking and Monkeypatching\ndef test_mocking_get_mask(monkeypatch, random_tensor):\n    mock_mask = torch.zeros(100, 100)\n    monkeypatch.setattr(\n        ParallelTransformerBlock, \"get_mask\", lambda self, n, device: mock_mask",
        "detail": "BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_mocking_get_mask",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.tests.tests",
        "description": "BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_mocking_get_mask(monkeypatch, random_tensor):\n    mock_mask = torch.zeros(100, 100)\n    monkeypatch.setattr(\n        ParallelTransformerBlock, \"get_mask\", lambda self, n, device: mock_mask\n    )\n    block = ParallelTransformerBlock(512, 64, 8, 4)\n    assert torch.equal(block.get_mask(100, random_tensor.device), mock_mask)\n# Add more tests based on the scenarios and edge cases you want to cover.",
        "detail": "BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_bitfeedforward_initialization",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.tests.test_bitffn",
        "description": "BitNet-main.BitNet-main.tests.test_bitffn",
        "peekOfCode": "def test_bitfeedforward_initialization():\n    bitffn = BitFeedForward(dim=512, ff_mult=4)\n    assert isinstance(bitffn.layer, nn.Sequential)\n    assert len(bitffn.layer) == 3\n    assert isinstance(bitffn.layer[0], BitLinear)\n    assert isinstance(bitffn.layer[1], nn.GELU)\n    assert isinstance(bitffn.layer[2], BitLinear)\n    assert bitffn.layer[0].out_features == 512 * 4\n    assert bitffn.layer[2].in_features == 512 * 4\ndef test_bitfeedforward_forward_pass():",
        "detail": "BitNet-main.BitNet-main.tests.test_bitffn",
        "documentation": {}
    },
    {
        "label": "test_bitfeedforward_forward_pass",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.tests.test_bitffn",
        "description": "BitNet-main.BitNet-main.tests.test_bitffn",
        "peekOfCode": "def test_bitfeedforward_forward_pass():\n    bitffn = BitFeedForward(dim=512, ff_mult=4)\n    x = torch.randn(1, 512)\n    out = bitffn(x)\n    assert out.shape == x.shape",
        "detail": "BitNet-main.BitNet-main.tests.test_bitffn",
        "documentation": {}
    },
    {
        "label": "test_bitlinear_initialization",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.tests.test_bitlinear",
        "description": "BitNet-main.BitNet-main.tests.test_bitlinear",
        "peekOfCode": "def test_bitlinear_initialization():\n    bitlinear = BitLinear(in_features=512, out_features=256, bias=True)\n    assert bitlinear.in_features == 512\n    assert bitlinear.out_features == 256\n    assert bitlinear.weight.shape == (256, 512)\n    assert bitlinear.bias.shape == (256,)\n    assert bitlinear.gamma.shape == (512,)\n    assert bitlinear.beta.shape == (256,)\ndef test_bitlinear_forward_pass():\n    bitlinear = BitLinear(in_features=512, out_features=256, bias=True)",
        "detail": "BitNet-main.BitNet-main.tests.test_bitlinear",
        "documentation": {}
    },
    {
        "label": "test_bitlinear_forward_pass",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.tests.test_bitlinear",
        "description": "BitNet-main.BitNet-main.tests.test_bitlinear",
        "peekOfCode": "def test_bitlinear_forward_pass():\n    bitlinear = BitLinear(in_features=512, out_features=256, bias=True)\n    x = torch.randn(1, 512)\n    out = bitlinear(x)\n    assert out.shape == (1, 256)\ndef test_bitlinear_no_bias():\n    bitlinear = BitLinear(in_features=512, out_features=256, bias=False)\n    assert bitlinear.bias is None\ndef test_bitlinear_quantization():\n    bitlinear = BitLinear(in_features=512, out_features=256, bias=True)",
        "detail": "BitNet-main.BitNet-main.tests.test_bitlinear",
        "documentation": {}
    },
    {
        "label": "test_bitlinear_no_bias",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.tests.test_bitlinear",
        "description": "BitNet-main.BitNet-main.tests.test_bitlinear",
        "peekOfCode": "def test_bitlinear_no_bias():\n    bitlinear = BitLinear(in_features=512, out_features=256, bias=False)\n    assert bitlinear.bias is None\ndef test_bitlinear_quantization():\n    bitlinear = BitLinear(in_features=512, out_features=256, bias=True)\n    x = torch.randn(1, 512)\n    out = bitlinear(x)\n    assert torch.all(out <= bitlinear.beta.unsqueeze(0).expand_as(out))\n    assert torch.all(out >= -bitlinear.beta.unsqueeze(0).expand_as(out))",
        "detail": "BitNet-main.BitNet-main.tests.test_bitlinear",
        "documentation": {}
    },
    {
        "label": "test_bitlinear_quantization",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.tests.test_bitlinear",
        "description": "BitNet-main.BitNet-main.tests.test_bitlinear",
        "peekOfCode": "def test_bitlinear_quantization():\n    bitlinear = BitLinear(in_features=512, out_features=256, bias=True)\n    x = torch.randn(1, 512)\n    out = bitlinear(x)\n    assert torch.all(out <= bitlinear.beta.unsqueeze(0).expand_as(out))\n    assert torch.all(out >= -bitlinear.beta.unsqueeze(0).expand_as(out))",
        "detail": "BitNet-main.BitNet-main.tests.test_bitlinear",
        "documentation": {}
    },
    {
        "label": "test_bitnet_transformer_initialization",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.tests.test_transformer",
        "description": "BitNet-main.BitNet-main.tests.test_transformer",
        "peekOfCode": "def test_bitnet_transformer_initialization():\n    bitnet = BitNetTransformer(num_tokens=20000, dim=512, heads=8, depth=6, ff_mult=4)\n    assert len(bitnet.layers) == 6\n    assert len(bitnet.ffn_layers) == 6\n    assert all(isinstance(layer, MultiheadAttention) for layer in bitnet.layers)\n    assert all(isinstance(layer, BitFeedForward) for layer in bitnet.ffn_layers)\ndef test_bitnet_transformer_forward_pass():\n    bitnet = BitNetTransformer(num_tokens=20000, dim=512, heads=8, depth=6, ff_mult=4)\n    x = torch.randn(1, 100, 512)\n    out = bitnet(x)",
        "detail": "BitNet-main.BitNet-main.tests.test_transformer",
        "documentation": {}
    },
    {
        "label": "test_bitnet_transformer_forward_pass",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.tests.test_transformer",
        "description": "BitNet-main.BitNet-main.tests.test_transformer",
        "peekOfCode": "def test_bitnet_transformer_forward_pass():\n    bitnet = BitNetTransformer(num_tokens=20000, dim=512, heads=8, depth=6, ff_mult=4)\n    x = torch.randn(1, 100, 512)\n    out = bitnet(x)\n    assert out.shape == x.shape",
        "detail": "BitNet-main.BitNet-main.tests.test_transformer",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.bit_attention",
        "description": "BitNet-main.BitNet-main.bit_attention",
        "peekOfCode": "x = torch.randn(1, 10, 512)\n# Create an instance of the BitMGQA model with input size 512, 8 attention heads, and 4 layers\ngqa = BitMGQA(512, 8, 4)\n# Pass the input tensor through the BitMGQA model and get the output and attention weights\nout, _ = gqa(x, x, x, need_weights=True)\n# Print the shapes of the output tensor and attention tensor\nprint(out)",
        "detail": "BitNet-main.BitNet-main.bit_attention",
        "documentation": {}
    },
    {
        "label": "gqa",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.bit_attention",
        "description": "BitNet-main.BitNet-main.bit_attention",
        "peekOfCode": "gqa = BitMGQA(512, 8, 4)\n# Pass the input tensor through the BitMGQA model and get the output and attention weights\nout, _ = gqa(x, x, x, need_weights=True)\n# Print the shapes of the output tensor and attention tensor\nprint(out)",
        "detail": "BitNet-main.BitNet-main.bit_attention",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.bit_ffn",
        "description": "BitNet-main.BitNet-main.bit_ffn",
        "peekOfCode": "x = torch.randn(10, 512)\n# Create an instance of the BitFeedForward class with the following parameters:\n# - input_dim: 512\n# - hidden_dim: 512\n# - num_layers: 4\n# - swish: True (use Swish activation function)\n# - post_act_ln: True (apply Layer Normalization after each activation)\n# - dropout: 0.1 (apply dropout with a probability of 0.1)\nff = BitFeedForward(512, 512, 4, swish=True, post_act_ln=True, dropout=0.1)\n# Apply the BitFeedForward network to the input tensor x",
        "detail": "BitNet-main.BitNet-main.bit_ffn",
        "documentation": {}
    },
    {
        "label": "ff",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.bit_ffn",
        "description": "BitNet-main.BitNet-main.bit_ffn",
        "peekOfCode": "ff = BitFeedForward(512, 512, 4, swish=True, post_act_ln=True, dropout=0.1)\n# Apply the BitFeedForward network to the input tensor x\ny = ff(x)\n# Print the shape of the output tensor y\nprint(y)  # torch.Size([10, 512])",
        "detail": "BitNet-main.BitNet-main.bit_ffn",
        "documentation": {}
    },
    {
        "label": "y",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.bit_ffn",
        "description": "BitNet-main.BitNet-main.bit_ffn",
        "peekOfCode": "y = ff(x)\n# Print the shape of the output tensor y\nprint(y)  # torch.Size([10, 512])",
        "detail": "BitNet-main.BitNet-main.bit_ffn",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.bit_linear_new",
        "description": "BitNet-main.BitNet-main.bit_linear_new",
        "peekOfCode": "x = torch.randn(16, 1000, 512)\n# Create an instance of the BitLinearNew class with input size 10, output size 20, and 2 groups\nlayer = BitLinearNew(\n    512,\n    20,\n)\n# Perform a forward pass through the BitLinearNew layer with input x\noutput = layer(x)\n# Print the output tensor\nprint(output)",
        "detail": "BitNet-main.BitNet-main.bit_linear_new",
        "documentation": {}
    },
    {
        "label": "layer",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.bit_linear_new",
        "description": "BitNet-main.BitNet-main.bit_linear_new",
        "peekOfCode": "layer = BitLinearNew(\n    512,\n    20,\n)\n# Perform a forward pass through the BitLinearNew layer with input x\noutput = layer(x)\n# Print the output tensor\nprint(output)\nprint(output.shape)",
        "detail": "BitNet-main.BitNet-main.bit_linear_new",
        "documentation": {}
    },
    {
        "label": "output",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.bit_linear_new",
        "description": "BitNet-main.BitNet-main.bit_linear_new",
        "peekOfCode": "output = layer(x)\n# Print the output tensor\nprint(output)\nprint(output.shape)",
        "detail": "BitNet-main.BitNet-main.bit_linear_new",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.bit_mamba",
        "description": "BitNet-main.BitNet-main.bit_mamba",
        "peekOfCode": "x = torch.randint(0, 100, (2, 10))\n# Create an instance of the BitMamba model with input size 512, hidden size 100, output size 10, and depth size 6\nmodel = BitMamba(512, 100, 10, 6, return_tokens=True)\n# Pass the input tensor through the model and get the output\noutput = model(x)\n# Print the output tensor\nprint(output)\n# Print the shape of the output tensor\nprint(output.shape)",
        "detail": "BitNet-main.BitNet-main.bit_mamba",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.bit_mamba",
        "description": "BitNet-main.BitNet-main.bit_mamba",
        "peekOfCode": "model = BitMamba(512, 100, 10, 6, return_tokens=True)\n# Pass the input tensor through the model and get the output\noutput = model(x)\n# Print the output tensor\nprint(output)\n# Print the shape of the output tensor\nprint(output.shape)",
        "detail": "BitNet-main.BitNet-main.bit_mamba",
        "documentation": {}
    },
    {
        "label": "output",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.bit_mamba",
        "description": "BitNet-main.BitNet-main.bit_mamba",
        "peekOfCode": "output = model(x)\n# Print the output tensor\nprint(output)\n# Print the shape of the output tensor\nprint(output.shape)",
        "detail": "BitNet-main.BitNet-main.bit_mamba",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.bit_moe_example",
        "description": "BitNet-main.BitNet-main.bit_moe_example",
        "peekOfCode": "x = torch.randn(2, 4, 8)\n# Create BitMoE model with specified input and output dimensions\nmodel = BitMoE(8, 4, 2)\n# Forward pass through the model\noutput = model(x)\n# Print the output\nprint(output)",
        "detail": "BitNet-main.BitNet-main.bit_moe_example",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.bit_moe_example",
        "description": "BitNet-main.BitNet-main.bit_moe_example",
        "peekOfCode": "model = BitMoE(8, 4, 2)\n# Forward pass through the model\noutput = model(x)\n# Print the output\nprint(output)",
        "detail": "BitNet-main.BitNet-main.bit_moe_example",
        "documentation": {}
    },
    {
        "label": "output",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.bit_moe_example",
        "description": "BitNet-main.BitNet-main.bit_moe_example",
        "peekOfCode": "output = model(x)\n# Print the output\nprint(output)",
        "detail": "BitNet-main.BitNet-main.bit_moe_example",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.example",
        "description": "BitNet-main.BitNet-main.example",
        "peekOfCode": "x = torch.randn(10, 10000, 512)\n# BitLinear layer\nlayer = BitLinear(512, 400)\n# Output\ny = layer(x)\nprint(y)\nprint(y.shape)",
        "detail": "BitNet-main.BitNet-main.example",
        "documentation": {}
    },
    {
        "label": "layer",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.example",
        "description": "BitNet-main.BitNet-main.example",
        "peekOfCode": "layer = BitLinear(512, 400)\n# Output\ny = layer(x)\nprint(y)\nprint(y.shape)",
        "detail": "BitNet-main.BitNet-main.example",
        "documentation": {}
    },
    {
        "label": "y",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.example",
        "description": "BitNet-main.BitNet-main.example",
        "peekOfCode": "y = layer(x)\nprint(y)\nprint(y.shape)",
        "detail": "BitNet-main.BitNet-main.example",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.huggingface_example",
        "description": "BitNet-main.BitNet-main.huggingface_example",
        "peekOfCode": "model_name = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\n# Replace Linear layers with BitLinear\nreplace_linears_in_hf(model)\n# Example text to classify\ntext = \"Replace this with your text\"\ninputs = tokenizer(\n    text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512\n)",
        "detail": "BitNet-main.BitNet-main.huggingface_example",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.huggingface_example",
        "description": "BitNet-main.BitNet-main.huggingface_example",
        "peekOfCode": "tokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\n# Replace Linear layers with BitLinear\nreplace_linears_in_hf(model)\n# Example text to classify\ntext = \"Replace this with your text\"\ninputs = tokenizer(\n    text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512\n)\n# Perform inference",
        "detail": "BitNet-main.BitNet-main.huggingface_example",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.huggingface_example",
        "description": "BitNet-main.BitNet-main.huggingface_example",
        "peekOfCode": "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n# Replace Linear layers with BitLinear\nreplace_linears_in_hf(model)\n# Example text to classify\ntext = \"Replace this with your text\"\ninputs = tokenizer(\n    text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512\n)\n# Perform inference\nmodel.eval()  # Set the model to evaluation mode",
        "detail": "BitNet-main.BitNet-main.huggingface_example",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.huggingface_example",
        "description": "BitNet-main.BitNet-main.huggingface_example",
        "peekOfCode": "text = \"Replace this with your text\"\ninputs = tokenizer(\n    text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512\n)\n# Perform inference\nmodel.eval()  # Set the model to evaluation mode\nwith torch.no_grad():\n    outputs = model(**inputs)\n    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n    print(predictions)",
        "detail": "BitNet-main.BitNet-main.huggingface_example",
        "documentation": {}
    },
    {
        "label": "inputs",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.huggingface_example",
        "description": "BitNet-main.BitNet-main.huggingface_example",
        "peekOfCode": "inputs = tokenizer(\n    text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512\n)\n# Perform inference\nmodel.eval()  # Set the model to evaluation mode\nwith torch.no_grad():\n    outputs = model(**inputs)\n    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n    print(predictions)\n# Process predictions",
        "detail": "BitNet-main.BitNet-main.huggingface_example",
        "documentation": {}
    },
    {
        "label": "predicted_class_id",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.huggingface_example",
        "description": "BitNet-main.BitNet-main.huggingface_example",
        "peekOfCode": "predicted_class_id = predictions.argmax().item()\nprint(f\"Predicted class ID: {predicted_class_id}\")\n# Optionally, map the predicted class ID to a label, if you know the classification labels\n# labels = [\"Label 1\", \"Label 2\", ...]  # Define your labels corresponding to the model's classes\n# print(f\"Predicted label: {labels[predicted_class_id]}\")",
        "detail": "BitNet-main.BitNet-main.huggingface_example",
        "documentation": {}
    },
    {
        "label": "a",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.kernel_test",
        "description": "BitNet-main.BitNet-main.kernel_test",
        "peekOfCode": "a = torch.randn(10, 20, dtype=torch.half, device=\"cuda\")  # Example tensor\nb = torch.randn(20, 30, dtype=torch.half, device=\"cuda\")  # Example tensor\nc = torch.empty(10, 30, dtype=torch.half, device=\"cuda\")  # Output tensor\nw_scale = 1.0  # Example scale factor\nx_scale = 1.0  # Example scale factor\n# Call the custom CUDA GEMM operation\ngemm_lowbit(a, b, c, w_scale, x_scale)\nprint(c)  # View the result",
        "detail": "BitNet-main.BitNet-main.kernel_test",
        "documentation": {}
    },
    {
        "label": "b",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.kernel_test",
        "description": "BitNet-main.BitNet-main.kernel_test",
        "peekOfCode": "b = torch.randn(20, 30, dtype=torch.half, device=\"cuda\")  # Example tensor\nc = torch.empty(10, 30, dtype=torch.half, device=\"cuda\")  # Output tensor\nw_scale = 1.0  # Example scale factor\nx_scale = 1.0  # Example scale factor\n# Call the custom CUDA GEMM operation\ngemm_lowbit(a, b, c, w_scale, x_scale)\nprint(c)  # View the result",
        "detail": "BitNet-main.BitNet-main.kernel_test",
        "documentation": {}
    },
    {
        "label": "c",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.kernel_test",
        "description": "BitNet-main.BitNet-main.kernel_test",
        "peekOfCode": "c = torch.empty(10, 30, dtype=torch.half, device=\"cuda\")  # Output tensor\nw_scale = 1.0  # Example scale factor\nx_scale = 1.0  # Example scale factor\n# Call the custom CUDA GEMM operation\ngemm_lowbit(a, b, c, w_scale, x_scale)\nprint(c)  # View the result",
        "detail": "BitNet-main.BitNet-main.kernel_test",
        "documentation": {}
    },
    {
        "label": "w_scale",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.kernel_test",
        "description": "BitNet-main.BitNet-main.kernel_test",
        "peekOfCode": "w_scale = 1.0  # Example scale factor\nx_scale = 1.0  # Example scale factor\n# Call the custom CUDA GEMM operation\ngemm_lowbit(a, b, c, w_scale, x_scale)\nprint(c)  # View the result",
        "detail": "BitNet-main.BitNet-main.kernel_test",
        "documentation": {}
    },
    {
        "label": "x_scale",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.kernel_test",
        "description": "BitNet-main.BitNet-main.kernel_test",
        "peekOfCode": "x_scale = 1.0  # Example scale factor\n# Call the custom CUDA GEMM operation\ngemm_lowbit(a, b, c, w_scale, x_scale)\nprint(c)  # View the result",
        "detail": "BitNet-main.BitNet-main.kernel_test",
        "documentation": {}
    },
    {
        "label": "CachedWheelsCommand",
        "kind": 6,
        "importPath": "BitNet-main.BitNet-main.setup",
        "description": "BitNet-main.BitNet-main.setup",
        "peekOfCode": "class CachedWheelsCommand(_bdist_wheel):\n    \"\"\"\n    The CachedWheelsCommand plugs into the default bdist wheel, which is ran by pip when it cannot\n    find an existing wheel (which is currently the case for all installs). We use\n    the environment parameters to detect whether there is already a pre-built version of a compatible\n    wheel available and short-circuits the standard full build pipeline.\n    \"\"\"\n    def run(self):\n        if FORCE_BUILD:\n            return super().run()",
        "detail": "BitNet-main.BitNet-main.setup",
        "documentation": {}
    },
    {
        "label": "get_platform",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.setup",
        "description": "BitNet-main.BitNet-main.setup",
        "peekOfCode": "def get_platform():\n    \"\"\"\n    Returns the platform name as used in wheel filenames.\n    \"\"\"\n    if sys.platform.startswith(\"linux\"):\n        return \"linux_x86_64\"\n    elif sys.platform == \"darwin\":\n        mac_version = \".\".join(platform.mac_ver()[0].split(\".\")[:2])\n        return f\"macosx_{mac_version}_x86_64\"\n    elif sys.platform == \"win32\":",
        "detail": "BitNet-main.BitNet-main.setup",
        "documentation": {}
    },
    {
        "label": "get_cuda_bare_metal_version",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.setup",
        "description": "BitNet-main.BitNet-main.setup",
        "peekOfCode": "def get_cuda_bare_metal_version(cuda_dir):\n    raw_output = subprocess.check_output(\n        [cuda_dir + \"/bin/nvcc\", \"-V\"], universal_newlines=True\n    )\n    output = raw_output.split()\n    release_idx = output.index(\"release\") + 1\n    bare_metal_version = parse(output[release_idx].split(\",\")[0])\n    return raw_output, bare_metal_version\ndef check_if_cuda_home_none(global_option: str) -> None:\n    if CUDA_HOME is not None:",
        "detail": "BitNet-main.BitNet-main.setup",
        "documentation": {}
    },
    {
        "label": "check_if_cuda_home_none",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.setup",
        "description": "BitNet-main.BitNet-main.setup",
        "peekOfCode": "def check_if_cuda_home_none(global_option: str) -> None:\n    if CUDA_HOME is not None:\n        return\n    # warn instead of error because user could be downloading prebuilt wheels, so nvcc won't be necessary\n    # in that case.\n    warnings.warn(\n        f\"{global_option} was requested, but nvcc was not found.  Are you sure your environment has nvcc available?  \"\n        \"If you're installing within a container from https://hub.docker.com/r/pytorch/pytorch, \"\n        \"only images whose names contain 'devel' will provide nvcc.\"\n    )",
        "detail": "BitNet-main.BitNet-main.setup",
        "documentation": {}
    },
    {
        "label": "append_nvcc_threads",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.setup",
        "description": "BitNet-main.BitNet-main.setup",
        "peekOfCode": "def append_nvcc_threads(nvcc_extra_args):\n    return nvcc_extra_args + [\"--threads\", \"4\"]\ncmdclass = {}\next_modules = []\nif not SKIP_CUDA_BUILD:\n    print(\"\\n\\ntorch.__version__  = {}\\n\\n\".format(torch.__version__))\n    TORCH_MAJOR = int(torch.__version__.split(\".\")[0])\n    TORCH_MINOR = int(torch.__version__.split(\".\")[1])\n    check_if_cuda_home_none(PACKAGE_NAME)\n    # Check, if CUDA11 is installed for compute capability 8.0",
        "detail": "BitNet-main.BitNet-main.setup",
        "documentation": {}
    },
    {
        "label": "get_package_version",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.setup",
        "description": "BitNet-main.BitNet-main.setup",
        "peekOfCode": "def get_package_version():\n    with open(Path(this_dir) / PACKAGE_NAME / \"__init__.py\", \"r\") as f:\n        version_match = re.search(r\"^__version__\\s*=\\s*(.*)$\", f.read(), re.MULTILINE)\n    public_version = ast.literal_eval(version_match.group(1))\n    local_version = os.environ.get(\"MAMBA_LOCAL_VERSION\")\n    if local_version:\n        return f\"{public_version}+{local_version}\"\n    else:\n        return str(public_version)\ndef get_wheel_url():",
        "detail": "BitNet-main.BitNet-main.setup",
        "documentation": {}
    },
    {
        "label": "get_wheel_url",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.setup",
        "description": "BitNet-main.BitNet-main.setup",
        "peekOfCode": "def get_wheel_url():\n    # Determine the version numbers that will be used to determine the correct wheel\n    # We're using the CUDA version used to build torch, not the one currently installed\n    # _, cuda_version_raw = get_cuda_bare_metal_version(CUDA_HOME)\n    torch_cuda_version = parse(torch.version.cuda)\n    torch_version_raw = parse(torch.__version__)\n    # For CUDA 11, we only compile for CUDA 11.8, and for CUDA 12 we only compile for CUDA 12.2\n    # to save CI time. Minor versions should be compatible.\n    torch_cuda_version = (\n        parse(\"11.8\") if torch_cuda_version.major == 11 else parse(\"12.2\")",
        "detail": "BitNet-main.BitNet-main.setup",
        "documentation": {}
    },
    {
        "label": "this_dir",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.setup",
        "description": "BitNet-main.BitNet-main.setup",
        "peekOfCode": "this_dir = os.path.dirname(os.path.abspath(__file__))\nPACKAGE_NAME = \"bitnet\"\nBASE_WHEEL_URL = (\n    \"https://github.com/kyegomez/BitNet/releases/download/{tag_name}/{wheel_name}\"\n)\n# FORCE_BUILD: Force a fresh build locally, instead of attempting to find prebuilt wheels\n# SKIP_CUDA_BUILD: Intended to allow CI to use a simple `python setup.py sdist` run to copy over raw files, without any cuda compilation\nFORCE_BUILD = os.getenv(\"MAMBA_FORCE_BUILD\", \"FALSE\") == \"TRUE\"\nSKIP_CUDA_BUILD = os.getenv(\"MAMBA_SKIP_CUDA_BUILD\", \"FALSE\") == \"TRUE\"\n# For CI, we want the option to build with C++11 ABI since the nvcr images use C++11 ABI",
        "detail": "BitNet-main.BitNet-main.setup",
        "documentation": {}
    },
    {
        "label": "PACKAGE_NAME",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.setup",
        "description": "BitNet-main.BitNet-main.setup",
        "peekOfCode": "PACKAGE_NAME = \"bitnet\"\nBASE_WHEEL_URL = (\n    \"https://github.com/kyegomez/BitNet/releases/download/{tag_name}/{wheel_name}\"\n)\n# FORCE_BUILD: Force a fresh build locally, instead of attempting to find prebuilt wheels\n# SKIP_CUDA_BUILD: Intended to allow CI to use a simple `python setup.py sdist` run to copy over raw files, without any cuda compilation\nFORCE_BUILD = os.getenv(\"MAMBA_FORCE_BUILD\", \"FALSE\") == \"TRUE\"\nSKIP_CUDA_BUILD = os.getenv(\"MAMBA_SKIP_CUDA_BUILD\", \"FALSE\") == \"TRUE\"\n# For CI, we want the option to build with C++11 ABI since the nvcr images use C++11 ABI\nFORCE_CXX11_ABI = os.getenv(\"MAMBA_FORCE_CXX11_ABI\", \"FALSE\") == \"TRUE\"",
        "detail": "BitNet-main.BitNet-main.setup",
        "documentation": {}
    },
    {
        "label": "BASE_WHEEL_URL",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.setup",
        "description": "BitNet-main.BitNet-main.setup",
        "peekOfCode": "BASE_WHEEL_URL = (\n    \"https://github.com/kyegomez/BitNet/releases/download/{tag_name}/{wheel_name}\"\n)\n# FORCE_BUILD: Force a fresh build locally, instead of attempting to find prebuilt wheels\n# SKIP_CUDA_BUILD: Intended to allow CI to use a simple `python setup.py sdist` run to copy over raw files, without any cuda compilation\nFORCE_BUILD = os.getenv(\"MAMBA_FORCE_BUILD\", \"FALSE\") == \"TRUE\"\nSKIP_CUDA_BUILD = os.getenv(\"MAMBA_SKIP_CUDA_BUILD\", \"FALSE\") == \"TRUE\"\n# For CI, we want the option to build with C++11 ABI since the nvcr images use C++11 ABI\nFORCE_CXX11_ABI = os.getenv(\"MAMBA_FORCE_CXX11_ABI\", \"FALSE\") == \"TRUE\"\ndef get_platform():",
        "detail": "BitNet-main.BitNet-main.setup",
        "documentation": {}
    },
    {
        "label": "FORCE_BUILD",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.setup",
        "description": "BitNet-main.BitNet-main.setup",
        "peekOfCode": "FORCE_BUILD = os.getenv(\"MAMBA_FORCE_BUILD\", \"FALSE\") == \"TRUE\"\nSKIP_CUDA_BUILD = os.getenv(\"MAMBA_SKIP_CUDA_BUILD\", \"FALSE\") == \"TRUE\"\n# For CI, we want the option to build with C++11 ABI since the nvcr images use C++11 ABI\nFORCE_CXX11_ABI = os.getenv(\"MAMBA_FORCE_CXX11_ABI\", \"FALSE\") == \"TRUE\"\ndef get_platform():\n    \"\"\"\n    Returns the platform name as used in wheel filenames.\n    \"\"\"\n    if sys.platform.startswith(\"linux\"):\n        return \"linux_x86_64\"",
        "detail": "BitNet-main.BitNet-main.setup",
        "documentation": {}
    },
    {
        "label": "SKIP_CUDA_BUILD",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.setup",
        "description": "BitNet-main.BitNet-main.setup",
        "peekOfCode": "SKIP_CUDA_BUILD = os.getenv(\"MAMBA_SKIP_CUDA_BUILD\", \"FALSE\") == \"TRUE\"\n# For CI, we want the option to build with C++11 ABI since the nvcr images use C++11 ABI\nFORCE_CXX11_ABI = os.getenv(\"MAMBA_FORCE_CXX11_ABI\", \"FALSE\") == \"TRUE\"\ndef get_platform():\n    \"\"\"\n    Returns the platform name as used in wheel filenames.\n    \"\"\"\n    if sys.platform.startswith(\"linux\"):\n        return \"linux_x86_64\"\n    elif sys.platform == \"darwin\":",
        "detail": "BitNet-main.BitNet-main.setup",
        "documentation": {}
    },
    {
        "label": "FORCE_CXX11_ABI",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.setup",
        "description": "BitNet-main.BitNet-main.setup",
        "peekOfCode": "FORCE_CXX11_ABI = os.getenv(\"MAMBA_FORCE_CXX11_ABI\", \"FALSE\") == \"TRUE\"\ndef get_platform():\n    \"\"\"\n    Returns the platform name as used in wheel filenames.\n    \"\"\"\n    if sys.platform.startswith(\"linux\"):\n        return \"linux_x86_64\"\n    elif sys.platform == \"darwin\":\n        mac_version = \".\".join(platform.mac_ver()[0].split(\".\")[:2])\n        return f\"macosx_{mac_version}_x86_64\"",
        "detail": "BitNet-main.BitNet-main.setup",
        "documentation": {}
    },
    {
        "label": "cmdclass",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.setup",
        "description": "BitNet-main.BitNet-main.setup",
        "peekOfCode": "cmdclass = {}\next_modules = []\nif not SKIP_CUDA_BUILD:\n    print(\"\\n\\ntorch.__version__  = {}\\n\\n\".format(torch.__version__))\n    TORCH_MAJOR = int(torch.__version__.split(\".\")[0])\n    TORCH_MINOR = int(torch.__version__.split(\".\")[1])\n    check_if_cuda_home_none(PACKAGE_NAME)\n    # Check, if CUDA11 is installed for compute capability 8.0\n    cc_flag = []\n    if CUDA_HOME is not None:",
        "detail": "BitNet-main.BitNet-main.setup",
        "documentation": {}
    },
    {
        "label": "ext_modules",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.setup",
        "description": "BitNet-main.BitNet-main.setup",
        "peekOfCode": "ext_modules = []\nif not SKIP_CUDA_BUILD:\n    print(\"\\n\\ntorch.__version__  = {}\\n\\n\".format(torch.__version__))\n    TORCH_MAJOR = int(torch.__version__.split(\".\")[0])\n    TORCH_MINOR = int(torch.__version__.split(\".\")[1])\n    check_if_cuda_home_none(PACKAGE_NAME)\n    # Check, if CUDA11 is installed for compute capability 8.0\n    cc_flag = []\n    if CUDA_HOME is not None:\n        _, bare_metal_version = get_cuda_bare_metal_version(CUDA_HOME)",
        "detail": "BitNet-main.BitNet-main.setup",
        "documentation": {}
    },
    {
        "label": "TextSamplerDataset",
        "kind": 6,
        "importPath": "BitNet-main.BitNet-main.train",
        "description": "BitNet-main.BitNet-main.train",
        "peekOfCode": "class TextSamplerDataset(Dataset):\n    def __init__(self, data, seq_len):\n        super().__init__()\n        self.data = data\n        self.seq_len = seq_len\n    def __getitem__(self, index):\n        rand_start = torch.randint(0, self.data.size(0) - self.seq_len, (1,))\n        full_seq = self.data[rand_start : rand_start + self.seq_len + 1].long()\n        return full_seq.cuda()\n    def __len__(self):",
        "detail": "BitNet-main.BitNet-main.train",
        "documentation": {}
    },
    {
        "label": "cycle",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.train",
        "description": "BitNet-main.BitNet-main.train",
        "peekOfCode": "def cycle(loader):\n    while True:\n        yield from loader\ndef decode_token(token):\n    return str(chr(max(32, token)))\ndef decode_tokens(tokens):\n    return \"\".join(list(map(decode_token, tokens)))\n# instantiate GPT-like decoder model\nmodel = BitNetTransformer(num_tokens=256, dim=512, depth=8)\nmodel = AutoregressiveWrapper(model, max_seq_len=SEQ_LEN)",
        "detail": "BitNet-main.BitNet-main.train",
        "documentation": {}
    },
    {
        "label": "decode_token",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.train",
        "description": "BitNet-main.BitNet-main.train",
        "peekOfCode": "def decode_token(token):\n    return str(chr(max(32, token)))\ndef decode_tokens(tokens):\n    return \"\".join(list(map(decode_token, tokens)))\n# instantiate GPT-like decoder model\nmodel = BitNetTransformer(num_tokens=256, dim=512, depth=8)\nmodel = AutoregressiveWrapper(model, max_seq_len=SEQ_LEN)\n# Use all available GPUs\nif torch.cuda.device_count() > 1:\n    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")",
        "detail": "BitNet-main.BitNet-main.train",
        "documentation": {}
    },
    {
        "label": "decode_tokens",
        "kind": 2,
        "importPath": "BitNet-main.BitNet-main.train",
        "description": "BitNet-main.BitNet-main.train",
        "peekOfCode": "def decode_tokens(tokens):\n    return \"\".join(list(map(decode_token, tokens)))\n# instantiate GPT-like decoder model\nmodel = BitNetTransformer(num_tokens=256, dim=512, depth=8)\nmodel = AutoregressiveWrapper(model, max_seq_len=SEQ_LEN)\n# Use all available GPUs\nif torch.cuda.device_count() > 1:\n    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n    model = torch.nn.DataParallel(model)\nmodel.cuda()",
        "detail": "BitNet-main.BitNet-main.train",
        "documentation": {}
    },
    {
        "label": "NUM_BATCHES",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.train",
        "description": "BitNet-main.BitNet-main.train",
        "peekOfCode": "NUM_BATCHES = int(1e5)\nBATCH_SIZE = 4\nGRADIENT_ACCUMULATE_EVERY = 4\nLEARNING_RATE = 2e-4\nVALIDATE_EVERY = 100\nGENERATE_EVERY = 500\nGENERATE_LENGTH = 512\nSEQ_LEN = 1024\n# helpers\ndef cycle(loader):",
        "detail": "BitNet-main.BitNet-main.train",
        "documentation": {}
    },
    {
        "label": "BATCH_SIZE",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.train",
        "description": "BitNet-main.BitNet-main.train",
        "peekOfCode": "BATCH_SIZE = 4\nGRADIENT_ACCUMULATE_EVERY = 4\nLEARNING_RATE = 2e-4\nVALIDATE_EVERY = 100\nGENERATE_EVERY = 500\nGENERATE_LENGTH = 512\nSEQ_LEN = 1024\n# helpers\ndef cycle(loader):\n    while True:",
        "detail": "BitNet-main.BitNet-main.train",
        "documentation": {}
    },
    {
        "label": "GRADIENT_ACCUMULATE_EVERY",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.train",
        "description": "BitNet-main.BitNet-main.train",
        "peekOfCode": "GRADIENT_ACCUMULATE_EVERY = 4\nLEARNING_RATE = 2e-4\nVALIDATE_EVERY = 100\nGENERATE_EVERY = 500\nGENERATE_LENGTH = 512\nSEQ_LEN = 1024\n# helpers\ndef cycle(loader):\n    while True:\n        yield from loader",
        "detail": "BitNet-main.BitNet-main.train",
        "documentation": {}
    },
    {
        "label": "LEARNING_RATE",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.train",
        "description": "BitNet-main.BitNet-main.train",
        "peekOfCode": "LEARNING_RATE = 2e-4\nVALIDATE_EVERY = 100\nGENERATE_EVERY = 500\nGENERATE_LENGTH = 512\nSEQ_LEN = 1024\n# helpers\ndef cycle(loader):\n    while True:\n        yield from loader\ndef decode_token(token):",
        "detail": "BitNet-main.BitNet-main.train",
        "documentation": {}
    },
    {
        "label": "VALIDATE_EVERY",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.train",
        "description": "BitNet-main.BitNet-main.train",
        "peekOfCode": "VALIDATE_EVERY = 100\nGENERATE_EVERY = 500\nGENERATE_LENGTH = 512\nSEQ_LEN = 1024\n# helpers\ndef cycle(loader):\n    while True:\n        yield from loader\ndef decode_token(token):\n    return str(chr(max(32, token)))",
        "detail": "BitNet-main.BitNet-main.train",
        "documentation": {}
    },
    {
        "label": "GENERATE_EVERY",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.train",
        "description": "BitNet-main.BitNet-main.train",
        "peekOfCode": "GENERATE_EVERY = 500\nGENERATE_LENGTH = 512\nSEQ_LEN = 1024\n# helpers\ndef cycle(loader):\n    while True:\n        yield from loader\ndef decode_token(token):\n    return str(chr(max(32, token)))\ndef decode_tokens(tokens):",
        "detail": "BitNet-main.BitNet-main.train",
        "documentation": {}
    },
    {
        "label": "GENERATE_LENGTH",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.train",
        "description": "BitNet-main.BitNet-main.train",
        "peekOfCode": "GENERATE_LENGTH = 512\nSEQ_LEN = 1024\n# helpers\ndef cycle(loader):\n    while True:\n        yield from loader\ndef decode_token(token):\n    return str(chr(max(32, token)))\ndef decode_tokens(tokens):\n    return \"\".join(list(map(decode_token, tokens)))",
        "detail": "BitNet-main.BitNet-main.train",
        "documentation": {}
    },
    {
        "label": "SEQ_LEN",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.train",
        "description": "BitNet-main.BitNet-main.train",
        "peekOfCode": "SEQ_LEN = 1024\n# helpers\ndef cycle(loader):\n    while True:\n        yield from loader\ndef decode_token(token):\n    return str(chr(max(32, token)))\ndef decode_tokens(tokens):\n    return \"\".join(list(map(decode_token, tokens)))\n# instantiate GPT-like decoder model",
        "detail": "BitNet-main.BitNet-main.train",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.train",
        "description": "BitNet-main.BitNet-main.train",
        "peekOfCode": "model = BitNetTransformer(num_tokens=256, dim=512, depth=8)\nmodel = AutoregressiveWrapper(model, max_seq_len=SEQ_LEN)\n# Use all available GPUs\nif torch.cuda.device_count() > 1:\n    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n    model = torch.nn.DataParallel(model)\nmodel.cuda()\n# prepare enwik8 data\nwith gzip.open(\"./data/enwik8.gz\") as file:\n    X = np.fromstring(file.read(int(95e6)), dtype=np.uint8)",
        "detail": "BitNet-main.BitNet-main.train",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.train",
        "description": "BitNet-main.BitNet-main.train",
        "peekOfCode": "model = AutoregressiveWrapper(model, max_seq_len=SEQ_LEN)\n# Use all available GPUs\nif torch.cuda.device_count() > 1:\n    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n    model = torch.nn.DataParallel(model)\nmodel.cuda()\n# prepare enwik8 data\nwith gzip.open(\"./data/enwik8.gz\") as file:\n    X = np.fromstring(file.read(int(95e6)), dtype=np.uint8)\n    trX, vaX = np.split(X, [int(90e6)])",
        "detail": "BitNet-main.BitNet-main.train",
        "documentation": {}
    },
    {
        "label": "train_dataset",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.train",
        "description": "BitNet-main.BitNet-main.train",
        "peekOfCode": "train_dataset = TextSamplerDataset(data_train, SEQ_LEN)\nval_dataset = TextSamplerDataset(data_val, SEQ_LEN)\ntrain_loader = cycle(DataLoader(train_dataset, batch_size=BATCH_SIZE))\nval_loader = cycle(DataLoader(val_dataset, batch_size=BATCH_SIZE))\n# optimizer\noptim = StableAdamWUnfused(\n    model.parameters(),\n    lr=LEARNING_RATE,\n)\n# training",
        "detail": "BitNet-main.BitNet-main.train",
        "documentation": {}
    },
    {
        "label": "val_dataset",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.train",
        "description": "BitNet-main.BitNet-main.train",
        "peekOfCode": "val_dataset = TextSamplerDataset(data_val, SEQ_LEN)\ntrain_loader = cycle(DataLoader(train_dataset, batch_size=BATCH_SIZE))\nval_loader = cycle(DataLoader(val_dataset, batch_size=BATCH_SIZE))\n# optimizer\noptim = StableAdamWUnfused(\n    model.parameters(),\n    lr=LEARNING_RATE,\n)\n# training\nfor i in tqdm.tqdm(range(NUM_BATCHES), mininterval=10.0, desc=\"training\"):",
        "detail": "BitNet-main.BitNet-main.train",
        "documentation": {}
    },
    {
        "label": "train_loader",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.train",
        "description": "BitNet-main.BitNet-main.train",
        "peekOfCode": "train_loader = cycle(DataLoader(train_dataset, batch_size=BATCH_SIZE))\nval_loader = cycle(DataLoader(val_dataset, batch_size=BATCH_SIZE))\n# optimizer\noptim = StableAdamWUnfused(\n    model.parameters(),\n    lr=LEARNING_RATE,\n)\n# training\nfor i in tqdm.tqdm(range(NUM_BATCHES), mininterval=10.0, desc=\"training\"):\n    model.train()",
        "detail": "BitNet-main.BitNet-main.train",
        "documentation": {}
    },
    {
        "label": "val_loader",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.train",
        "description": "BitNet-main.BitNet-main.train",
        "peekOfCode": "val_loader = cycle(DataLoader(val_dataset, batch_size=BATCH_SIZE))\n# optimizer\noptim = StableAdamWUnfused(\n    model.parameters(),\n    lr=LEARNING_RATE,\n)\n# training\nfor i in tqdm.tqdm(range(NUM_BATCHES), mininterval=10.0, desc=\"training\"):\n    model.train()\n    for __ in range(GRADIENT_ACCUMULATE_EVERY):",
        "detail": "BitNet-main.BitNet-main.train",
        "documentation": {}
    },
    {
        "label": "optim",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.train",
        "description": "BitNet-main.BitNet-main.train",
        "peekOfCode": "optim = StableAdamWUnfused(\n    model.parameters(),\n    lr=LEARNING_RATE,\n)\n# training\nfor i in tqdm.tqdm(range(NUM_BATCHES), mininterval=10.0, desc=\"training\"):\n    model.train()\n    for __ in range(GRADIENT_ACCUMULATE_EVERY):\n        loss = model(next(train_loader))\n        loss.mean().backward()",
        "detail": "BitNet-main.BitNet-main.train",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.transformer_example",
        "description": "BitNet-main.BitNet-main.transformer_example",
        "peekOfCode": "x = torch.randint(0, 20000, (1, 1024))\n# Initialize the BitNetTransformer model\nbitnet = BitNetTransformer(\n    num_tokens=20000,  # Number of unique tokens in the input\n    dim=1024,  # Dimension of the input and output embeddings\n    depth=6,  # Number of transformer layers\n    heads=8,  # Number of attention heads\n    ff_mult=4,  # Multiplier for the hidden dimension in the feed-forward network\n)\n# Pass the tensor through the transformer model",
        "detail": "BitNet-main.BitNet-main.transformer_example",
        "documentation": {}
    },
    {
        "label": "bitnet",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.transformer_example",
        "description": "BitNet-main.BitNet-main.transformer_example",
        "peekOfCode": "bitnet = BitNetTransformer(\n    num_tokens=20000,  # Number of unique tokens in the input\n    dim=1024,  # Dimension of the input and output embeddings\n    depth=6,  # Number of transformer layers\n    heads=8,  # Number of attention heads\n    ff_mult=4,  # Multiplier for the hidden dimension in the feed-forward network\n)\n# Pass the tensor through the transformer model\nlogits = bitnet(x)\n# Print the shape of the output",
        "detail": "BitNet-main.BitNet-main.transformer_example",
        "documentation": {}
    },
    {
        "label": "logits",
        "kind": 5,
        "importPath": "BitNet-main.BitNet-main.transformer_example",
        "description": "BitNet-main.BitNet-main.transformer_example",
        "peekOfCode": "logits = bitnet(x)\n# Print the shape of the output\nprint(logits)",
        "detail": "BitNet-main.BitNet-main.transformer_example",
        "documentation": {}
    },
    {
        "label": "AutoregressiveWrapper",
        "kind": 6,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.at",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.at",
        "peekOfCode": "class AutoregressiveWrapper(nn.Module):\n    \"\"\"\n    AutoregressiveWrapper is a wrapper class that adds autoregressive generation functionality to a given neural network.\n    Args:\n        net (nn.Module): The neural network model.\n        max_seq_len (int): The maximum sequence length for generation. Defaults to 2048.\n        pad_value (int): The padding value for generated sequences. Defaults to 0.\n    \"\"\"\n    def __init__(self, net, max_seq_len=2048, pad_value=0):\n        super().__init__()",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.at",
        "documentation": {}
    },
    {
        "label": "exists",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.at",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.at",
        "peekOfCode": "def exists(val):\n    return val is not None\ndef eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.at",
        "documentation": {}
    },
    {
        "label": "eval_decorator",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.at",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.at",
        "peekOfCode": "def eval_decorator(fn):\n    def inner(model, *args, **kwargs):\n        was_training = model.training\n        model.eval()\n        out = fn(model, *args, **kwargs)\n        model.train(was_training)\n        return out\n    return inner\n# top k filtering\ndef top_k(logits, thres=0.9):",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.at",
        "documentation": {}
    },
    {
        "label": "top_k",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.at",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.at",
        "peekOfCode": "def top_k(logits, thres=0.9):\n    k = int((1 - thres) * logits.shape[-1])\n    val, ind = torch.topk(logits, k)\n    probs = torch.full_like(logits, float(\"-inf\"))\n    probs.scatter_(1, ind, val)\n    return probs\nclass AutoregressiveWrapper(nn.Module):\n    \"\"\"\n    AutoregressiveWrapper is a wrapper class that adds autoregressive generation functionality to a given neural network.\n    Args:",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.at",
        "documentation": {}
    },
    {
        "label": "BitLinear",
        "kind": 6,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bitlinear",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bitlinear",
        "peekOfCode": "class BitLinear(nn.Linear):\n    \"\"\"\n    Custom linear layer with bit quantization.\n    Args:\n        dim (int): The input dimension of the layer.\n        training (bool, optional): Whether the layer is in training mode or not. Defaults to False.\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n    Attributes:\n        dim (int): The input dimension of the layer.",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bitlinear",
        "documentation": {}
    },
    {
        "label": "activation_quant",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bitlinear",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bitlinear",
        "peekOfCode": "def activation_quant(x: Tensor):\n    \"\"\"Per token quantization to 8bits. No grouping is needed for quantization\n    Args:\n        x (Tensor): _description_\n    Returns:\n        _type_: _description_\n    \"\"\"\n    scale = 127.0 / x.abs().max(dim=-1, keepdim=True).values.clamp_(min=1e-5)\n    y = (x * scale).round().clamp_(-128, 127) / scale\n    return y",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bitlinear",
        "documentation": {}
    },
    {
        "label": "weight_quant",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bitlinear",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bitlinear",
        "peekOfCode": "def weight_quant(w: Tensor):\n    scale = w.abs().mean()\n    e = w.mean()\n    u = (w - e).sign() * scale\n    return u\nclass BitLinear(nn.Linear):\n    \"\"\"\n    Custom linear layer with bit quantization.\n    Args:\n        dim (int): The input dimension of the layer.",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bitlinear",
        "documentation": {}
    },
    {
        "label": "BitMGQA",
        "kind": 6,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_attention",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_attention",
        "peekOfCode": "class BitMGQA(nn.Module):\n    \"\"\"Multi-head grouped query attention (GQA) layer.\n    Reference:\n        \"GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints\"\n        https://arxiv.org/pdf/2305.13245v1.pdf\n    GQA is a variant of multihead attention (MHA) that uses fewer write heads\n    (key / value) than query heads.  GQA can be viewed as a generalization of\n    multi-query attention (MQA), which uses a single write head. GQA and MQA give\n    significant speedups over standard MHA in decoder layers, with minimal loss in\n    accuracy. In the paper, GQA is shown to be more accurate than MQA, while still",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_attention",
        "documentation": {}
    },
    {
        "label": "scaled_dot_product_gqa",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_attention",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_attention",
        "peekOfCode": "def scaled_dot_product_gqa(\n    query: Tensor,\n    key: Tensor,\n    value: Tensor,\n    dropout: float = 0.0,\n    scale: Optional[float] = None,\n    mask: Optional[Tensor] = None,\n    is_causal: Optional[bool] = None,\n    need_weights: bool = False,\n    average_attn_weights: bool = False,",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_attention",
        "documentation": {}
    },
    {
        "label": "GLU",
        "kind": 6,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_ffn",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_ffn",
        "peekOfCode": "class GLU(nn.Module):\n    \"\"\"\n    Gated Linear Unit (GLU) module.\n    Args:\n        dim_in (int): Input dimension.\n        dim_out (int): Output dimension.\n        activation (Callable): Activation function to be applied to the gate.\n        mult_bias (bool, optional): Whether to multiply the bias term. Defaults to False.\n        linear (Callable, optional): Linear function to be used for projection. Defaults to False.\n    \"\"\"",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_ffn",
        "documentation": {}
    },
    {
        "label": "BitFeedForward",
        "kind": 6,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_ffn",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_ffn",
        "peekOfCode": "class BitFeedForward(nn.Module):\n    \"\"\"\n    BitFeedForward module performs feed-forward operations on the input tensor.\n    Args:\n        dim (int): The input dimension.\n        dim_out (int, optional): The output dimension. If not provided, it is set to the input dimension.\n        mult (int, optional): The multiplier for the inner dimension. Default is 4.\n        glu (bool, optional): Whether to use Gated Linear Unit (GLU) activation. Default is False.\n        glu_mult_bias (bool, optional): Whether to apply bias to the GLU activation. Default is False.\n        swish (bool, optional): Whether to use Swish activation. Default is False.",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_ffn",
        "documentation": {}
    },
    {
        "label": "default",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_ffn",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_ffn",
        "peekOfCode": "def default(val, d):\n    return val if val is not None else d\ndef init_zero_(tensor):\n    nn.init.constant_(tensor, 0.0)\n# [GLU]\nclass GLU(nn.Module):\n    \"\"\"\n    Gated Linear Unit (GLU) module.\n    Args:\n        dim_in (int): Input dimension.",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_ffn",
        "documentation": {}
    },
    {
        "label": "init_zero_",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_ffn",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_ffn",
        "peekOfCode": "def init_zero_(tensor):\n    nn.init.constant_(tensor, 0.0)\n# [GLU]\nclass GLU(nn.Module):\n    \"\"\"\n    Gated Linear Unit (GLU) module.\n    Args:\n        dim_in (int): Input dimension.\n        dim_out (int): Output dimension.\n        activation (Callable): Activation function to be applied to the gate.",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_ffn",
        "documentation": {}
    },
    {
        "label": "BitLinearNew",
        "kind": 6,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_linear_new",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_linear_new",
        "peekOfCode": "class BitLinearNew(nn.Linear):\n    \"\"\"\n    Custom linear layer with bit quantization.\n    Args:\n        dim (int): The input dimension of the layer.\n        training (bool, optional): Whether the layer is in training mode or not. Defaults to False.\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n    Attributes:\n        dim (int): The input dimension of the layer.",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_linear_new",
        "documentation": {}
    },
    {
        "label": "activation_quant",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_linear_new",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_linear_new",
        "peekOfCode": "def activation_quant(x: Tensor):\n    \"\"\"Per token quantization to 8bits. No grouping is needed for quantization\n    Args:\n        x (Tensor): _description_\n    Returns:\n        _type_: _description_\n    \"\"\"\n    scale = 127.0 / x.abs().max(dim=-1, keepdim=True).values.clamp_(min=1e-5)\n    y = (x * scale).round().clamp_(-128, 127) / scale\n    return y",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_linear_new",
        "documentation": {}
    },
    {
        "label": "weight_quant",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_linear_new",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_linear_new",
        "peekOfCode": "def weight_quant(w: Tensor):\n    scale = w.abs().mean()\n    e = w.mean()\n    u = (w - e).sign() * scale\n    return u\nclass BitLinearNew(nn.Linear):\n    \"\"\"\n    Custom linear layer with bit quantization.\n    Args:\n        dim (int): The input dimension of the layer.",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_linear_new",
        "documentation": {}
    },
    {
        "label": "ModelArgs",
        "kind": 6,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_llama",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_llama",
        "peekOfCode": "class ModelArgs:\n    dim: int = 4096\n    n_layers: int = 32\n    n_heads: int = 32\n    n_kv_heads: Optional[int] = None\n    vocab_size: int = -1  # defined later by tokenizer\n    multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2\n    ffn_dim_multiplier: Optional[float] = None\n    norm_eps: float = 1e-5\n    max_batch_size: int = 32",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_llama",
        "documentation": {}
    },
    {
        "label": "RMSNorm",
        "kind": 6,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_llama",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_llama",
        "peekOfCode": "class RMSNorm(torch.nn.Module):\n    def __init__(self, dim: int, eps: float = 1e-6):\n        \"\"\"\n        Initialize the RMSNorm normalization layer.\n        Args:\n            dim (int): The dimension of the input tensor.\n            eps (float, optional): A small value added to the denominator for numerical stability. Default is 1e-6.\n        Attributes:\n            eps (float): A small value added to the denominator for numerical stability.\n            weight (nn.Parameter): Learnable scaling parameter.",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_llama",
        "documentation": {}
    },
    {
        "label": "Attention",
        "kind": 6,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_llama",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_llama",
        "peekOfCode": "class Attention(nn.Module):\n    \"\"\"Multi-head attention module.\"\"\"\n    def __init__(self, args: ModelArgs):\n        \"\"\"\n        Initialize the Attention module.\n        Args:\n            args (ModelArgs): Model configuration parameters.\n        Attributes:\n            n_kv_heads (int): Number of key and value heads.\n            n_local_heads (int): Number of local query heads.",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_llama",
        "documentation": {}
    },
    {
        "label": "FeedForward",
        "kind": 6,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_llama",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_llama",
        "peekOfCode": "class FeedForward(nn.Module):\n    def __init__(\n        self,\n        dim: int,\n        hidden_dim: int,\n        multiple_of: int,\n        ffn_dim_multiplier: Optional[float],\n    ):\n        \"\"\"\n        Initialize the FeedForward module.",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_llama",
        "documentation": {}
    },
    {
        "label": "TransformerBlock",
        "kind": 6,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_llama",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_llama",
        "peekOfCode": "class TransformerBlock(nn.Module):\n    def __init__(self, layer_id: int, args: ModelArgs):\n        \"\"\"\n        Initialize a TransformerBlock.\n        Args:\n            layer_id (int): Identifier for the layer.\n            args (ModelArgs): Model configuration parameters.\n        Attributes:\n            n_heads (int): Number of attention heads.\n            dim (int): Dimension size of the model.",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_llama",
        "documentation": {}
    },
    {
        "label": "Transformer",
        "kind": 6,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_llama",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_llama",
        "peekOfCode": "class Transformer(nn.Module):\n    def __init__(self, params: ModelArgs):\n        \"\"\"\n        Initialize a Transformer model.\n        Args:\n            params (ModelArgs): Model configuration parameters.\n        Attributes:\n            params (ModelArgs): Model configuration parameters.\n            vocab_size (int): Vocabulary size.\n            n_layers (int): Number of layers in the model.",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_llama",
        "documentation": {}
    },
    {
        "label": "precompute_freqs_cis",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_llama",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_llama",
        "peekOfCode": "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n    \"\"\"\n    Precompute the frequency tensor for complex exponentials (cis) with given dimensions.\n    This function calculates a frequency tensor with complex exponentials using the given dimension 'dim'\n    and the end index 'end'. The 'theta' parameter scales the frequencies.\n    The returned tensor contains complex values in complex64 data type.\n    Args:\n        dim (int): Dimension of the frequency tensor.\n        end (int): End index for precomputing frequencies.\n        theta (float, optional): Scaling factor for frequency computation. Defaults to 10000.0.",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_llama",
        "documentation": {}
    },
    {
        "label": "reshape_for_broadcast",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_llama",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_llama",
        "peekOfCode": "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n    \"\"\"\n    Reshape frequency tensor for broadcasting it with another tensor.\n    This function reshapes the frequency tensor to have the same shape as the target tensor 'x'\n    for the purpose of broadcasting the frequency tensor during element-wise operations.\n    Args:\n        freqs_cis (torch.Tensor): Frequency tensor to be reshaped.\n        x (torch.Tensor): Target tensor for broadcasting compatibility.\n    Returns:\n        torch.Tensor: Reshaped frequency tensor.",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_llama",
        "documentation": {}
    },
    {
        "label": "apply_rotary_emb",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_llama",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_llama",
        "peekOfCode": "def apply_rotary_emb(\n    xq: torch.Tensor,\n    xk: torch.Tensor,\n    freqs_cis: torch.Tensor,\n) -> Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Apply rotary embeddings to input tensors using the given frequency tensor.\n    This function applies rotary embeddings to the given query 'xq' and key 'xk' tensors using the provided\n    frequency tensor 'freqs_cis'. The input tensors are reshaped as complex numbers, and the frequency tensor\n    is reshaped for broadcasting compatibility. The resulting tensors contain rotary embeddings and are",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_llama",
        "documentation": {}
    },
    {
        "label": "repeat_kv",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_llama",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_llama",
        "peekOfCode": "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n    \"\"\"torch.repeat_interleave(x, dim=2, repeats=n_rep)\"\"\"\n    bs, slen, n_kv_heads, head_dim = x.shape\n    if n_rep == 1:\n        return x\n    return (\n        x[:, :, :, None, :]\n        .expand(bs, slen, n_kv_heads, n_rep, head_dim)\n        .reshape(bs, slen, n_kv_heads * n_rep, head_dim)\n    )",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_llama",
        "documentation": {}
    },
    {
        "label": "BitLora",
        "kind": 6,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_lora",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_lora",
        "peekOfCode": "class BitLora(BitLinear):\n    \"\"\"\n    BitLora class represents a custom linear layer with LoRa (Low Rank) regularization.\n    Args:\n        rank (int): The rank of the LoRa regularization. Default is 4.\n        lora_alpha (int): The scaling factor for LoRa regularization. Default is 1.\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n    Attributes:\n        rank (int): The rank of the LoRa regularization.",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_lora",
        "documentation": {}
    },
    {
        "label": "weight_quant",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_lora",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_lora",
        "peekOfCode": "def weight_quant(w: Tensor):\n    scale = w.abs().mean()\n    e = w.mean()\n    u = (w - e).sign() * scale\n    return u\ndef activation_quant(x: Tensor):\n    \"\"\"Per token quantization to 8bits. No grouping is needed for quantization\n    Args:\n        x (Tensor): _description_\n    Returns:",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_lora",
        "documentation": {}
    },
    {
        "label": "activation_quant",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_lora",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_lora",
        "peekOfCode": "def activation_quant(x: Tensor):\n    \"\"\"Per token quantization to 8bits. No grouping is needed for quantization\n    Args:\n        x (Tensor): _description_\n    Returns:\n        _type_: _description_\n    \"\"\"\n    scale = 127.0 / x.abs().max(dim=-1, keepdim=True).values.clamp_(min=1e-5)\n    y = (x * scale).round().clamp_(-128, 127) / scale\n    return y",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_lora",
        "documentation": {}
    },
    {
        "label": "RMSNorm",
        "kind": 6,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_mamba",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_mamba",
        "peekOfCode": "class RMSNorm(nn.Module):\n    def __init__(self, dim: int, eps: float = 1e-5):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(dim))\n    def forward(self, x):\n        output = (\n            x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight\n        )\n        return output",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_mamba",
        "documentation": {}
    },
    {
        "label": "PScan",
        "kind": 6,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_mamba",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_mamba",
        "peekOfCode": "class PScan(torch.autograd.Function):\n    @staticmethod\n    def pscan(A, X):\n        # A : (B, D, L, N)\n        # X : (B, D, L, N)\n        # modifies X in place by doing a parallel scan.\n        # more formally, X will be populated by these values :\n        # H[t] = A[t] * H[t-1] + X[t] with H[0] = 0\n        # which are computed in parallel (2*log2(T) sequential steps (ideally), instead of T sequential steps)\n        B, D, L, _ = A.size()",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_mamba",
        "documentation": {}
    },
    {
        "label": "MambaConfig",
        "kind": 6,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_mamba",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_mamba",
        "peekOfCode": "class MambaConfig:\n    dim: int  # D\n    depth: int\n    dt_rank: Union[int, str] = \"auto\"\n    d_state: int = 16  # N in paper/comments\n    expand_factor: int = 2  # E in paper/comments\n    d_conv: int = 4\n    dt_min: float = 0.001\n    dt_max: float = 0.1\n    dt_init: str = \"random\"  # \"random\" or \"constant\"",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_mamba",
        "documentation": {}
    },
    {
        "label": "ResidualBlock",
        "kind": 6,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_mamba",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_mamba",
        "peekOfCode": "class ResidualBlock(nn.Module):\n    def __init__(self, config: MambaConfig):\n        super().__init__()\n        self.mixer = MambaBlock(config)\n        self.norm = RMSNorm(config.dim)\n    def forward(self, x):\n        # x : (B, L, D)\n        # output : (B, L, D)\n        output = self.mixer(self.norm(x)) + x\n        return output",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_mamba",
        "documentation": {}
    },
    {
        "label": "MambaBlock",
        "kind": 6,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_mamba",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_mamba",
        "peekOfCode": "class MambaBlock(nn.Module):\n    def __init__(self, config: MambaConfig):\n        super().__init__()\n        self.config = config\n        # projects block input from D to 2*ED (two branches)\n        self.in_proj = BitLinear(config.dim, 2 * config.d_inner, bias=config.bias)\n        self.conv1d = nn.Conv1d(\n            in_channels=config.d_inner,\n            out_channels=config.d_inner,\n            kernel_size=config.d_conv,",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_mamba",
        "documentation": {}
    },
    {
        "label": "Mamba",
        "kind": 6,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_mamba",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_mamba",
        "peekOfCode": "class Mamba(nn.Module):\n    def __init__(\n        self,\n        num_tokens: int,\n        sequence_length: int,\n        config: MambaConfig,\n        return_embeddings: bool = True,\n        return_tokens: bool = True,\n    ):\n        super().__init__()",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_mamba",
        "documentation": {}
    },
    {
        "label": "BitMamba",
        "kind": 6,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_mamba",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_mamba",
        "peekOfCode": "class BitMamba(nn.Module):\n    \"\"\"\n    BitMamba module for performing computations using the BitNet architecture.\n    Args:\n        dim (int): The input dimension (D).\n        depth (int): The depth of the BitNet architecture.\n        dt_rank (Union[int, str], optional): The rank of the time step tensor. Defaults to \"auto\".\n        d_state (int, optional): The dimension of the state tensor (N in paper/comments). Defaults to 16.\n        expand_factor (int, optional): The expansion factor for the inner dimension (E in paper/comments). Defaults to 2.\n        d_conv (int, optional): The dimension of the convolutional filters. Defaults to 4.",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_mamba",
        "documentation": {}
    },
    {
        "label": "pscan",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_mamba",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_mamba",
        "peekOfCode": "pscan = PScan.apply\n@dataclass\nclass MambaConfig:\n    dim: int  # D\n    depth: int\n    dt_rank: Union[int, str] = \"auto\"\n    d_state: int = 16  # N in paper/comments\n    expand_factor: int = 2  # E in paper/comments\n    d_conv: int = 4\n    dt_min: float = 0.001",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_mamba",
        "documentation": {}
    },
    {
        "label": "Expert",
        "kind": 6,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_moe",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_moe",
        "peekOfCode": "class Expert(nn.Module):\n    \"\"\"An MLP is a simple linear layer followed by a non-linearity i.e. each Expert\n    Args:\n        dim (int): The input dimension of the linear layer.\n        dropout (float, optional): The dropout probability. Defaults to 0.1.\n    Attributes:\n        net (nn.Sequential): The sequential network consisting of linear layers, ReLU activation, and dropout.\n    \"\"\"\n    def __init__(self, dim: int, dropout: int = 0.1):\n        super().__init__()",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_moe",
        "documentation": {}
    },
    {
        "label": "NoisyTopkRouter",
        "kind": 6,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_moe",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_moe",
        "peekOfCode": "class NoisyTopkRouter(nn.Module):\n    \"\"\"\n    A class representing a Noisy Top-k Router module.\n    This module takes the output tensor from a multihead self attention block and performs routing\n    by selecting the top-k experts based on the logits. It adds scaled unit Gaussian noise to the logits\n    and applies softmax to obtain the final router output.\n    Args:\n        dim (int): The input dimension of the tensor.\n        num_experts (int): The number of experts.\n        top_k (int): The number of experts to select.",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_moe",
        "documentation": {}
    },
    {
        "label": "BitMoE",
        "kind": 6,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_moe",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_moe",
        "peekOfCode": "class BitMoE(nn.Module):\n    \"\"\"\n    BitMoE (Bitwise Mixture of Experts) module.\n    Args:\n        dim (int): The input dimension.\n        num_experts (int): The number of experts in the mixture.\n        top_k (int, optional): The number of experts to select for each input. Defaults to 2.\n    \"\"\"\n    def __init__(self, dim: int, num_experts: int, top_k: int = 2):\n        super(BitMoE, self).__init__()",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_moe",
        "documentation": {}
    },
    {
        "label": "#",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_moe",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_moe",
        "peekOfCode": "# = Linear projection + non linear activation functions like [RELU, GELU, etc] + Dropout[optional] + Normalization[optional, LayerNorm]\n# Expert module\nclass Expert(nn.Module):\n    \"\"\"An MLP is a simple linear layer followed by a non-linearity i.e. each Expert\n    Args:\n        dim (int): The input dimension of the linear layer.\n        dropout (float, optional): The dropout probability. Defaults to 0.1.\n    Attributes:\n        net (nn.Sequential): The sequential network consisting of linear layers, ReLU activation, and dropout.\n    \"\"\"",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_moe",
        "documentation": {}
    },
    {
        "label": "RMSNorm",
        "kind": 6,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_transformer",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_transformer",
        "peekOfCode": "class RMSNorm(nn.Module):\n    \"\"\"\n    Root Mean Square Normalization (RMSNorm) module.\n    Args:\n        dim (int): The input dimension.\n        affine (bool, optional): If True, apply an affine transformation to the normalized output.\n            Default is True.\n    Attributes:\n        scale (float): The scaling factor for the normalized output.\n        gamma (torch.Tensor or float): The learnable parameter for the affine transformation.",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_transformer",
        "documentation": {}
    },
    {
        "label": "Transformer",
        "kind": 6,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_transformer",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_transformer",
        "peekOfCode": "class Transformer(nn.Module):\n    \"\"\"\n    Transformer module that applies multi-head attention and feed-forward layers.\n    Args:\n        dim (int): The dimension of the input and output tensors.\n        heads (int): The number of attention heads.\n        depth (int): The number of transformer layers.\n        ff_mult (int, optional): The multiplier for the hidden dimension in the feed-forward layers.\n            Defaults to 2.\n        *args: Variable length argument list.",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_transformer",
        "documentation": {}
    },
    {
        "label": "BitNetTransformer",
        "kind": 6,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_transformer",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_transformer",
        "peekOfCode": "class BitNetTransformer(nn.Module):\n    \"\"\"\n    BitNetTransformer is a transformer-based model for BitNet.\n    Args:\n        dim (int): The dimension of the token embeddings.\n        depth (int): The number of transformer layers.\n        num_tokens (int): The number of tokens in the vocabulary.\n        heads (int, optional): The number of attention heads in the transformer. Defaults to 8.\n        ff_mult (int, optional): The multiplier for the feed-forward layer dimension. Defaults to 4.\n    Examples:",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_transformer",
        "documentation": {}
    },
    {
        "label": "l2norm",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_transformer",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_transformer",
        "peekOfCode": "def l2norm(t, dim=-1):\n    return F.normalize(t, dim=dim)\nclass RMSNorm(nn.Module):\n    \"\"\"\n    Root Mean Square Normalization (RMSNorm) module.\n    Args:\n        dim (int): The input dimension.\n        affine (bool, optional): If True, apply an affine transformation to the normalized output.\n            Default is True.\n    Attributes:",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.bit_transformer",
        "documentation": {}
    },
    {
        "label": "BitNetInference",
        "kind": 6,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.inference",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.inference",
        "peekOfCode": "class BitNetInference:\n    \"\"\"\n    A class used to perform inference with the BitNetTransformer model.\n    ...\n    Attributes\n    ----------\n    model : torch.nn.Module\n        an instance of the BitNetTransformer model\n    device : str\n        the device to run the model on ('cpu' or 'cuda')",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.inference",
        "documentation": {}
    },
    {
        "label": "replace_linears_in_hf",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.replace_hf",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.replace_hf",
        "peekOfCode": "def replace_linears_in_hf(\n    model,\n):\n    \"\"\"\n    Replaces all instances of nn.Linear in the given model with BitLinear15b.\n    Args:\n        model (nn.Module): The model to modify.\n    Returns:\n        None\n    \"\"\"",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.replace_hf",
        "documentation": {}
    },
    {
        "label": "replace_linears_in_pytorch_model",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.replace_hf",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.replace_hf",
        "peekOfCode": "def replace_linears_in_pytorch_model(\n    model,\n):\n    \"\"\"\n    Replaces all instances of nn.Linear in the given model with BitLinear15b.\n    Args:\n        model (nn.Module): The model to modify.\n    Returns:\n        None\n    \"\"\"",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bitnet.replace_hf",
        "documentation": {}
    },
    {
        "label": "test_absmax_quantize",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_absmax_quantize():\n    tensor = torch.tensor([1.5, -2.0, 3.0, -4.0])\n    quant, dequant = absmax_quantize(tensor)\n    assert quant.dtype == torch.int8\n    assert torch.allclose(dequant, tensor, atol=1e-2)\ndef test_bitlinear_initialization():\n    layer = BitLinear(10, 20)\n    assert layer.in_features == 10\n    assert layer.out_features == 20\n    assert layer.weight.shape == (20, 10)",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_bitlinear_initialization",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_bitlinear_initialization():\n    layer = BitLinear(10, 20)\n    assert layer.in_features == 10\n    assert layer.out_features == 20\n    assert layer.weight.shape == (20, 10)\ndef test_bitlinear_forward():\n    layer = BitLinear(10, 20)\n    input_tensor = torch.randn(5, 10)\n    output = layer(input_tensor)\n    assert output.shape == (5, 20)",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_bitlinear_forward",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_bitlinear_forward():\n    layer = BitLinear(10, 20)\n    input_tensor = torch.randn(5, 10)\n    output = layer(input_tensor)\n    assert output.shape == (5, 20)\n# Fixtures:\n@pytest.fixture\ndef random_tensor():\n    return torch.randn(5, 10)\n# Parameterized Testing:",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "random_tensor",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def random_tensor():\n    return torch.randn(5, 10)\n# Parameterized Testing:\n@pytest.mark.parametrize(\"bits\", [4, 8, 12, 16])\ndef test_absmax_quantize_bits(random_tensor, bits):\n    quant, dequant = absmax_quantize(random_tensor, bits=bits)\n    assert quant.dtype == torch.int8\n    assert torch.allclose(dequant, random_tensor, atol=1e-2)\n# More Tests for BitLinear:\n@pytest.mark.parametrize(",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_absmax_quantize_bits",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_absmax_quantize_bits(random_tensor, bits):\n    quant, dequant = absmax_quantize(random_tensor, bits=bits)\n    assert quant.dtype == torch.int8\n    assert torch.allclose(dequant, random_tensor, atol=1e-2)\n# More Tests for BitLinear:\n@pytest.mark.parametrize(\n    \"in_features,out_features\", [(10, 20), (20, 40), (5, 10), (15, 10)]\n)\ndef test_bitlinear_shapes(in_features, out_features):\n    layer = BitLinear(in_features, out_features)",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_bitlinear_shapes",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_bitlinear_shapes(in_features, out_features):\n    layer = BitLinear(in_features, out_features)\n    assert layer.weight.shape == (out_features, in_features)\n@pytest.mark.parametrize(\"groups\", [1, 2, 5])\ndef test_bitlinear_groups(groups):\n    layer = BitLinear(10, 20, groups=groups)\n    assert layer.groups == groups\ndef test_bitlinear_reset_parameters():\n    layer = BitLinear(10, 20)\n    original_weights = layer.weight.clone()",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_bitlinear_groups",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_bitlinear_groups(groups):\n    layer = BitLinear(10, 20, groups=groups)\n    assert layer.groups == groups\ndef test_bitlinear_reset_parameters():\n    layer = BitLinear(10, 20)\n    original_weights = layer.weight.clone()\n    layer.reset_parameters()\n    assert not torch.equal(original_weights, layer.weight)\n@pytest.mark.parametrize(\"groups\", [1, 2, 5])\ndef test_bitlinear_forward_with_groups(random_tensor, groups):",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_bitlinear_reset_parameters",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_bitlinear_reset_parameters():\n    layer = BitLinear(10, 20)\n    original_weights = layer.weight.clone()\n    layer.reset_parameters()\n    assert not torch.equal(original_weights, layer.weight)\n@pytest.mark.parametrize(\"groups\", [1, 2, 5])\ndef test_bitlinear_forward_with_groups(random_tensor, groups):\n    layer = BitLinear(10, 20, groups=groups)\n    output = layer(random_tensor)\n    assert output.shape == (5, 20)",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_bitlinear_forward_with_groups",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_bitlinear_forward_with_groups(random_tensor, groups):\n    layer = BitLinear(10, 20, groups=groups)\n    output = layer(random_tensor)\n    assert output.shape == (5, 20)\ndef test_bitlinear_zero_input():\n    layer = BitLinear(10, 20)\n    input_tensor = torch.zeros(5, 10)\n    output = layer(input_tensor)\n    assert torch.allclose(output, torch.zeros(5, 20), atol=1e-2)\ndef test_bitlinear_weight_sign():",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_bitlinear_zero_input",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_bitlinear_zero_input():\n    layer = BitLinear(10, 20)\n    input_tensor = torch.zeros(5, 10)\n    output = layer(input_tensor)\n    assert torch.allclose(output, torch.zeros(5, 20), atol=1e-2)\ndef test_bitlinear_weight_sign():\n    layer = BitLinear(10, 20)\n    input_tensor = torch.randn(5, 10)\n    output_before = layer(input_tensor)\n    layer.weight.data = torch.abs(layer.weight.data)",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_bitlinear_weight_sign",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_bitlinear_weight_sign():\n    layer = BitLinear(10, 20)\n    input_tensor = torch.randn(5, 10)\n    output_before = layer(input_tensor)\n    layer.weight.data = torch.abs(layer.weight.data)\n    output_after = layer(input_tensor)\n    assert not torch.allclose(output_before, output_after)\n@pytest.mark.parametrize(\"groups\", [1, 2, 5])\ndef test_bitlinear_weight_group_normalization(groups):\n    layer = BitLinear(10, 20, groups=groups)",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_bitlinear_weight_group_normalization",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_bitlinear_weight_group_normalization(groups):\n    layer = BitLinear(10, 20, groups=groups)\n    weight = layer.weight.view(groups, -1)\n    mean = weight.mean(dim=1, keepdim=True)\n    assert torch.allclose(mean, torch.zeros_like(mean), atol=1e-2)\ndef test_bitlinear_weight_group_scaling():\n    layer = BitLinear(10, 20, groups=5)\n    weight = layer.weight.view(layer.groups, -1)\n    beta = torch.abs(weight).sum(dim=1, keepdim=True) / (\n        weight.shape[0] * weight.shape[1]",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_bitlinear_weight_group_scaling",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_bitlinear_weight_group_scaling():\n    layer = BitLinear(10, 20, groups=5)\n    weight = layer.weight.view(layer.groups, -1)\n    beta = torch.abs(weight).sum(dim=1, keepdim=True) / (\n        weight.shape[0] * weight.shape[1]\n    )\n    scaled_weight = weight * beta\n    assert torch.allclose(scaled_weight, layer.weight.view(20, 10))\ndef test_bitlinear_input_quantization(random_tensor):\n    layer = BitLinear(10, 20)",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_bitlinear_input_quantization",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_bitlinear_input_quantization(random_tensor):\n    layer = BitLinear(10, 20)\n    quant_input, _ = absmax_quantize(random_tensor)\n    output = layer(quant_input.float())\n    assert output.shape == (5, 20)\n# ... Continue adding more tests ...\n# - Test the forward pass with extreme input values.\n# - Test with different types of input tensors (e.g., int8, float16).\n# - Test the forward pass with batch sizes other than 5.\n# - Verify that using different initializations produces different results.",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "random_tensor",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def random_tensor():\n    \"\"\"A fixture to generate a random tensor\"\"\"\n    return torch.randn(32, 512)\n@pytest.fixture\ndef bitnet_model():\n    \"\"\"A fixture to create an instance of BitNetTransformer model\"\"\"\n    return BitNetTransformer(\n        num_tokens=20000,\n        dim=512,\n        depth=6,",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "bitnet_model",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def bitnet_model():\n    \"\"\"A fixture to create an instance of BitNetTransformer model\"\"\"\n    return BitNetTransformer(\n        num_tokens=20000,\n        dim=512,\n        depth=6,\n        dim_head=64,\n        heads=8,\n        ff_mult=4,\n    )",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_parallel_transformer_block",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_parallel_transformer_block(dim, dim_head, heads, ff_mult, random_tensor):\n    block = ParallelTransformerBlock(dim, dim_head, heads, ff_mult)\n    output = block(random_tensor)\n    assert output.shape == random_tensor.shape\n@pytest.mark.parametrize(\n    \"dim, depth, heads, dim_head, ff_mult\",\n    [\n        (512, 6, 8, 64, 4),\n        (256, 3, 4, 32, 2),\n        (128, 2, 2, 16, 1),",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_transformer",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_transformer(dim, depth, heads, dim_head, ff_mult, random_tensor):\n    transformer = Transformer(dim, depth, heads, dim_head, ff_mult)\n    output = transformer(random_tensor)\n    assert output.shape == random_tensor.shape\ndef test_bitnet_transformer_forward(bitnet_model):\n    tokens = torch.randint(0, 20000, (1, 512))\n    logits = bitnet_model(tokens)\n    assert logits.shape == (1, 20000)\ndef test_parallel_transformer_block_masking(random_tensor):\n    block = ParallelTransformerBlock(512, 64, 8, 4)",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_bitnet_transformer_forward",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_bitnet_transformer_forward(bitnet_model):\n    tokens = torch.randint(0, 20000, (1, 512))\n    logits = bitnet_model(tokens)\n    assert logits.shape == (1, 20000)\ndef test_parallel_transformer_block_masking(random_tensor):\n    block = ParallelTransformerBlock(512, 64, 8, 4)\n    mask1 = block.get_mask(100, random_tensor.device)\n    mask2 = block.get_mask(200, random_tensor.device)\n    assert mask1.shape == (100, 100)\n    assert mask2.shape == (200, 200)",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_parallel_transformer_block_masking",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_parallel_transformer_block_masking(random_tensor):\n    block = ParallelTransformerBlock(512, 64, 8, 4)\n    mask1 = block.get_mask(100, random_tensor.device)\n    mask2 = block.get_mask(200, random_tensor.device)\n    assert mask1.shape == (100, 100)\n    assert mask2.shape == (200, 200)\ndef test_bitnet_transformer_embed(bitnet_model):\n    tokens = torch.randint(0, 20000, (1, 512))\n    embedded = bitnet_model.emb(tokens)\n    assert embedded.shape == (1, 512, 512)",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_bitnet_transformer_embed",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_bitnet_transformer_embed(bitnet_model):\n    tokens = torch.randint(0, 20000, (1, 512))\n    embedded = bitnet_model.emb(tokens)\n    assert embedded.shape == (1, 512, 512)\n@pytest.mark.parametrize(\n    \"dim, dim_head, heads, ff_mult\",\n    [\n        (512, 64, 8, 4),\n        (256, 32, 4, 2),\n        (128, 16, 2, 1),",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_parallel_transformer_block_raises_for_incorrect_input",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_parallel_transformer_block_raises_for_incorrect_input(\n    dim, dim_head, heads, ff_mult\n):\n    block = ParallelTransformerBlock(dim, dim_head, heads, ff_mult)\n    with pytest.raises(Exception):\n        block(torch.randn(32, 100))\n@pytest.mark.parametrize(\n    \"batch_size, seq_len\",\n    [\n        (1, 512),",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_bitnet_transformer_for_various_input_shapes",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_bitnet_transformer_for_various_input_shapes(bitnet_model, batch_size, seq_len):\n    tokens = torch.randint(0, 20000, (batch_size, seq_len))\n    logits = bitnet_model(tokens)\n    assert logits.shape == (batch_size, 20000)\ndef test_rotary_embedding(bitnet_model, random_tensor):\n    block = ParallelTransformerBlock(512, 64, 8, 4)\n    rotary_emb1 = block.get_rotary_embedding(100, random_tensor.device)\n    rotary_emb2 = block.get_rotary_embedding(200, random_tensor.device)\n    assert rotary_emb1.shape == (100, 64)\n    assert rotary_emb2.shape == (200, 64)",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_rotary_embedding",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_rotary_embedding(bitnet_model, random_tensor):\n    block = ParallelTransformerBlock(512, 64, 8, 4)\n    rotary_emb1 = block.get_rotary_embedding(100, random_tensor.device)\n    rotary_emb2 = block.get_rotary_embedding(200, random_tensor.device)\n    assert rotary_emb1.shape == (100, 64)\n    assert rotary_emb2.shape == (200, 64)\n@pytest.mark.parametrize(\"mask_value\", [100, 200, 300])\ndef test_mask_persistency(random_tensor, mask_value):\n    block = ParallelTransformerBlock(512, 64, 8, 4)\n    block.get_mask(mask_value, random_tensor.device)",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_mask_persistency",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_mask_persistency(random_tensor, mask_value):\n    block = ParallelTransformerBlock(512, 64, 8, 4)\n    block.get_mask(mask_value, random_tensor.device)\n    assert block.mask.shape[0] == mask_value\n@pytest.mark.parametrize(\n    \"input_value, expected_output_shape\",\n    [\n        (torch.randint(0, 20000, (1, 512)), (1, 20000)),\n        (torch.randint(0, 20000, (32, 256)), (32, 20000)),\n    ],",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_bitnet_transformer_output_shapes",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_bitnet_transformer_output_shapes(\n    bitnet_model, input_value, expected_output_shape\n):\n    logits = bitnet_model(input_value)\n    assert logits.shape == expected_output_shape\ndef test_exceptions_on_wrong_dtype():\n    block = ParallelTransformerBlock(512, 64, 8, 4)\n    with pytest.raises(Exception):\n        block(torch.randn(32, 512).int())\ndef test_bitnet_transformer_logit_values(bitnet_model):",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_exceptions_on_wrong_dtype",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_exceptions_on_wrong_dtype():\n    block = ParallelTransformerBlock(512, 64, 8, 4)\n    with pytest.raises(Exception):\n        block(torch.randn(32, 512).int())\ndef test_bitnet_transformer_logit_values(bitnet_model):\n    tokens = torch.randint(0, 20000, (1, 512))\n    logits = bitnet_model(tokens)\n    probs = F.softmax(logits, dim=-1)\n    assert torch.allclose(probs.sum(dim=-1), torch.tensor(1.0))\n# Mocking and Monkeypatching",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_bitnet_transformer_logit_values",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_bitnet_transformer_logit_values(bitnet_model):\n    tokens = torch.randint(0, 20000, (1, 512))\n    logits = bitnet_model(tokens)\n    probs = F.softmax(logits, dim=-1)\n    assert torch.allclose(probs.sum(dim=-1), torch.tensor(1.0))\n# Mocking and Monkeypatching\ndef test_mocking_get_mask(monkeypatch, random_tensor):\n    mock_mask = torch.zeros(100, 100)\n    monkeypatch.setattr(\n        ParallelTransformerBlock, \"get_mask\", lambda self, n, device: mock_mask",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_mocking_get_mask",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "peekOfCode": "def test_mocking_get_mask(monkeypatch, random_tensor):\n    mock_mask = torch.zeros(100, 100)\n    monkeypatch.setattr(\n        ParallelTransformerBlock, \"get_mask\", lambda self, n, device: mock_mask\n    )\n    block = ParallelTransformerBlock(512, 64, 8, 4)\n    assert torch.equal(block.get_mask(100, random_tensor.device), mock_mask)\n# Add more tests based on the scenarios and edge cases you want to cover.",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.tests.tests",
        "documentation": {}
    },
    {
        "label": "test_bitfeedforward_initialization",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.tests.test_bitffn",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.tests.test_bitffn",
        "peekOfCode": "def test_bitfeedforward_initialization():\n    bitffn = BitFeedForward(dim=512, ff_mult=4)\n    assert isinstance(bitffn.layer, nn.Sequential)\n    assert len(bitffn.layer) == 3\n    assert isinstance(bitffn.layer[0], BitLinear)\n    assert isinstance(bitffn.layer[1], nn.GELU)\n    assert isinstance(bitffn.layer[2], BitLinear)\n    assert bitffn.layer[0].out_features == 512 * 4\n    assert bitffn.layer[2].in_features == 512 * 4\ndef test_bitfeedforward_forward_pass():",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.tests.test_bitffn",
        "documentation": {}
    },
    {
        "label": "test_bitfeedforward_forward_pass",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.tests.test_bitffn",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.tests.test_bitffn",
        "peekOfCode": "def test_bitfeedforward_forward_pass():\n    bitffn = BitFeedForward(dim=512, ff_mult=4)\n    x = torch.randn(1, 512)\n    out = bitffn(x)\n    assert out.shape == x.shape",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.tests.test_bitffn",
        "documentation": {}
    },
    {
        "label": "test_bitlinear_initialization",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.tests.test_bitlinear",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.tests.test_bitlinear",
        "peekOfCode": "def test_bitlinear_initialization():\n    bitlinear = BitLinear(in_features=512, out_features=256, bias=True)\n    assert bitlinear.in_features == 512\n    assert bitlinear.out_features == 256\n    assert bitlinear.weight.shape == (256, 512)\n    assert bitlinear.bias.shape == (256,)\n    assert bitlinear.gamma.shape == (512,)\n    assert bitlinear.beta.shape == (256,)\ndef test_bitlinear_forward_pass():\n    bitlinear = BitLinear(in_features=512, out_features=256, bias=True)",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.tests.test_bitlinear",
        "documentation": {}
    },
    {
        "label": "test_bitlinear_forward_pass",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.tests.test_bitlinear",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.tests.test_bitlinear",
        "peekOfCode": "def test_bitlinear_forward_pass():\n    bitlinear = BitLinear(in_features=512, out_features=256, bias=True)\n    x = torch.randn(1, 512)\n    out = bitlinear(x)\n    assert out.shape == (1, 256)\ndef test_bitlinear_no_bias():\n    bitlinear = BitLinear(in_features=512, out_features=256, bias=False)\n    assert bitlinear.bias is None\ndef test_bitlinear_quantization():\n    bitlinear = BitLinear(in_features=512, out_features=256, bias=True)",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.tests.test_bitlinear",
        "documentation": {}
    },
    {
        "label": "test_bitlinear_no_bias",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.tests.test_bitlinear",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.tests.test_bitlinear",
        "peekOfCode": "def test_bitlinear_no_bias():\n    bitlinear = BitLinear(in_features=512, out_features=256, bias=False)\n    assert bitlinear.bias is None\ndef test_bitlinear_quantization():\n    bitlinear = BitLinear(in_features=512, out_features=256, bias=True)\n    x = torch.randn(1, 512)\n    out = bitlinear(x)\n    assert torch.all(out <= bitlinear.beta.unsqueeze(0).expand_as(out))\n    assert torch.all(out >= -bitlinear.beta.unsqueeze(0).expand_as(out))",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.tests.test_bitlinear",
        "documentation": {}
    },
    {
        "label": "test_bitlinear_quantization",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.tests.test_bitlinear",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.tests.test_bitlinear",
        "peekOfCode": "def test_bitlinear_quantization():\n    bitlinear = BitLinear(in_features=512, out_features=256, bias=True)\n    x = torch.randn(1, 512)\n    out = bitlinear(x)\n    assert torch.all(out <= bitlinear.beta.unsqueeze(0).expand_as(out))\n    assert torch.all(out >= -bitlinear.beta.unsqueeze(0).expand_as(out))",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.tests.test_bitlinear",
        "documentation": {}
    },
    {
        "label": "test_bitnet_transformer_initialization",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.tests.test_transformer",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.tests.test_transformer",
        "peekOfCode": "def test_bitnet_transformer_initialization():\n    bitnet = BitNetTransformer(num_tokens=20000, dim=512, heads=8, depth=6, ff_mult=4)\n    assert len(bitnet.layers) == 6\n    assert len(bitnet.ffn_layers) == 6\n    assert all(isinstance(layer, MultiheadAttention) for layer in bitnet.layers)\n    assert all(isinstance(layer, BitFeedForward) for layer in bitnet.ffn_layers)\ndef test_bitnet_transformer_forward_pass():\n    bitnet = BitNetTransformer(num_tokens=20000, dim=512, heads=8, depth=6, ff_mult=4)\n    x = torch.randn(1, 100, 512)\n    out = bitnet(x)",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.tests.test_transformer",
        "documentation": {}
    },
    {
        "label": "test_bitnet_transformer_forward_pass",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.tests.test_transformer",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.tests.test_transformer",
        "peekOfCode": "def test_bitnet_transformer_forward_pass():\n    bitnet = BitNetTransformer(num_tokens=20000, dim=512, heads=8, depth=6, ff_mult=4)\n    x = torch.randn(1, 100, 512)\n    out = bitnet(x)\n    assert out.shape == x.shape",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.tests.test_transformer",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bit_attention",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bit_attention",
        "peekOfCode": "x = torch.randn(1, 10, 512)\n# Create an instance of the BitMGQA model with input size 512, 8 attention heads, and 4 layers\ngqa = BitMGQA(512, 8, 4)\n# Pass the input tensor through the BitMGQA model and get the output and attention weights\nout, _ = gqa(x, x, x, need_weights=True)\n# Print the shapes of the output tensor and attention tensor\nprint(out)",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bit_attention",
        "documentation": {}
    },
    {
        "label": "gqa",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bit_attention",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bit_attention",
        "peekOfCode": "gqa = BitMGQA(512, 8, 4)\n# Pass the input tensor through the BitMGQA model and get the output and attention weights\nout, _ = gqa(x, x, x, need_weights=True)\n# Print the shapes of the output tensor and attention tensor\nprint(out)",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bit_attention",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bit_ffn",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bit_ffn",
        "peekOfCode": "x = torch.randn(10, 512)\n# Create an instance of the BitFeedForward class with the following parameters:\n# - input_dim: 512\n# - hidden_dim: 512\n# - num_layers: 4\n# - swish: True (use Swish activation function)\n# - post_act_ln: True (apply Layer Normalization after each activation)\n# - dropout: 0.1 (apply dropout with a probability of 0.1)\nff = BitFeedForward(512, 512, 4, swish=True, post_act_ln=True, dropout=0.1)\n# Apply the BitFeedForward network to the input tensor x",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bit_ffn",
        "documentation": {}
    },
    {
        "label": "ff",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bit_ffn",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bit_ffn",
        "peekOfCode": "ff = BitFeedForward(512, 512, 4, swish=True, post_act_ln=True, dropout=0.1)\n# Apply the BitFeedForward network to the input tensor x\ny = ff(x)\n# Print the shape of the output tensor y\nprint(y)  # torch.Size([10, 512])",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bit_ffn",
        "documentation": {}
    },
    {
        "label": "y",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bit_ffn",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bit_ffn",
        "peekOfCode": "y = ff(x)\n# Print the shape of the output tensor y\nprint(y)  # torch.Size([10, 512])",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bit_ffn",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bit_linear_new",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bit_linear_new",
        "peekOfCode": "x = torch.randn(16, 1000, 512)\n# Create an instance of the BitLinearNew class with input size 10, output size 20, and 2 groups\nlayer = BitLinearNew(\n    512,\n    20,\n)\n# Perform a forward pass through the BitLinearNew layer with input x\noutput = layer(x)\n# Print the output tensor\nprint(output)",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bit_linear_new",
        "documentation": {}
    },
    {
        "label": "layer",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bit_linear_new",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bit_linear_new",
        "peekOfCode": "layer = BitLinearNew(\n    512,\n    20,\n)\n# Perform a forward pass through the BitLinearNew layer with input x\noutput = layer(x)\n# Print the output tensor\nprint(output)\nprint(output.shape)",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bit_linear_new",
        "documentation": {}
    },
    {
        "label": "output",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bit_linear_new",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bit_linear_new",
        "peekOfCode": "output = layer(x)\n# Print the output tensor\nprint(output)\nprint(output.shape)",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bit_linear_new",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bit_mamba",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bit_mamba",
        "peekOfCode": "x = torch.randint(0, 100, (2, 10))\n# Create an instance of the BitMamba model with input size 512, hidden size 100, output size 10, and depth size 6\nmodel = BitMamba(512, 100, 10, 6, return_tokens=True)\n# Pass the input tensor through the model and get the output\noutput = model(x)\n# Print the output tensor\nprint(output)\n# Print the shape of the output tensor\nprint(output.shape)",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bit_mamba",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bit_mamba",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bit_mamba",
        "peekOfCode": "model = BitMamba(512, 100, 10, 6, return_tokens=True)\n# Pass the input tensor through the model and get the output\noutput = model(x)\n# Print the output tensor\nprint(output)\n# Print the shape of the output tensor\nprint(output.shape)",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bit_mamba",
        "documentation": {}
    },
    {
        "label": "output",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bit_mamba",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bit_mamba",
        "peekOfCode": "output = model(x)\n# Print the output tensor\nprint(output)\n# Print the shape of the output tensor\nprint(output.shape)",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bit_mamba",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bit_moe_example",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bit_moe_example",
        "peekOfCode": "x = torch.randn(2, 4, 8)\n# Create BitMoE model with specified input and output dimensions\nmodel = BitMoE(8, 4, 2)\n# Forward pass through the model\noutput = model(x)\n# Print the output\nprint(output)",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bit_moe_example",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bit_moe_example",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bit_moe_example",
        "peekOfCode": "model = BitMoE(8, 4, 2)\n# Forward pass through the model\noutput = model(x)\n# Print the output\nprint(output)",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bit_moe_example",
        "documentation": {}
    },
    {
        "label": "output",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.bit_moe_example",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.bit_moe_example",
        "peekOfCode": "output = model(x)\n# Print the output\nprint(output)",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.bit_moe_example",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.example",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.example",
        "peekOfCode": "x = torch.randn(10, 10000, 512)\n# BitLinear layer\nlayer = BitLinear(512, 400)\n# Output\ny = layer(x)\nprint(y)\nprint(y.shape)",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.example",
        "documentation": {}
    },
    {
        "label": "layer",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.example",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.example",
        "peekOfCode": "layer = BitLinear(512, 400)\n# Output\ny = layer(x)\nprint(y)\nprint(y.shape)",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.example",
        "documentation": {}
    },
    {
        "label": "y",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.example",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.example",
        "peekOfCode": "y = layer(x)\nprint(y)\nprint(y.shape)",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.example",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.huggingface_example",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.huggingface_example",
        "peekOfCode": "model_name = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\n# Replace Linear layers with BitLinear\nreplace_linears_in_hf(model)\n# Example text to classify\ntext = \"Replace this with your text\"\ninputs = tokenizer(\n    text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512\n)",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.huggingface_example",
        "documentation": {}
    },
    {
        "label": "tokenizer",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.huggingface_example",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.huggingface_example",
        "peekOfCode": "tokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\n# Replace Linear layers with BitLinear\nreplace_linears_in_hf(model)\n# Example text to classify\ntext = \"Replace this with your text\"\ninputs = tokenizer(\n    text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512\n)\n# Perform inference",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.huggingface_example",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.huggingface_example",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.huggingface_example",
        "peekOfCode": "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n# Replace Linear layers with BitLinear\nreplace_linears_in_hf(model)\n# Example text to classify\ntext = \"Replace this with your text\"\ninputs = tokenizer(\n    text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512\n)\n# Perform inference\nmodel.eval()  # Set the model to evaluation mode",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.huggingface_example",
        "documentation": {}
    },
    {
        "label": "text",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.huggingface_example",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.huggingface_example",
        "peekOfCode": "text = \"Replace this with your text\"\ninputs = tokenizer(\n    text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512\n)\n# Perform inference\nmodel.eval()  # Set the model to evaluation mode\nwith torch.no_grad():\n    outputs = model(**inputs)\n    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n    print(predictions)",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.huggingface_example",
        "documentation": {}
    },
    {
        "label": "inputs",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.huggingface_example",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.huggingface_example",
        "peekOfCode": "inputs = tokenizer(\n    text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512\n)\n# Perform inference\nmodel.eval()  # Set the model to evaluation mode\nwith torch.no_grad():\n    outputs = model(**inputs)\n    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n    print(predictions)\n# Process predictions",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.huggingface_example",
        "documentation": {}
    },
    {
        "label": "predicted_class_id",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.huggingface_example",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.huggingface_example",
        "peekOfCode": "predicted_class_id = predictions.argmax().item()\nprint(f\"Predicted class ID: {predicted_class_id}\")\n# Optionally, map the predicted class ID to a label, if you know the classification labels\n# labels = [\"Label 1\", \"Label 2\", ...]  # Define your labels corresponding to the model's classes\n# print(f\"Predicted label: {labels[predicted_class_id]}\")",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.huggingface_example",
        "documentation": {}
    },
    {
        "label": "a",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.kernel_test",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.kernel_test",
        "peekOfCode": "a = torch.randn(10, 20, dtype=torch.half, device=\"cuda\")  # Example tensor\nb = torch.randn(20, 30, dtype=torch.half, device=\"cuda\")  # Example tensor\nc = torch.empty(10, 30, dtype=torch.half, device=\"cuda\")  # Output tensor\nw_scale = 1.0  # Example scale factor\nx_scale = 1.0  # Example scale factor\n# Call the custom CUDA GEMM operation\ngemm_lowbit(a, b, c, w_scale, x_scale)\nprint(c)  # View the result",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.kernel_test",
        "documentation": {}
    },
    {
        "label": "b",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.kernel_test",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.kernel_test",
        "peekOfCode": "b = torch.randn(20, 30, dtype=torch.half, device=\"cuda\")  # Example tensor\nc = torch.empty(10, 30, dtype=torch.half, device=\"cuda\")  # Output tensor\nw_scale = 1.0  # Example scale factor\nx_scale = 1.0  # Example scale factor\n# Call the custom CUDA GEMM operation\ngemm_lowbit(a, b, c, w_scale, x_scale)\nprint(c)  # View the result",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.kernel_test",
        "documentation": {}
    },
    {
        "label": "c",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.kernel_test",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.kernel_test",
        "peekOfCode": "c = torch.empty(10, 30, dtype=torch.half, device=\"cuda\")  # Output tensor\nw_scale = 1.0  # Example scale factor\nx_scale = 1.0  # Example scale factor\n# Call the custom CUDA GEMM operation\ngemm_lowbit(a, b, c, w_scale, x_scale)\nprint(c)  # View the result",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.kernel_test",
        "documentation": {}
    },
    {
        "label": "w_scale",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.kernel_test",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.kernel_test",
        "peekOfCode": "w_scale = 1.0  # Example scale factor\nx_scale = 1.0  # Example scale factor\n# Call the custom CUDA GEMM operation\ngemm_lowbit(a, b, c, w_scale, x_scale)\nprint(c)  # View the result",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.kernel_test",
        "documentation": {}
    },
    {
        "label": "x_scale",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.kernel_test",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.kernel_test",
        "peekOfCode": "x_scale = 1.0  # Example scale factor\n# Call the custom CUDA GEMM operation\ngemm_lowbit(a, b, c, w_scale, x_scale)\nprint(c)  # View the result",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.kernel_test",
        "documentation": {}
    },
    {
        "label": "CachedWheelsCommand",
        "kind": 6,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.setup",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.setup",
        "peekOfCode": "class CachedWheelsCommand(_bdist_wheel):\n    \"\"\"\n    The CachedWheelsCommand plugs into the default bdist wheel, which is ran by pip when it cannot\n    find an existing wheel (which is currently the case for all installs). We use\n    the environment parameters to detect whether there is already a pre-built version of a compatible\n    wheel available and short-circuits the standard full build pipeline.\n    \"\"\"\n    def run(self):\n        if FORCE_BUILD:\n            return super().run()",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.setup",
        "documentation": {}
    },
    {
        "label": "get_platform",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.setup",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.setup",
        "peekOfCode": "def get_platform():\n    \"\"\"\n    Returns the platform name as used in wheel filenames.\n    \"\"\"\n    if sys.platform.startswith(\"linux\"):\n        return \"linux_x86_64\"\n    elif sys.platform == \"darwin\":\n        mac_version = \".\".join(platform.mac_ver()[0].split(\".\")[:2])\n        return f\"macosx_{mac_version}_x86_64\"\n    elif sys.platform == \"win32\":",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.setup",
        "documentation": {}
    },
    {
        "label": "get_cuda_bare_metal_version",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.setup",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.setup",
        "peekOfCode": "def get_cuda_bare_metal_version(cuda_dir):\n    raw_output = subprocess.check_output(\n        [cuda_dir + \"/bin/nvcc\", \"-V\"], universal_newlines=True\n    )\n    output = raw_output.split()\n    release_idx = output.index(\"release\") + 1\n    bare_metal_version = parse(output[release_idx].split(\",\")[0])\n    return raw_output, bare_metal_version\ndef check_if_cuda_home_none(global_option: str) -> None:\n    if CUDA_HOME is not None:",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.setup",
        "documentation": {}
    },
    {
        "label": "check_if_cuda_home_none",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.setup",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.setup",
        "peekOfCode": "def check_if_cuda_home_none(global_option: str) -> None:\n    if CUDA_HOME is not None:\n        return\n    # warn instead of error because user could be downloading prebuilt wheels, so nvcc won't be necessary\n    # in that case.\n    warnings.warn(\n        f\"{global_option} was requested, but nvcc was not found.  Are you sure your environment has nvcc available?  \"\n        \"If you're installing within a container from https://hub.docker.com/r/pytorch/pytorch, \"\n        \"only images whose names contain 'devel' will provide nvcc.\"\n    )",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.setup",
        "documentation": {}
    },
    {
        "label": "append_nvcc_threads",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.setup",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.setup",
        "peekOfCode": "def append_nvcc_threads(nvcc_extra_args):\n    return nvcc_extra_args + [\"--threads\", \"4\"]\ncmdclass = {}\next_modules = []\nif not SKIP_CUDA_BUILD:\n    print(\"\\n\\ntorch.__version__  = {}\\n\\n\".format(torch.__version__))\n    TORCH_MAJOR = int(torch.__version__.split(\".\")[0])\n    TORCH_MINOR = int(torch.__version__.split(\".\")[1])\n    check_if_cuda_home_none(PACKAGE_NAME)\n    # Check, if CUDA11 is installed for compute capability 8.0",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.setup",
        "documentation": {}
    },
    {
        "label": "get_package_version",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.setup",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.setup",
        "peekOfCode": "def get_package_version():\n    with open(Path(this_dir) / PACKAGE_NAME / \"__init__.py\", \"r\") as f:\n        version_match = re.search(r\"^__version__\\s*=\\s*(.*)$\", f.read(), re.MULTILINE)\n    public_version = ast.literal_eval(version_match.group(1))\n    local_version = os.environ.get(\"MAMBA_LOCAL_VERSION\")\n    if local_version:\n        return f\"{public_version}+{local_version}\"\n    else:\n        return str(public_version)\ndef get_wheel_url():",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.setup",
        "documentation": {}
    },
    {
        "label": "get_wheel_url",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.setup",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.setup",
        "peekOfCode": "def get_wheel_url():\n    # Determine the version numbers that will be used to determine the correct wheel\n    # We're using the CUDA version used to build torch, not the one currently installed\n    # _, cuda_version_raw = get_cuda_bare_metal_version(CUDA_HOME)\n    torch_cuda_version = parse(torch.version.cuda)\n    torch_version_raw = parse(torch.__version__)\n    # For CUDA 11, we only compile for CUDA 11.8, and for CUDA 12 we only compile for CUDA 12.2\n    # to save CI time. Minor versions should be compatible.\n    torch_cuda_version = (\n        parse(\"11.8\") if torch_cuda_version.major == 11 else parse(\"12.2\")",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.setup",
        "documentation": {}
    },
    {
        "label": "this_dir",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.setup",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.setup",
        "peekOfCode": "this_dir = os.path.dirname(os.path.abspath(__file__))\nPACKAGE_NAME = \"bitnet\"\nBASE_WHEEL_URL = (\n    \"https://github.com/kyegomez/BitNet/releases/download/{tag_name}/{wheel_name}\"\n)\n# FORCE_BUILD: Force a fresh build locally, instead of attempting to find prebuilt wheels\n# SKIP_CUDA_BUILD: Intended to allow CI to use a simple `python setup.py sdist` run to copy over raw files, without any cuda compilation\nFORCE_BUILD = os.getenv(\"MAMBA_FORCE_BUILD\", \"FALSE\") == \"TRUE\"\nSKIP_CUDA_BUILD = os.getenv(\"MAMBA_SKIP_CUDA_BUILD\", \"FALSE\") == \"TRUE\"\n# For CI, we want the option to build with C++11 ABI since the nvcr images use C++11 ABI",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.setup",
        "documentation": {}
    },
    {
        "label": "PACKAGE_NAME",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.setup",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.setup",
        "peekOfCode": "PACKAGE_NAME = \"bitnet\"\nBASE_WHEEL_URL = (\n    \"https://github.com/kyegomez/BitNet/releases/download/{tag_name}/{wheel_name}\"\n)\n# FORCE_BUILD: Force a fresh build locally, instead of attempting to find prebuilt wheels\n# SKIP_CUDA_BUILD: Intended to allow CI to use a simple `python setup.py sdist` run to copy over raw files, without any cuda compilation\nFORCE_BUILD = os.getenv(\"MAMBA_FORCE_BUILD\", \"FALSE\") == \"TRUE\"\nSKIP_CUDA_BUILD = os.getenv(\"MAMBA_SKIP_CUDA_BUILD\", \"FALSE\") == \"TRUE\"\n# For CI, we want the option to build with C++11 ABI since the nvcr images use C++11 ABI\nFORCE_CXX11_ABI = os.getenv(\"MAMBA_FORCE_CXX11_ABI\", \"FALSE\") == \"TRUE\"",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.setup",
        "documentation": {}
    },
    {
        "label": "BASE_WHEEL_URL",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.setup",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.setup",
        "peekOfCode": "BASE_WHEEL_URL = (\n    \"https://github.com/kyegomez/BitNet/releases/download/{tag_name}/{wheel_name}\"\n)\n# FORCE_BUILD: Force a fresh build locally, instead of attempting to find prebuilt wheels\n# SKIP_CUDA_BUILD: Intended to allow CI to use a simple `python setup.py sdist` run to copy over raw files, without any cuda compilation\nFORCE_BUILD = os.getenv(\"MAMBA_FORCE_BUILD\", \"FALSE\") == \"TRUE\"\nSKIP_CUDA_BUILD = os.getenv(\"MAMBA_SKIP_CUDA_BUILD\", \"FALSE\") == \"TRUE\"\n# For CI, we want the option to build with C++11 ABI since the nvcr images use C++11 ABI\nFORCE_CXX11_ABI = os.getenv(\"MAMBA_FORCE_CXX11_ABI\", \"FALSE\") == \"TRUE\"\ndef get_platform():",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.setup",
        "documentation": {}
    },
    {
        "label": "FORCE_BUILD",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.setup",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.setup",
        "peekOfCode": "FORCE_BUILD = os.getenv(\"MAMBA_FORCE_BUILD\", \"FALSE\") == \"TRUE\"\nSKIP_CUDA_BUILD = os.getenv(\"MAMBA_SKIP_CUDA_BUILD\", \"FALSE\") == \"TRUE\"\n# For CI, we want the option to build with C++11 ABI since the nvcr images use C++11 ABI\nFORCE_CXX11_ABI = os.getenv(\"MAMBA_FORCE_CXX11_ABI\", \"FALSE\") == \"TRUE\"\ndef get_platform():\n    \"\"\"\n    Returns the platform name as used in wheel filenames.\n    \"\"\"\n    if sys.platform.startswith(\"linux\"):\n        return \"linux_x86_64\"",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.setup",
        "documentation": {}
    },
    {
        "label": "SKIP_CUDA_BUILD",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.setup",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.setup",
        "peekOfCode": "SKIP_CUDA_BUILD = os.getenv(\"MAMBA_SKIP_CUDA_BUILD\", \"FALSE\") == \"TRUE\"\n# For CI, we want the option to build with C++11 ABI since the nvcr images use C++11 ABI\nFORCE_CXX11_ABI = os.getenv(\"MAMBA_FORCE_CXX11_ABI\", \"FALSE\") == \"TRUE\"\ndef get_platform():\n    \"\"\"\n    Returns the platform name as used in wheel filenames.\n    \"\"\"\n    if sys.platform.startswith(\"linux\"):\n        return \"linux_x86_64\"\n    elif sys.platform == \"darwin\":",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.setup",
        "documentation": {}
    },
    {
        "label": "FORCE_CXX11_ABI",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.setup",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.setup",
        "peekOfCode": "FORCE_CXX11_ABI = os.getenv(\"MAMBA_FORCE_CXX11_ABI\", \"FALSE\") == \"TRUE\"\ndef get_platform():\n    \"\"\"\n    Returns the platform name as used in wheel filenames.\n    \"\"\"\n    if sys.platform.startswith(\"linux\"):\n        return \"linux_x86_64\"\n    elif sys.platform == \"darwin\":\n        mac_version = \".\".join(platform.mac_ver()[0].split(\".\")[:2])\n        return f\"macosx_{mac_version}_x86_64\"",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.setup",
        "documentation": {}
    },
    {
        "label": "cmdclass",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.setup",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.setup",
        "peekOfCode": "cmdclass = {}\next_modules = []\nif not SKIP_CUDA_BUILD:\n    print(\"\\n\\ntorch.__version__  = {}\\n\\n\".format(torch.__version__))\n    TORCH_MAJOR = int(torch.__version__.split(\".\")[0])\n    TORCH_MINOR = int(torch.__version__.split(\".\")[1])\n    check_if_cuda_home_none(PACKAGE_NAME)\n    # Check, if CUDA11 is installed for compute capability 8.0\n    cc_flag = []\n    if CUDA_HOME is not None:",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.setup",
        "documentation": {}
    },
    {
        "label": "ext_modules",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.setup",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.setup",
        "peekOfCode": "ext_modules = []\nif not SKIP_CUDA_BUILD:\n    print(\"\\n\\ntorch.__version__  = {}\\n\\n\".format(torch.__version__))\n    TORCH_MAJOR = int(torch.__version__.split(\".\")[0])\n    TORCH_MINOR = int(torch.__version__.split(\".\")[1])\n    check_if_cuda_home_none(PACKAGE_NAME)\n    # Check, if CUDA11 is installed for compute capability 8.0\n    cc_flag = []\n    if CUDA_HOME is not None:\n        _, bare_metal_version = get_cuda_bare_metal_version(CUDA_HOME)",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.setup",
        "documentation": {}
    },
    {
        "label": "TextSamplerDataset",
        "kind": 6,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "peekOfCode": "class TextSamplerDataset(Dataset):\n    def __init__(self, data, seq_len):\n        super().__init__()\n        self.data = data\n        self.seq_len = seq_len\n    def __getitem__(self, index):\n        rand_start = torch.randint(0, self.data.size(0) - self.seq_len, (1,))\n        full_seq = self.data[rand_start : rand_start + self.seq_len + 1].long()\n        return full_seq.cuda()\n    def __len__(self):",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "documentation": {}
    },
    {
        "label": "cycle",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "peekOfCode": "def cycle(loader):\n    while True:\n        yield from loader\ndef decode_token(token):\n    return str(chr(max(32, token)))\ndef decode_tokens(tokens):\n    return \"\".join(list(map(decode_token, tokens)))\n# instantiate GPT-like decoder model\nmodel = BitNetTransformer(num_tokens=256, dim=512, depth=8)\nmodel = AutoregressiveWrapper(model, max_seq_len=SEQ_LEN)",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "documentation": {}
    },
    {
        "label": "decode_token",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "peekOfCode": "def decode_token(token):\n    return str(chr(max(32, token)))\ndef decode_tokens(tokens):\n    return \"\".join(list(map(decode_token, tokens)))\n# instantiate GPT-like decoder model\nmodel = BitNetTransformer(num_tokens=256, dim=512, depth=8)\nmodel = AutoregressiveWrapper(model, max_seq_len=SEQ_LEN)\n# Use all available GPUs\nif torch.cuda.device_count() > 1:\n    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "documentation": {}
    },
    {
        "label": "decode_tokens",
        "kind": 2,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "peekOfCode": "def decode_tokens(tokens):\n    return \"\".join(list(map(decode_token, tokens)))\n# instantiate GPT-like decoder model\nmodel = BitNetTransformer(num_tokens=256, dim=512, depth=8)\nmodel = AutoregressiveWrapper(model, max_seq_len=SEQ_LEN)\n# Use all available GPUs\nif torch.cuda.device_count() > 1:\n    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n    model = torch.nn.DataParallel(model)\nmodel.cuda()",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "documentation": {}
    },
    {
        "label": "NUM_BATCHES",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "peekOfCode": "NUM_BATCHES = int(1e5)\nBATCH_SIZE = 4\nGRADIENT_ACCUMULATE_EVERY = 4\nLEARNING_RATE = 2e-4\nVALIDATE_EVERY = 100\nGENERATE_EVERY = 500\nGENERATE_LENGTH = 512\nSEQ_LEN = 1024\n# helpers\ndef cycle(loader):",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "documentation": {}
    },
    {
        "label": "BATCH_SIZE",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "peekOfCode": "BATCH_SIZE = 4\nGRADIENT_ACCUMULATE_EVERY = 4\nLEARNING_RATE = 2e-4\nVALIDATE_EVERY = 100\nGENERATE_EVERY = 500\nGENERATE_LENGTH = 512\nSEQ_LEN = 1024\n# helpers\ndef cycle(loader):\n    while True:",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "documentation": {}
    },
    {
        "label": "GRADIENT_ACCUMULATE_EVERY",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "peekOfCode": "GRADIENT_ACCUMULATE_EVERY = 4\nLEARNING_RATE = 2e-4\nVALIDATE_EVERY = 100\nGENERATE_EVERY = 500\nGENERATE_LENGTH = 512\nSEQ_LEN = 1024\n# helpers\ndef cycle(loader):\n    while True:\n        yield from loader",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "documentation": {}
    },
    {
        "label": "LEARNING_RATE",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "peekOfCode": "LEARNING_RATE = 2e-4\nVALIDATE_EVERY = 100\nGENERATE_EVERY = 500\nGENERATE_LENGTH = 512\nSEQ_LEN = 1024\n# helpers\ndef cycle(loader):\n    while True:\n        yield from loader\ndef decode_token(token):",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "documentation": {}
    },
    {
        "label": "VALIDATE_EVERY",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "peekOfCode": "VALIDATE_EVERY = 100\nGENERATE_EVERY = 500\nGENERATE_LENGTH = 512\nSEQ_LEN = 1024\n# helpers\ndef cycle(loader):\n    while True:\n        yield from loader\ndef decode_token(token):\n    return str(chr(max(32, token)))",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "documentation": {}
    },
    {
        "label": "GENERATE_EVERY",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "peekOfCode": "GENERATE_EVERY = 500\nGENERATE_LENGTH = 512\nSEQ_LEN = 1024\n# helpers\ndef cycle(loader):\n    while True:\n        yield from loader\ndef decode_token(token):\n    return str(chr(max(32, token)))\ndef decode_tokens(tokens):",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "documentation": {}
    },
    {
        "label": "GENERATE_LENGTH",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "peekOfCode": "GENERATE_LENGTH = 512\nSEQ_LEN = 1024\n# helpers\ndef cycle(loader):\n    while True:\n        yield from loader\ndef decode_token(token):\n    return str(chr(max(32, token)))\ndef decode_tokens(tokens):\n    return \"\".join(list(map(decode_token, tokens)))",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "documentation": {}
    },
    {
        "label": "SEQ_LEN",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "peekOfCode": "SEQ_LEN = 1024\n# helpers\ndef cycle(loader):\n    while True:\n        yield from loader\ndef decode_token(token):\n    return str(chr(max(32, token)))\ndef decode_tokens(tokens):\n    return \"\".join(list(map(decode_token, tokens)))\n# instantiate GPT-like decoder model",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "peekOfCode": "model = BitNetTransformer(num_tokens=256, dim=512, depth=8)\nmodel = AutoregressiveWrapper(model, max_seq_len=SEQ_LEN)\n# Use all available GPUs\nif torch.cuda.device_count() > 1:\n    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n    model = torch.nn.DataParallel(model)\nmodel.cuda()\n# prepare enwik8 data\nwith gzip.open(\"./data/enwik8.gz\") as file:\n    X = np.fromstring(file.read(int(95e6)), dtype=np.uint8)",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "peekOfCode": "model = AutoregressiveWrapper(model, max_seq_len=SEQ_LEN)\n# Use all available GPUs\nif torch.cuda.device_count() > 1:\n    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n    model = torch.nn.DataParallel(model)\nmodel.cuda()\n# prepare enwik8 data\nwith gzip.open(\"./data/enwik8.gz\") as file:\n    X = np.fromstring(file.read(int(95e6)), dtype=np.uint8)\n    trX, vaX = np.split(X, [int(90e6)])",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "documentation": {}
    },
    {
        "label": "train_dataset",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "peekOfCode": "train_dataset = TextSamplerDataset(data_train, SEQ_LEN)\nval_dataset = TextSamplerDataset(data_val, SEQ_LEN)\ntrain_loader = cycle(DataLoader(train_dataset, batch_size=BATCH_SIZE))\nval_loader = cycle(DataLoader(val_dataset, batch_size=BATCH_SIZE))\n# optimizer\noptim = StableAdamWUnfused(\n    model.parameters(),\n    lr=LEARNING_RATE,\n)\n# training",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "documentation": {}
    },
    {
        "label": "val_dataset",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "peekOfCode": "val_dataset = TextSamplerDataset(data_val, SEQ_LEN)\ntrain_loader = cycle(DataLoader(train_dataset, batch_size=BATCH_SIZE))\nval_loader = cycle(DataLoader(val_dataset, batch_size=BATCH_SIZE))\n# optimizer\noptim = StableAdamWUnfused(\n    model.parameters(),\n    lr=LEARNING_RATE,\n)\n# training\nfor i in tqdm.tqdm(range(NUM_BATCHES), mininterval=10.0, desc=\"training\"):",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "documentation": {}
    },
    {
        "label": "train_loader",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "peekOfCode": "train_loader = cycle(DataLoader(train_dataset, batch_size=BATCH_SIZE))\nval_loader = cycle(DataLoader(val_dataset, batch_size=BATCH_SIZE))\n# optimizer\noptim = StableAdamWUnfused(\n    model.parameters(),\n    lr=LEARNING_RATE,\n)\n# training\nfor i in tqdm.tqdm(range(NUM_BATCHES), mininterval=10.0, desc=\"training\"):\n    model.train()",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "documentation": {}
    },
    {
        "label": "val_loader",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "peekOfCode": "val_loader = cycle(DataLoader(val_dataset, batch_size=BATCH_SIZE))\n# optimizer\noptim = StableAdamWUnfused(\n    model.parameters(),\n    lr=LEARNING_RATE,\n)\n# training\nfor i in tqdm.tqdm(range(NUM_BATCHES), mininterval=10.0, desc=\"training\"):\n    model.train()\n    for __ in range(GRADIENT_ACCUMULATE_EVERY):",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "documentation": {}
    },
    {
        "label": "optim",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "peekOfCode": "optim = StableAdamWUnfused(\n    model.parameters(),\n    lr=LEARNING_RATE,\n)\n# training\nfor i in tqdm.tqdm(range(NUM_BATCHES), mininterval=10.0, desc=\"training\"):\n    model.train()\n    for __ in range(GRADIENT_ACCUMULATE_EVERY):\n        loss = model(next(train_loader))\n        loss.mean().backward()",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.train",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.transformer_example",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.transformer_example",
        "peekOfCode": "x = torch.randint(0, 20000, (1, 1024))\n# Initialize the BitNetTransformer model\nbitnet = BitNetTransformer(\n    num_tokens=20000,  # Number of unique tokens in the input\n    dim=1024,  # Dimension of the input and output embeddings\n    depth=6,  # Number of transformer layers\n    heads=8,  # Number of attention heads\n    ff_mult=4,  # Multiplier for the hidden dimension in the feed-forward network\n)\n# Pass the tensor through the transformer model",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.transformer_example",
        "documentation": {}
    },
    {
        "label": "bitnet",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.transformer_example",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.transformer_example",
        "peekOfCode": "bitnet = BitNetTransformer(\n    num_tokens=20000,  # Number of unique tokens in the input\n    dim=1024,  # Dimension of the input and output embeddings\n    depth=6,  # Number of transformer layers\n    heads=8,  # Number of attention heads\n    ff_mult=4,  # Multiplier for the hidden dimension in the feed-forward network\n)\n# Pass the tensor through the transformer model\nlogits = bitnet(x)\n# Print the shape of the output",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.transformer_example",
        "documentation": {}
    },
    {
        "label": "logits",
        "kind": 5,
        "importPath": "CapibaraMerge.BitNet-main.BitNet-main.transformer_example",
        "description": "CapibaraMerge.BitNet-main.BitNet-main.transformer_example",
        "peekOfCode": "logits = bitnet(x)\n# Print the shape of the output\nprint(logits)",
        "detail": "CapibaraMerge.BitNet-main.BitNet-main.transformer_example",
        "documentation": {}
    },
    {
        "label": "KANLinear",
        "kind": 6,
        "importPath": "CapibaraMerge.efficient_kan.kan",
        "description": "CapibaraMerge.efficient_kan.kan",
        "peekOfCode": "class KANLinear(torch.nn.Module):\n    def __init__(\n        self,\n        in_features,\n        out_features,\n        grid_size=5,\n        spline_order=3,\n        scale_noise=0.1,\n        scale_base=1.0,\n        scale_spline=1.0,",
        "detail": "CapibaraMerge.efficient_kan.kan",
        "documentation": {}
    },
    {
        "label": "KAN",
        "kind": 6,
        "importPath": "CapibaraMerge.efficient_kan.kan",
        "description": "CapibaraMerge.efficient_kan.kan",
        "peekOfCode": "class KAN(torch.nn.Module):\n    def __init__(\n        self,\n        layers_hidden,\n        grid_size=5,\n        spline_order=3,\n        scale_noise=0.1,\n        scale_base=1.0,\n        scale_spline=1.0,\n        base_activation=torch.nn.SiLU,",
        "detail": "CapibaraMerge.efficient_kan.kan",
        "documentation": {}
    },
    {
        "label": "RMSNorm",
        "kind": 6,
        "importPath": "CapibaraMerge.mambabyte.model",
        "description": "CapibaraMerge.mambabyte.model",
        "peekOfCode": "class RMSNorm(nn.Module):\n    def __init__(self, d_model: int, eps: float = 1e-5):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(d_model))\n    def forward(self, x):\n        output = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight\n        return output\nclass PScan(torch.autograd.Function):\n    @staticmethod",
        "detail": "CapibaraMerge.mambabyte.model",
        "documentation": {}
    },
    {
        "label": "PScan",
        "kind": 6,
        "importPath": "CapibaraMerge.mambabyte.model",
        "description": "CapibaraMerge.mambabyte.model",
        "peekOfCode": "class PScan(torch.autograd.Function):\n    @staticmethod\n    def pscan(A, X):\n        # A : (B, D, L, N)\n        # X : (B, D, L, N)\n        # modifies X in place by doing a parallel scan.\n        # more formally, X will be populated by these values :\n        # H[t] = A[t] * H[t-1] + X[t] with H[0] = 0\n        # which are computed in parallel (2*log2(T) sequential steps (ideally), instead of T sequential steps)\n        B, D, L, _ = A.size()",
        "detail": "CapibaraMerge.mambabyte.model",
        "documentation": {}
    },
    {
        "label": "MambaConfig",
        "kind": 6,
        "importPath": "CapibaraMerge.mambabyte.model",
        "description": "CapibaraMerge.mambabyte.model",
        "peekOfCode": "class MambaConfig:\n    dim: int  # D\n    depth: int\n    dt_rank: Union[int, str] = \"auto\"\n    d_state: int = 16  # N in paper/comments\n    expand_factor: int = 2  # E in paper/comments\n    d_conv: int = 4\n    dt_min: float = 0.001\n    dt_max: float = 0.1\n    dt_init: str = \"random\"  # \"random\" or \"constant\"",
        "detail": "CapibaraMerge.mambabyte.model",
        "documentation": {}
    },
    {
        "label": "ResidualBlock",
        "kind": 6,
        "importPath": "CapibaraMerge.mambabyte.model",
        "description": "CapibaraMerge.mambabyte.model",
        "peekOfCode": "class ResidualBlock(nn.Module):\n    def __init__(self, config: MambaConfig):\n        super().__init__()\n        self.mixer = MambaBlock(config)\n        self.norm = RMSNorm(config.dim)\n    def forward(self, x):\n        # x : (B, L, D)\n        # output : (B, L, D)\n        output = self.mixer(self.norm(x)) + x\n        return output",
        "detail": "CapibaraMerge.mambabyte.model",
        "documentation": {}
    },
    {
        "label": "MambaBlock",
        "kind": 6,
        "importPath": "CapibaraMerge.mambabyte.model",
        "description": "CapibaraMerge.mambabyte.model",
        "peekOfCode": "class MambaBlock(nn.Module):\n    def __init__(self, config: MambaConfig):\n        super().__init__()\n        self.config = config\n        # projects block input from D to 2*ED (two branches)\n        self.in_proj = nn.Linear(\n            config.dim, 2 * config.d_inner, bias=config.bias\n        )\n        self.conv1d = nn.Conv1d(\n            in_channels=config.d_inner,",
        "detail": "CapibaraMerge.mambabyte.model",
        "documentation": {}
    },
    {
        "label": "Mamba",
        "kind": 6,
        "importPath": "CapibaraMerge.mambabyte.model",
        "description": "CapibaraMerge.mambabyte.model",
        "peekOfCode": "class Mamba(nn.Module):\n    def __init__(self, config: MambaConfig):\n        super().__init__()\n        self.config = config\n        self.layers = nn.ModuleList(\n            [ResidualBlock(config) for _ in range(config.depth)]\n        )\n        # self.norm_f = RMSNorm(config.dim)\n    def forward(self, x):\n        # x : (B, L, D)",
        "detail": "CapibaraMerge.mambabyte.model",
        "documentation": {}
    },
    {
        "label": "pscan",
        "kind": 5,
        "importPath": "CapibaraMerge.mambabyte.model",
        "description": "CapibaraMerge.mambabyte.model",
        "peekOfCode": "pscan = PScan.apply\n@dataclass\nclass MambaConfig:\n    dim: int  # D\n    depth: int\n    dt_rank: Union[int, str] = \"auto\"\n    d_state: int = 16  # N in paper/comments\n    expand_factor: int = 2  # E in paper/comments\n    d_conv: int = 4\n    dt_min: float = 0.001",
        "detail": "CapibaraMerge.mambabyte.model",
        "documentation": {}
    },
    {
        "label": "transform",
        "kind": 5,
        "importPath": "efficient-kan-master.efficient-kan-master.examples.mnist",
        "description": "efficient-kan-master.efficient-kan-master.examples.mnist",
        "peekOfCode": "transform = transforms.Compose(\n    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n)\ntrainset = torchvision.datasets.MNIST(\n    root=\"./data\", train=True, download=True, transform=transform\n)\nvalset = torchvision.datasets.MNIST(\n    root=\"./data\", train=False, download=True, transform=transform\n)\ntrainloader = DataLoader(trainset, batch_size=64, shuffle=True)",
        "detail": "efficient-kan-master.efficient-kan-master.examples.mnist",
        "documentation": {}
    },
    {
        "label": "trainset",
        "kind": 5,
        "importPath": "efficient-kan-master.efficient-kan-master.examples.mnist",
        "description": "efficient-kan-master.efficient-kan-master.examples.mnist",
        "peekOfCode": "trainset = torchvision.datasets.MNIST(\n    root=\"./data\", train=True, download=True, transform=transform\n)\nvalset = torchvision.datasets.MNIST(\n    root=\"./data\", train=False, download=True, transform=transform\n)\ntrainloader = DataLoader(trainset, batch_size=64, shuffle=True)\nvalloader = DataLoader(valset, batch_size=64, shuffle=False)\n# Define model\nmodel = KAN([28 * 28, 64, 10])",
        "detail": "efficient-kan-master.efficient-kan-master.examples.mnist",
        "documentation": {}
    },
    {
        "label": "valset",
        "kind": 5,
        "importPath": "efficient-kan-master.efficient-kan-master.examples.mnist",
        "description": "efficient-kan-master.efficient-kan-master.examples.mnist",
        "peekOfCode": "valset = torchvision.datasets.MNIST(\n    root=\"./data\", train=False, download=True, transform=transform\n)\ntrainloader = DataLoader(trainset, batch_size=64, shuffle=True)\nvalloader = DataLoader(valset, batch_size=64, shuffle=False)\n# Define model\nmodel = KAN([28 * 28, 64, 10])\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n# Define optimizer",
        "detail": "efficient-kan-master.efficient-kan-master.examples.mnist",
        "documentation": {}
    },
    {
        "label": "trainloader",
        "kind": 5,
        "importPath": "efficient-kan-master.efficient-kan-master.examples.mnist",
        "description": "efficient-kan-master.efficient-kan-master.examples.mnist",
        "peekOfCode": "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\nvalloader = DataLoader(valset, batch_size=64, shuffle=False)\n# Define model\nmodel = KAN([28 * 28, 64, 10])\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n# Define optimizer\noptimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n# Define learning rate scheduler\nscheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)",
        "detail": "efficient-kan-master.efficient-kan-master.examples.mnist",
        "documentation": {}
    },
    {
        "label": "valloader",
        "kind": 5,
        "importPath": "efficient-kan-master.efficient-kan-master.examples.mnist",
        "description": "efficient-kan-master.efficient-kan-master.examples.mnist",
        "peekOfCode": "valloader = DataLoader(valset, batch_size=64, shuffle=False)\n# Define model\nmodel = KAN([28 * 28, 64, 10])\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n# Define optimizer\noptimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n# Define learning rate scheduler\nscheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n# Define loss",
        "detail": "efficient-kan-master.efficient-kan-master.examples.mnist",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "efficient-kan-master.efficient-kan-master.examples.mnist",
        "description": "efficient-kan-master.efficient-kan-master.examples.mnist",
        "peekOfCode": "model = KAN([28 * 28, 64, 10])\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n# Define optimizer\noptimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n# Define learning rate scheduler\nscheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n# Define loss\ncriterion = nn.CrossEntropyLoss()\nfor epoch in range(10):",
        "detail": "efficient-kan-master.efficient-kan-master.examples.mnist",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "efficient-kan-master.efficient-kan-master.examples.mnist",
        "description": "efficient-kan-master.efficient-kan-master.examples.mnist",
        "peekOfCode": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n# Define optimizer\noptimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n# Define learning rate scheduler\nscheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n# Define loss\ncriterion = nn.CrossEntropyLoss()\nfor epoch in range(10):\n    # Train",
        "detail": "efficient-kan-master.efficient-kan-master.examples.mnist",
        "documentation": {}
    },
    {
        "label": "optimizer",
        "kind": 5,
        "importPath": "efficient-kan-master.efficient-kan-master.examples.mnist",
        "description": "efficient-kan-master.efficient-kan-master.examples.mnist",
        "peekOfCode": "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n# Define learning rate scheduler\nscheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n# Define loss\ncriterion = nn.CrossEntropyLoss()\nfor epoch in range(10):\n    # Train\n    model.train()\n    with tqdm(trainloader) as pbar:\n        for i, (images, labels) in enumerate(pbar):",
        "detail": "efficient-kan-master.efficient-kan-master.examples.mnist",
        "documentation": {}
    },
    {
        "label": "scheduler",
        "kind": 5,
        "importPath": "efficient-kan-master.efficient-kan-master.examples.mnist",
        "description": "efficient-kan-master.efficient-kan-master.examples.mnist",
        "peekOfCode": "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)\n# Define loss\ncriterion = nn.CrossEntropyLoss()\nfor epoch in range(10):\n    # Train\n    model.train()\n    with tqdm(trainloader) as pbar:\n        for i, (images, labels) in enumerate(pbar):\n            images = images.view(-1, 28 * 28).to(device)\n            optimizer.zero_grad()",
        "detail": "efficient-kan-master.efficient-kan-master.examples.mnist",
        "documentation": {}
    },
    {
        "label": "criterion",
        "kind": 5,
        "importPath": "efficient-kan-master.efficient-kan-master.examples.mnist",
        "description": "efficient-kan-master.efficient-kan-master.examples.mnist",
        "peekOfCode": "criterion = nn.CrossEntropyLoss()\nfor epoch in range(10):\n    # Train\n    model.train()\n    with tqdm(trainloader) as pbar:\n        for i, (images, labels) in enumerate(pbar):\n            images = images.view(-1, 28 * 28).to(device)\n            optimizer.zero_grad()\n            output = model(images)\n            loss = criterion(output, labels.to(device))",
        "detail": "efficient-kan-master.efficient-kan-master.examples.mnist",
        "documentation": {}
    },
    {
        "label": "KANLinear",
        "kind": 6,
        "importPath": "efficient-kan-master.efficient-kan-master.src.efficient_kan.kan",
        "description": "efficient-kan-master.efficient-kan-master.src.efficient_kan.kan",
        "peekOfCode": "class KANLinear(torch.nn.Module):\n    def __init__(\n        self,\n        in_features,\n        out_features,\n        grid_size=5,\n        spline_order=3,\n        scale_noise=0.1,\n        scale_base=1.0,\n        scale_spline=1.0,",
        "detail": "efficient-kan-master.efficient-kan-master.src.efficient_kan.kan",
        "documentation": {}
    },
    {
        "label": "KAN",
        "kind": 6,
        "importPath": "efficient-kan-master.efficient-kan-master.src.efficient_kan.kan",
        "description": "efficient-kan-master.efficient-kan-master.src.efficient_kan.kan",
        "peekOfCode": "class KAN(torch.nn.Module):\n    def __init__(\n        self,\n        layers_hidden,\n        grid_size=5,\n        spline_order=3,\n        scale_noise=0.1,\n        scale_base=1.0,\n        scale_spline=1.0,\n        base_activation=torch.nn.SiLU,",
        "detail": "efficient-kan-master.efficient-kan-master.src.efficient_kan.kan",
        "documentation": {}
    },
    {
        "label": "test_mul",
        "kind": 2,
        "importPath": "efficient-kan-master.efficient-kan-master.tests.test_simple_math",
        "description": "efficient-kan-master.efficient-kan-master.tests.test_simple_math",
        "peekOfCode": "def test_mul():\n    kan = KAN([2, 2, 1], base_activation=nn.Identity)\n    optimizer = torch.optim.LBFGS(kan.parameters(), lr=1)\n    with tqdm(range(100)) as pbar:\n        for i in pbar:\n            loss, reg_loss = None, None\n            def closure():\n                optimizer.zero_grad()\n                x = torch.rand(1024, 2)\n                y = kan(x, update_grid=(i % 20 == 0))",
        "detail": "efficient-kan-master.efficient-kan-master.tests.test_simple_math",
        "documentation": {}
    },
    {
        "label": "RMSNorm",
        "kind": 6,
        "importPath": "mambabyte.model",
        "description": "mambabyte.model",
        "peekOfCode": "class RMSNorm(nn.Module):\n    def __init__(self, d_model: int, eps: float = 1e-5):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(d_model))\n    def forward(self, x):\n        output = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight\n        return output\nclass PScan(torch.autograd.Function):\n    @staticmethod",
        "detail": "mambabyte.model",
        "documentation": {}
    },
    {
        "label": "PScan",
        "kind": 6,
        "importPath": "mambabyte.model",
        "description": "mambabyte.model",
        "peekOfCode": "class PScan(torch.autograd.Function):\n    @staticmethod\n    def pscan(A, X):\n        # A : (B, D, L, N)\n        # X : (B, D, L, N)\n        # modifies X in place by doing a parallel scan.\n        # more formally, X will be populated by these values :\n        # H[t] = A[t] * H[t-1] + X[t] with H[0] = 0\n        # which are computed in parallel (2*log2(T) sequential steps (ideally), instead of T sequential steps)\n        B, D, L, _ = A.size()",
        "detail": "mambabyte.model",
        "documentation": {}
    },
    {
        "label": "MambaConfig",
        "kind": 6,
        "importPath": "mambabyte.model",
        "description": "mambabyte.model",
        "peekOfCode": "class MambaConfig:\n    dim: int  # D\n    depth: int\n    dt_rank: Union[int, str] = \"auto\"\n    d_state: int = 16  # N in paper/comments\n    expand_factor: int = 2  # E in paper/comments\n    d_conv: int = 4\n    dt_min: float = 0.001\n    dt_max: float = 0.1\n    dt_init: str = \"random\"  # \"random\" or \"constant\"",
        "detail": "mambabyte.model",
        "documentation": {}
    },
    {
        "label": "ResidualBlock",
        "kind": 6,
        "importPath": "mambabyte.model",
        "description": "mambabyte.model",
        "peekOfCode": "class ResidualBlock(nn.Module):\n    def __init__(self, config: MambaConfig):\n        super().__init__()\n        self.mixer = MambaBlock(config)\n        self.norm = RMSNorm(config.dim)\n    def forward(self, x):\n        # x : (B, L, D)\n        # output : (B, L, D)\n        output = self.mixer(self.norm(x)) + x\n        return output",
        "detail": "mambabyte.model",
        "documentation": {}
    },
    {
        "label": "MambaBlock",
        "kind": 6,
        "importPath": "mambabyte.model",
        "description": "mambabyte.model",
        "peekOfCode": "class MambaBlock(nn.Module):\n    def __init__(self, config: MambaConfig):\n        super().__init__()\n        self.config = config\n        # projects block input from D to 2*ED (two branches)\n        self.in_proj = nn.Linear(\n            config.dim, 2 * config.d_inner, bias=config.bias\n        )\n        self.conv1d = nn.Conv1d(\n            in_channels=config.d_inner,",
        "detail": "mambabyte.model",
        "documentation": {}
    },
    {
        "label": "Mamba",
        "kind": 6,
        "importPath": "mambabyte.model",
        "description": "mambabyte.model",
        "peekOfCode": "class Mamba(nn.Module):\n    def __init__(self, config: MambaConfig):\n        super().__init__()\n        self.config = config\n        self.layers = nn.ModuleList(\n            [ResidualBlock(config) for _ in range(config.depth)]\n        )\n        # self.norm_f = RMSNorm(config.dim)\n    def forward(self, x):\n        # x : (B, L, D)",
        "detail": "mambabyte.model",
        "documentation": {}
    },
    {
        "label": "pscan",
        "kind": 5,
        "importPath": "mambabyte.model",
        "description": "mambabyte.model",
        "peekOfCode": "pscan = PScan.apply\n@dataclass\nclass MambaConfig:\n    dim: int  # D\n    depth: int\n    dt_rank: Union[int, str] = \"auto\"\n    d_state: int = 16  # N in paper/comments\n    expand_factor: int = 2  # E in paper/comments\n    d_conv: int = 4\n    dt_min: float = 0.001",
        "detail": "mambabyte.model",
        "documentation": {}
    },
    {
        "label": "x",
        "kind": 5,
        "importPath": "example",
        "description": "example",
        "peekOfCode": "x = torch.randn(2, 3, 4)\nconfig = MambaConfig(\n    dim = 4,\n    depth = 3,\n    dt_rank = 2,\n    d_state = 2,\n    expand_factor = 2,\n    d_conv = 3,\n    dt_min = 0.001,\n    dt_max = 0.1,",
        "detail": "example",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "example",
        "description": "example",
        "peekOfCode": "config = MambaConfig(\n    dim = 4,\n    depth = 3,\n    dt_rank = 2,\n    d_state = 2,\n    expand_factor = 2,\n    d_conv = 3,\n    dt_min = 0.001,\n    dt_max = 0.1,\n    dt_init = \"random\",",
        "detail": "example",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "example",
        "description": "example",
        "peekOfCode": "model = Mamba(config)\nout = model(x)\nprint(out)",
        "detail": "example",
        "documentation": {}
    },
    {
        "label": "out",
        "kind": 5,
        "importPath": "example",
        "description": "example",
        "peekOfCode": "out = model(x)\nprint(out)",
        "detail": "example",
        "documentation": {}
    }
]